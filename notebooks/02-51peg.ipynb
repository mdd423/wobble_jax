{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854ca1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wobbleenv/lib/python3.11/site-packages/jax/_src/api_util.py:174: SyntaxWarning: Jitted function has static_argnums=(3, 4, 5), but only accepts 5 positional arguments. This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has {argnums_name}={argnums}, \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.colors\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import jabble.model\n",
    "import jabble.dataset\n",
    "import jabble.loss\n",
    "import jabble.physics \n",
    "import astropy.units as u\n",
    "\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "# from jaxopt import GaussNewton\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "# from mpl_axes_aligner import align\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import jabble.physics\n",
    "\n",
    "import jax\n",
    "import jax.config\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_disable_jit\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d1a69-9b8d-445a-8c96-ffe4d4833863",
   "metadata": {},
   "source": [
    "\n",
    "<h1>02 - Fitting RVs to HARPS e2ds Spectra</h1>\n",
    "In this notebook, I am will show you how to load in data and metadata to jabble.data objects. Then fit models to each component of the data. Due to some edge errors with normalizing near the edge of the orders in the HARPS spectrograph, we fit a longer wavelength component normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c72b75-906c-48c1-b11d-f1cb48103260",
   "metadata": {},
   "source": [
    "\n",
    "Create output directory here as the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9961a7-5035-4cc7-a8b1-1be56ad0c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "out_dir = os.path.join('/scratch/mdd423/wobble_jax/','out',today.strftime(\"%y-%m-%d\"))\n",
    "os.makedirs(out_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cf2311c-3b0d-4095-bf02-e34b8570cef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/mdd423/wobble_jax/out/25-05-12'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e260ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b = h5py.File(\"../data/barnards_e2ds.hdf5\", \"r\")\n",
    "file_p = h5py.File(\"../data/51peg_e2ds.hdf5\"   , \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cad6efee-bdf7-4be8-9816-a3f7d5127e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dates\": shape (306,), type \"<f8\">"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_b['dates']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fd5722-2bc5-4c0b-b9c9-9c549405956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax:    0.4.23\n",
      "jaxlib: 0.4.23\n",
      "numpy:  1.24.3\n",
      "python: 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0]\n",
      "jax.devices (1 total, 1 local): [CpuDevice(id=0)]\n",
      "process_count: 1\n"
     ]
    }
   ],
   "source": [
    "jax.print_environment_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712d1c5-0502-4549-b18d-0b3e28440511",
   "metadata": {},
   "source": [
    "\n",
    "<h2>Jabble.Dataset</h2>\n",
    "The 4 fields needed to create a jabble.data object are: xs, ys, yivar, and mask. xs is the input to whats being modeled, this should be a list of 1 dimensional jax.numpy arrays, which may have different shapes (lengths). These correspond to the log wavelength values being evaluated. The logarithm of wavelength means that the redshift from relative radial velocity change becomes an additive property.\n",
    "\n",
    "$$x_{obs} = \\log(\\lambda_{obs})$$\n",
    "$$\\lambda_{obs} = \\sqrt{\\frac{1 + RV/c}{1- RV/c}}\\lambda_{emit}$$\n",
    "$$x_{obs} = \\log(\\sqrt{\\frac{1 + RV/c}{1- RV/c}}\\lambda_{emit})$$\n",
    "$$x_{obs} = \\log(\\sqrt{\\frac{1 + RV/c}{1- RV/c}}) + \\log(\\lambda_{emit})$$\n",
    "$$x_{obs} = \\delta x + x_{emit}$$\n",
    "\n",
    "where $x_{obs}$ is the observed log wavelength, $RV$ is the radial velocity, $c$ is the speed of light, and $x_{emit}$ is the emitted log wavelength. ys is the measured log flux at that log wavelength, xs. This value is assumed to be in photon per wavelength bin. Thus, multiplicative properties like absorption from tellurics become additive in the logarithm. \n",
    "\n",
    "$$y = \\log(f)$$\n",
    "$$y = \\log(f_s f_t) $$\n",
    "$$y = \\log(f_s) + \\log(f_t) $$\n",
    "$$y = y_s + y_t$$\n",
    "\n",
    "yivar stands for y inverse variance, or y information. This value is produced by the instrumentation team at these detectors using some assumption about instrument readout and dark current properties along with poisson photon noise. Thus the error on the flux is not being modeled here and is assumed as correct from the pipeline. These errors are given as the standard deviation of the flux, so we need to transform error of f into the error in logarithm of flux.\n",
    "\n",
    "$$\\sigma_y = \\sigma_f \\frac{dy}{df}$$\n",
    "$$\\sigma_y = \\frac{\\sigma_f}{f}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92fbca0a-1c72-43bb-9224-08eb3fac9e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_continuum(x, y, ivars, order=6, nsigma=[0.3,3.0], maxniter=50):\n",
    "    \"\"\"Fit the continuum using sigma clipping\n",
    "\n",
    "    Args:\n",
    "        x: The wavelengths\n",
    "        y: The log-fluxes\n",
    "        order: The polynomial order to use\n",
    "        nsigma: The sigma clipping threshold: tuple (low, high)\n",
    "        maxniter: The maximum number of iterations to do\n",
    "\n",
    "    Returns:\n",
    "        The value of the continuum at the wavelengths in x\n",
    "        Author: Megan Bedell\n",
    "\n",
    "    \"\"\"\n",
    "    A = np.vander(x - np.nanmean(x), order+1)\n",
    "    m = np.ones(len(x), dtype=bool)\n",
    "    for i in range(maxniter):\n",
    "        m[ivars == 0] = 0  # mask out the bad pixels\n",
    "        w, _, _, _ = np.linalg.lstsq(np.dot(A[m].T, A[m]), np.dot(A[m].T, y[m]))\n",
    "        mu = np.dot(A, w)\n",
    "        resid = y - mu\n",
    "        sigma = np.sqrt(np.nanmedian(resid**2))\n",
    "        #m_new = np.abs(resid) < nsigma*sigma\n",
    "        m_new = (resid > -nsigma[0]*sigma) & (resid < nsigma[1]*sigma)\n",
    "        if m.sum() == m_new.sum():\n",
    "            m = m_new\n",
    "            break\n",
    "        # print(m.shape,m_new.shape)\n",
    "        m = m_new\n",
    "    return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b00a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file,orders,device):\n",
    "    ys = []\n",
    "    xs = []\n",
    "    yivar = []\n",
    "    mask = []\n",
    "\n",
    "    init_rvs = np.array(file[\"bervs\"])\n",
    "    times_t = np.array([x for x in file[\"dates\"]])\n",
    "    full_init_rvs = []\n",
    "    # print(file[\"bervs\"].shape)\n",
    "    airmass = []\n",
    "    order_out = []\n",
    "    times = []\n",
    "    star_ids = []\n",
    "\n",
    "    for iii in orders:\n",
    "        for jjj in range(file[\"data\"].shape[1]):\n",
    "            contin = fit_continuum(np.array(file[\"xs\"][iii,jjj,:]),np.array(file[\"data\"][iii,jjj,:]),np.array(file[\"ivars\"][iii,jjj,:]),\\\n",
    "                                   order=6, nsigma=[0.3,3.0], maxniter=50)\n",
    "            \n",
    "            ys.append(jnp.array(file[\"data\"][iii,jjj,:]) - contin)\n",
    "            xs.append(jnp.array(file[\"xs\"][iii,jjj,:]))\n",
    "            yivar.append(jnp.array(file[\"ivars\"][iii,jjj,:]))\n",
    "            mask.append(jnp.zeros(file[\"data\"][iii,jjj,:].shape,dtype=bool))\n",
    "\n",
    "            # init_shifts.append(\n",
    "            airmass.append(file[\"airms\"][jjj])\n",
    "            order_out.append(iii)\n",
    "            times.append(file[\"dates\"][jjj])\n",
    "            star_ids.append(str(file))\n",
    "\n",
    "            full_init_rvs.append(file[\"bervs\"][jjj])\n",
    "    \n",
    "    full_init_rvs = jnp.array(full_init_rvs)\n",
    "    airmass = jnp.array(airmass)\n",
    "\n",
    "    dataset = jabble.dataset.Data.from_lists(xs,ys,yivar,mask)\n",
    "\n",
    "    dataset.metakeys['times'] = times_t\n",
    "    dataset.metadata['times'] = np.array(times)\n",
    "    dataset.metadata['orders'] = np.array(order_out)\n",
    "    dataset.metadata['star_ids'] = np.array(star_ids)\n",
    "    dataset.metadata['init_rvs'] = np.array(full_init_rvs)\n",
    "\n",
    "    dataset.metadata['airmass'] = airmass\n",
    "    dataset.metadata['bervs'] = full_init_rvs\n",
    "    dataset.to_device(device)\n",
    "    \n",
    "    # init_shifts = jax.device_put(init_shifts,device)\n",
    "    # airmass = jax.device_put(airmass,device)\n",
    "    \n",
    "    return dataset, init_rvs, airmass, full_init_rvs, times_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c749e55-b98c-4948-9a15-bb9faf2a03c6",
   "metadata": {},
   "source": [
    "<h2>Jabble.Model</h2>\n",
    "Now, we will define the components of the model. These stars will have 3 components: stellar, telluric, pseudo-normalization. \n",
    "\n",
    "$$y = y_\\mathrm{s} + y_\\mathrm{t} + y_\\mathrm{N}$$\n",
    "\n",
    "The stellar component will be be high resolution (twice R of the detector) in wavelength and static in flux across all times or epochs of data. However, a nonlinear comes from the redshift radial velocity being fit inside the linear model log flux model. Because our dataset will consist of different orders at the same time, and the same order at different time, and we are assuming the wavelenght solution to be true. Thus the redshift should be the same at different orders at the same time, so $i_*$ is the index of the epoch while $i$ is the data index, which is different for all orders and times or can be thought of simply as the data index.\n",
    "\n",
    "$$y_s = f(x + \\delta x_{i_*}|\\theta_\\mathrm{s})$$\n",
    "\n",
    "The tellurics model is also high resolution in wavelength and static across all times, but the airmass is used to stretch the model at each model at each time. This parameter is not fit, however is can be if the user chooses to do so!\n",
    "\n",
    "$$y_t = a_i f(x|\\theta_\\mathrm{t})$$\n",
    "\n",
    "And finally the pseudo-normalization model is low resolution in wavelength about one parameter per 50 Angstroms and is different across all times because we expect the detector state to vary greatly with time.\n",
    "\n",
    "$$y_\\mathrm{N} = f(x|\\theta_{\\mathrm{N},i})$$\n",
    "\n",
    "The function we choose to represent the flux of these models is the cardinal spline mixture, who are evenly spaced in wavelength. \n",
    "\n",
    "$$f(x|\\theta,p) = \\sum_i \\theta_i k(x|p,i)$$\n",
    "\n",
    "where $k$ is the cardinal basis/kernel function. This basis function is a piecewise polynomial positioned on an evenly spaced grid such that the total function will be continuous in $p-1$ derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19ccb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jabble.quickplay\n",
    "def get_model(dataset,resolution,p_val,vel_padding,init_rvs,rest_vels,airmass,pts_per_wavelength,norm_p_val):\n",
    "         \n",
    "    dx = jabble.physics.delta_x(2 * resolution)\n",
    "    x_grid = jnp.arange(np.min(np.concatenate(dataset.xs)), np.max(np.concatenate(dataset.xs)), step=dx, dtype=\"float64\")\n",
    "    \n",
    "    model_grid = jabble.model.create_x_grid(\n",
    "        x_grid, vel_padding.to(u.m/u.s).value, 2 * resolution\n",
    "    )  \n",
    "    # model = jabble.quickplay.get_wobble_model(init_rvs, airmass, model_grid, p_val,rest_vels) #+ jabble.quickplay.get_normalization_model(dataset,norm_p_val,pts_per_wavelength)\n",
    "\n",
    "    # model = jabble.quickplay.PseudoNormalModel(init_shifts, init_airmass, model_grid, p_val, dataset, norm_p_val, pts_per_wavelength)\n",
    "    init_shifts = jabble.physics.shifts(init_rvs)\n",
    "    rest_shifts = jabble.physics.shifts(rest_vels)\n",
    "    model = jabble.model.CompositeModel(\n",
    "        [\n",
    "            jabble.model.ShiftingModel(init_shifts,which_key='times'),#ShiftingModel(init_shifts),#\n",
    "            jabble.model.CardinalSplineMixture(model_grid, p_val),\n",
    "        ]\n",
    "    ) + jabble.model.CompositeModel(\n",
    "        [\n",
    "            jabble.model.ShiftingModel(rest_shifts,which_key='times'),\n",
    "            jabble.model.CardinalSplineMixture(model_grid, p_val),\n",
    "            jabble.model.StretchingModel(airmass),\n",
    "        ]\n",
    "    ) #+ jabble.model.get_normalization_model(dataset,norm_p_val,pts_per_wavelength)\n",
    "\n",
    "    # model.to_device(device)\n",
    "\n",
    "    # model.fit(2)\n",
    "    # print(type(model.get_parameters()),model.get_parameters().devices())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061a342-9e58-4016-9d20-d7fdc1970ea9",
   "metadata": {},
   "source": [
    "<h2>Training Cycle</h2>\n",
    "Now we define the fitting process of the parameters of all of these models with respect to the total set of data. This is require some clever initialization, so that we don't end up too far from the solution. First, we fit the parameters of the tellurics, stellar, normalization model. Regularizing the stellar and tellurics components, so that they are close 0. Then we fit the RV alone. Then we optimize everything together with regularization on the stellar and tellurics component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f6704c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cycle(model, dataset, loss, lmb, device_store, device_op, batch_size):\n",
    "    options = {\"maxiter\": 4096}\n",
    "    \n",
    "    # Fit Normalization Template\n",
    "    # model.fix()\n",
    "    # model.fit(2, 1)\n",
    "    # # model.fit(0,1)\n",
    "    # # model.fit(1,1)\n",
    "    # # model.display()\n",
    "    reg_s = lmb*jabble.loss.L2Reg([0,1])\n",
    "    reg_t = lmb*jabble.loss.L2Reg([1,1])\n",
    "    \n",
    "    # res1 = model.optimize(loss, dataset, device_store, device_op, batch_size, options=options)#model.optimize(loss, dataset)\n",
    "    # print(res1)\n",
    "\n",
    "    # Fit Stellar & Telluric Template\n",
    "    model.fix()\n",
    "    model.fit(0,1)\n",
    "    model.fit(1,1)\n",
    "    model.display()\n",
    "    \n",
    "    res1 = model.optimize(loss + reg_s + reg_t, dataset, device_store, device_op, batch_size, options=options)#model.optimize(loss, dataset)\n",
    "    print(res1)\n",
    "    \n",
    "    # Fit RV\n",
    "    model.fix()\n",
    "    model.fit(0,0)\n",
    "    res1 = model.optimize(loss, dataset, device_store, device_op, batch_size, options=options)\n",
    "    print(res1)\n",
    "\n",
    "    # RV Parabola Fit\n",
    "    # model.fix()\n",
    "    # shift_search = jabble.physics.shifts(np.linspace(-50, 50, 100))\n",
    "    # model[0][0].parabola_fit(shift_search, loss, model, dataset, device_op, device_store)\n",
    "    # model.to_device(device_op)\n",
    "    # print(type(model_p[0][0].p))\n",
    "\n",
    "    # Fit Everything\n",
    "    model.fix()\n",
    "    model.fit(0,0)\n",
    "    model.fit(0,1)\n",
    "    model.fit(1,1)\n",
    "\n",
    "    res1 = model.optimize(loss + reg_s + reg_t, dataset, device_store, device_op, batch_size, options=options)#model.optimize(loss, dataset)\n",
    "    print(res1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64d128-5f26-4373-a60c-6a6572ece1ae",
   "metadata": {},
   "source": [
    "Here we define the 'loss' to be the sum of the chi squared of each flux. This is typical choice when using jabble, and why we must define the inverse variance of y, $I_{y}$ in the data object.\n",
    "\n",
    "$$\\mathcal{L}_{\\chi^2} = \\sum I_y (ys - y(xs|\\delta x_{i_*},\\theta_\\mathrm{s},\\theta_\\mathrm{t},\\theta_{\\mathrm{N},i}) )^2$$\n",
    "\n",
    "$xs, ys, I_{y}$ are from the data and are then summed over all unmasked elements.\n",
    "\n",
    "$$\\mathcal{L}_{\\mathrm{reg}} = \\sum \\theta_\\mathrm{s}^2 + \\theta_\\mathrm{t}^2$$\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{\\chi^2} + \\lambda \\mathcal{L}_{\\mathrm{reg}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d730833b-c2f3-40e0-821e-d179c37781b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = jax.devices(\"cpu\")\n",
    "# gpus = jax.devices(\"gpu\")\n",
    "loss = jabble.loss.ChiSquare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383a8f0-c1b4-458d-b6bc-d8d96d9832b3",
   "metadata": {},
   "source": [
    "Here we define the parameters of the instrument that are needed to build our model. The instrument is HARPS, so the resolution is 115,000. The $p$ value for the CSM model on the stellar and telluric components is set to 2. This means just the first derivative of the model will be continuous. And we will add a wavelength padding to either side of the log wavelength grid out to $100$ km/s in log wavelength. And the lambda for the regularization term is defined here as $\\lambda = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437c441f-ed1a-41c0-9d65-5f7f0f6a6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 115_000\n",
    "p_val = 3\n",
    "vel_padding = 100 * u.km / u.s\n",
    "\n",
    "\n",
    "pts_per_wavelengths = [1/25.0]\n",
    "norm_p_vals = [3]\n",
    "lmbs = [1000.0]\n",
    "\n",
    "star_name_b, star_name_p = 'barnards','peg51'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011416e7-f905-41cb-89c1-3f2f38a68bd6",
   "metadata": {},
   "source": [
    "We will not fit the entire spectrum in one pass but we will fit chunks of multiple orders with one RV per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54e0513c-a875-4d66-a9fe-9b4745923690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1]\n",
      " [ 2  3]\n",
      " [ 4  5]\n",
      " [ 6  7]\n",
      " [ 8  9]\n",
      " [10 11]\n",
      " [12 13]\n",
      " [14 15]\n",
      " [16 17]\n",
      " [18 19]\n",
      " [20 21]\n",
      " [22 23]\n",
      " [24 25]\n",
      " [26 27]\n",
      " [28 29]\n",
      " [30 31]\n",
      " [32 33]\n",
      " [34 35]\n",
      " [36 37]\n",
      " [38 39]\n",
      " [40 41]\n",
      " [42 43]\n",
      " [44 45]\n",
      " [46 47]\n",
      " [48 49]\n",
      " [50 51]\n",
      " [52 53]\n",
      " [54 55]\n",
      " [56 57]\n",
      " [58 59]\n",
      " [60 61]\n",
      " [62 63]\n",
      " [64 65]\n",
      " [66 67]\n",
      " [68 69]\n",
      " [70 71]]\n"
     ]
    }
   ],
   "source": [
    "orders_s = np.arange(0,72,dtype=int)[:,None].reshape(-1,2)\n",
    "print(orders_s)\n",
    "# orders_s = [[34]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d401a1c-a929-4772-822d-7c18c0252bd5",
   "metadata": {},
   "source": [
    "Also decide what device you want to store extra data on, and which device to put the model and do the fitting on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6fa5e147-dca0-421f-9606-ac8bce3b3b84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/barnards_e2ds.hdf5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_b.filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a72e0e9b-dd67-486d-aed2-3d1bc20ab00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_store = cpus[0]\n",
    "# device_op = gpus[0]\n",
    "# batch_size = 5000\n",
    "\n",
    "# for orders in orders_s:\n",
    "#     for star_name,file in zip([star_name_b,star_name_p],[file_b,file_p]):\n",
    "#         os.makedirs(os.path.join(out_dir,star_name),exist_ok=True)\n",
    "#         for lmb in lmbs:\n",
    "#             for norm_p_val in norm_p_vals:\n",
    "#                 for pts_per_wavelength in pts_per_wavelengths:\n",
    "#                     data_name = os.path.join(out_dir,star_name,star_name + '_data_o{}_pre_norm.pkl'.format(orders))\n",
    "#                     model_name = os.path.join(out_dir,star_name,star_name + '_model_o{}_pre_norm.pkl'.format(orders))\n",
    "#                     dataset, init_rvs, airmass, full_init_rvs, times_t = get_dataset(file,orders,device_op)\n",
    "\n",
    "#                     # print(init_rvs.shape,full_init_rvs.shape)\n",
    "#                     datablock, metablock, meta_keys = dataset.blockify(device_store,return_keys=True)\n",
    "#                     # print(datablock,metablock)\n",
    "#                     # for datarow in datablock:\n",
    "#                     #     print(datarow)\n",
    "#                     # break\n",
    "\n",
    "#                     rest_rvs = jnp.zeros(init_rvs.shape)\n",
    "#                     model = get_model(dataset,resolution,p_val,vel_padding,-init_rvs,rest_rvs,airmass,pts_per_wavelength,norm_p_val)\n",
    "                    \n",
    "#                     # break\n",
    "#                     model.metadata[\"dataset\"] = file.filename\n",
    "#                     for key in dataset.metadata.keys():\n",
    "#                         model.metadata[key] = dataset.metadata[key]\n",
    "                    \n",
    "#                     model.to_device(device_op)\n",
    "#                     reg_s = lmb*jabble.loss.L2Reg([0,1])\n",
    "#                     reg_t = lmb*jabble.loss.L2Reg([1,1])\n",
    "#                     model = train_cycle(model, dataset, loss, lmb, device_store, device_op, batch_size)\n",
    "#                     # model.display()\n",
    "\n",
    "#                     model.fix()\n",
    "#                     model.fit(0,1)\n",
    "#                     model.fit(1,1)\n",
    "#                     reg_s.ready_indices(model)\n",
    "#                     reg_t.ready_indices(model)\n",
    "#                     jabble.quickplay.save(model,model_name,data_name,data=dataset,shifts=model[0][0].p,device=device_op,loss=loss+reg_s+reg_t)\n",
    "#                     # jabble.quickplay.save(model,model_name,mode=\"pkl\",data=dataset,device=device_op)\n",
    "\n",
    "#                     # with h5py.File(model_name + '.hdf','r') as file:\n",
    "#                     # print(model_name + '.hdf',file.keys())\n",
    "#                     # model = jabble.quickplay.load(model_name + '.hdf','hdf')\n",
    "#                         # for key in file.keys():\n",
    "#                         #     cls = eval('jabble.model.' + key)\n",
    "#                         #     model = cls.load_hdf(cls,file)\n",
    "#                         #     model.display()\n",
    "#                         #     print(model.results)\n",
    "#                         # print(file['AdditiveModel']['[0]CompositeModel']['[0][1]CardinalSplineMixture']['p_val'])\n",
    "#                     # break\n",
    "#                     # jabble.quickplay.load()\n",
    "#                     # jabble.model.save(model_name,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf6e49b4-72fb-4f3d-b00d-df1efbab0db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_deriv_plots(dataset,model,plt_epoch,device,plt_name):\n",
    "    l_width = np.arange(-0.5,0.5,0.2) + 11.5\n",
    "    \n",
    "    fig, axes = plt.subplots(3,figsize=(5,5),sharex='col',\\\n",
    "                             facecolor=(1, 1, 1),height_ratios=[1,1,1],dpi=200)\n",
    "    data, meta, keys = dataset.blockify(device,return_keys=True)\n",
    "\n",
    "    datarow = jabble.loss.dict_ele(data,plt_epoch,device)\n",
    "    metarow = jabble.loss.dict_ele(meta,plt_epoch,device)\n",
    "    \n",
    "    lrange = np.floor(np.exp(datarow['xs']).mean()) + l_width\n",
    "\n",
    "    xplot = np.linspace(np.log(lrange.min()),np.log(lrange.max()),\\\n",
    "                        dataset.xs[plt_epoch].shape[0]*10)\n",
    "\n",
    "    model.fix()\n",
    "    yplot_norm_tot  = model([],xplot,metarow)\n",
    "    yplot_norm_stel = model[0]([],xplot,metarow)\n",
    "    yplot_norm_tell = model[1]([],xplot,metarow)\n",
    "    yhat = model([],dataset.xs[plt_epoch][:],metarow)\n",
    "    \n",
    "    velocity = jabble.physics.velocities(model[0][0].p[plt_epoch])\n",
    "    \n",
    "    # Data\n",
    "    axes[0].errorbar(datarow[\"xs\"][:],datarow[\"ys\"][:],yerr=1/np.sqrt(datarow[\"yivar\"][:]),fmt='ok',zorder=3,alpha=0.5,ms=1)\n",
    "\n",
    "    # Stellar Model        \n",
    "    axes[0].plot(xplot,yplot_norm_stel,'-r',linewidth=1.2,zorder=2,alpha=0.5,ms=6)\n",
    "    # Telluric Model\n",
    "    axes[0].plot(xplot,yplot_norm_tell,'-b',linewidth=1.2,zorder=1,alpha=0.5,ms=6)\n",
    "    axes[0].set_yticks([])\n",
    "    # axes[0].set_ylim(-3,1)\n",
    "    axes[0].set_ylabel('f')\n",
    "    \n",
    "    model.fix()\n",
    "    model.fit(0,0)\n",
    "    deriv_model = jax.jacfwd(model,argnums=0)(model.get_parameters(),datarow['xs'],metarow)\n",
    "    deriv_model_plot = jax.jacfwd(model,argnums=0)(model.get_parameters(),xplot,metarow)\n",
    "    print(deriv_model.shape)\n",
    "    # deriv_axes = axes[0].twinx()\n",
    "    axes[1].plot(xplot,deriv_model_plot[:,plt_epoch],'y',alpha=0.5,ms=6)\n",
    "    axes[1].plot(xplot,np.zeros(xplot.shape[0]),'-k',alpha=0.3,linewidth=1,zorder=-1)\n",
    "    axes[1].set_xlim(np.log(lrange.min()),np.log(lrange.max()))\n",
    "    axes[1].set_yticks([])\n",
    "    axes[1].set_ylabel(' $\\\\frac{\\\\partial f}{\\\\partial \\\\delta x}$')\n",
    "\n",
    "    # info_axes = axes[0].twinx()\n",
    "    axes[2].step(datarow['xs'],deriv_model[:,plt_epoch]*datarow['yivar']*deriv_model[:,plt_epoch],where='mid',alpha=0.5,ms=4,c='g')\n",
    "    axes[2].plot(xplot,np.zeros(xplot.shape[0]),'-k',alpha=0.3,linewidth=1,zorder=-1)\n",
    "\n",
    "    axes[2].set_xlim(np.log(lrange.min()),np.log(lrange.max()))\n",
    "    axes[2].set_yticks([])\n",
    "    axes[2].set_ylabel(' $\\\\frac{\\\\partial f}{\\\\partial \\\\delta x} I_{y} \\\\frac{\\\\partial f}{\\\\partial \\\\delta x}$')\n",
    "    # deriv_axes.set_ylim(-1e4,1e4)\n",
    "\n",
    "    axes[0].set_xlim(xplot.min(),xplot.max())\n",
    "    axes[0].set_xticks(np.log(lrange))\n",
    "    axes[1].set_xticklabels(['{:0.1f}'.format(x) for x in lrange])\n",
    "    axes[0].set_xlim(np.log(lrange.min()),np.log(lrange.max()))\n",
    "        \n",
    "    fig.text(0.5, 0.00, 'Wavelength $[\\AA]$', ha='center')\n",
    "    # fig.text(0.08, 0.5, 'Normalized Flux', va='center', rotation='vertical')\n",
    "    if plt_name is not None:\n",
    "        plt.savefig(os.path.join(out_dir, plt_name),bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b9ccc2b-e07f-4c35-a458-38e76a8ac1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_deriv_plots(all_data[0],all_models[0],40,cpus[0],'info.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ba3d5ff-53ae-4e87-add7-ba9d64b281ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_savable_attr(model):\n",
    "#     for key in model.__dict__:\n",
    "#         # print(key)\n",
    "#         if key[0] != \"_\":\n",
    "#             # print(key)\n",
    "#             if '__len__' in dir(model.__dict__[key]):\n",
    "#                 if len(model.__dict__[key]) != 0:\n",
    "#                     print(key,model.__dict__[key])\n",
    "#                     if key == \"models\":\n",
    "#                        for submodel in model.models:\n",
    "#                             get_savable_attr(submodel)\n",
    "# get_savable_attr(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31963717-fa2c-4afc-af96-9f65403214f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "dir = glob.glob('/scratch/mdd423/wobble_jax/out/25-05-08/barnards/*.pkl')\n",
    "print(len(dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f76c3129-d0ef-4613-bdba-27b19172c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# lines = inspect.getsource(jabble.quickplay.save)\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e1ba0bf-9ac6-4cdb-8689-73124419682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def recurse_hdf(file):\n",
    "#     print(file.keys())\n",
    "#     for key in file.keys():\n",
    "#         # print(key)\n",
    "#         if isinstance(file[key], h5py.Group):\n",
    "            \n",
    "#             recurse_hdf(file[key])\n",
    "#         if key == \"results\":\n",
    "#             print(\"yes\")\n",
    "\n",
    "# def load_hdf(filename):\n",
    "#     with h5py.File(filename,'r') as file: \n",
    "#         print(filename)\n",
    "#         for key in file.keys():\n",
    "#             print(file,key,type(file[key]))\n",
    "#             if isinstance(file[key], h5py.Group):\n",
    "#                 print(key)\n",
    "#                 cls = eval(key)\n",
    "#                 model = cls.load_hdf(cls,file[key])\n",
    "#                 # recurse_hdf(group[key])\n",
    "#     return model\n",
    "# #model_name + '.hdf'\n",
    "# with h5py.File(dir[0],'r') as file: \n",
    "#     # recurse_hdf(dir[0])\n",
    "#     print(file.keys())\n",
    "#     # for key in file.keys():\n",
    "#     #     print(key)\n",
    "#     #     if key in dir(jabble.model):\n",
    "#     #         print(\"yes\")\n",
    "# # load_hdf(dir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0da56aed-4d9d-4eda-8963-0df30bb22874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/mdd423/wobble_jax/out/25-05-12'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c583961-e572-4565-98b0-7e94bc68f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_path = \"/scratch/mdd423/wobble_jax/out/25-05-08/peg51\"\n",
    "dir_files = glob.glob(os.path.join(summary_path,'*_RVS.hdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83d123c7-9b1f-4699-b82a-c20d60842d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([0, 1]), 1.0), (array([2, 3]), 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print([*zip(np.arange(0,4).reshape(2,2),np.ones(2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5348d24d-dc49-4064-955b-289ed0f8230e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_array, all_models, all_rv_array, all_data = jabble.quickplay.load_model_dir(summary_path,dir_files,cpus[0],force_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d61ce28-652f-4a23-8ab0-0fa5fb678772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_rv_array = all_rv_array[np.argsort(all_rv_array['min_order'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5521a705-8b01-46fc-a7b0-d338ce0af735",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rv_array['Loss_Avg'].shape,all_rv_array['RV_all'].shape,all_rv_array['Time_all'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63638949-7f68-428f-af7e-40a96e46a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55534978-a3dd-4a61-a44c-c86f8ce173f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_all_order_plot(times,rv_e,err_e,time_comb,rv_comb,err_comb,targ_time,targ_vel,targ_err,bervs):\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(6, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "    temp_vel  = targ_vel - (targ_vel)\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(targ_time,temp_vel,targ_err,fmt='.g',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "\n",
    "    targ_ind = np.argsort(targ_time)\n",
    "    comb_indi = np.argsort(np.argsort(time_comb))\n",
    "    temp_vel  = rv_comb - targ_vel[targ_ind][comb_indi]\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(time_comb,temp_vel,err_comb,fmt='.r',zorder=5,alpha=0.50,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    # temp_vel = rv_e.mean(axis=0) + bervs\n",
    "    # temp_vel -= temp_vel.mean()\n",
    "    # ax.errorbar(times % period,temp_vel,0.0,fmt='.b',zorder=2,alpha=0.60,ms=2,label='Avg RV')\n",
    "\n",
    "    for i in range(rv_e.shape[0]):\n",
    "        targ_ind = np.argsort(targ_time)\n",
    "        comb_indi = np.argsort(np.argsort(times[i,:]))\n",
    "        temp_vel  = rv_e[i,:] - targ_vel[targ_ind][comb_indi]\n",
    "        temp_vel -= temp_vel.mean()\n",
    "        ax.errorbar(times[i,:],temp_vel,yerr=err_e[i,:],fmt='.k',zorder=2,alpha=0.05,ms=2,label='Order Jabble RV')\n",
    "\n",
    "    # fig.legend()\n",
    "    ax.set_ylim(-500, 500)\n",
    "    # fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star Relative Radial Velocities')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"MJD\")\n",
    "    plt.savefig(os.path.join(out_dir, \"barn_rvs_time.png\"))\n",
    "\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    # plt.savefig(os.path.join(out_dir, \"02-barns_all_order_nobervs_epoch.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def rv_all_order_harps_plot(times,rv_e,err_e,times_comb,rv_comb,err_comb,targ_time,targ_vel,\\\n",
    "                            targ_err,bervs,loss_array,rv_difference_array,star_name):\n",
    "    \n",
    "    epoches_span = np.arange(0,len(times_comb),dtype=int)\n",
    "\n",
    "    # RV Err Comparison\n",
    "    \n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "    targ_ind = np.argsort(targ_time)\n",
    "    comb_indi = np.argsort(np.argsort(times_comb))\n",
    "\n",
    "    bervs_temp = -bervs[targ_ind][comb_indi] + bervs.mean()\n",
    "    norm_vel   = bervs_temp #(targ_vel[targ_ind][comb_indi] - targ_vel[targ_ind][comb_indi].mean())\n",
    "    # print(np.sum(times_comb == targ_time[targ_ind][comb_indi]))\n",
    "\n",
    "    # targ_norm  = (targ_vel[targ_ind][comb_indi] - targ_vel[targ_ind][comb_indi].mean()) - norm_vel\n",
    "    targ_line = ax.plot(epoches_span,targ_err[targ_ind][comb_indi],'.g',zorder=3,alpha=0.5,ms=2,label='HARPS RV')\n",
    "\n",
    "    \n",
    "    # comb_norm = (rv_comb - rv_comb.mean()) - norm_vel\n",
    "    comb_line = ax.plot(epoches_span,err_comb,'.r',zorder=1,alpha=0.3,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    # ax.set_title('Barnard\\'s Star Relative Radial Velocity Error')\n",
    "    ax.set_ylabel(\"RV Error [$m/s$]\")\n",
    "    ax.set_xlabel( \"Epochs\")\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-1e2,1e2)\n",
    "    plt.savefig(os.path.join(out_dir, \"{}_rvs_err_epoch.png\".format(star_name)),bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # RV Comparison\n",
    "    \n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "    targ_ind = np.argsort(targ_time)\n",
    "    comb_indi = np.argsort(np.argsort(times_comb))\n",
    "\n",
    "    bervs_temp = -bervs[targ_ind][comb_indi] + bervs.mean()\n",
    "    norm_vel   = bervs_temp #(targ_vel[targ_ind][comb_indi] - targ_vel[targ_ind][comb_indi].mean())\n",
    "    # print(np.sum(times_comb == targ_time[targ_ind][comb_indi]))\n",
    "\n",
    "    targ_norm  = (targ_vel[targ_ind][comb_indi] - targ_vel[targ_ind][comb_indi].mean()) - norm_vel\n",
    "    targ_line = ax.errorbar(epoches_span,targ_norm,targ_err[targ_ind][comb_indi],fmt='.g',zorder=3,alpha=0.5,ms=2,label='HARPS RV')\n",
    "\n",
    "    \n",
    "    comb_norm = (rv_comb - rv_comb.mean()) - norm_vel\n",
    "    comb_line = ax.errorbar(epoches_span,comb_norm,err_comb,fmt='.r',zorder=2,alpha=0.3,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    # temp_vel = (rv_e.mean(axis=0) - rv_e.mean()) #+ bervs_temp[targ_ind][comb_indi]\n",
    "    # avg_line = ax.errorbar(epoches_span,temp_vel,0.0,fmt='.b',zorder=2,alpha=0.0,ms=2,label='Avg RV')\n",
    "\n",
    "    for i in range(rv_e.shape[0]):\n",
    "        e_ind = np.argsort(times[i,:])\n",
    "        indiv_norm = (rv_e[i,:][e_ind][comb_indi] - rv_e[i,:].mean()) - norm_vel\n",
    "        # times[i,:][e_ind][comb_indi]\n",
    "        err_line = ax.errorbar(epoches_span,indiv_norm,yerr=err_e[i,:][e_ind][comb_indi],fmt='.k',zorder=1,alpha=0.03,ms=2,label='Order Jabble RV')\n",
    "\n",
    "    # ax.set_ylim(-100, 100)\n",
    "    ax.legend(handles=[targ_line,comb_line,err_line],loc=\"upper right\")\n",
    "\n",
    "    # ax.set_title('Barnard\\'s Star Relative Radial Velocities')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"Epochs\")\n",
    "    # ax.set_xlim(2.456e6 + 451,2.456e6+452)\n",
    "    ax.set_ylim(-50,50)\n",
    "    plt.savefig(os.path.join(out_dir, \"{}_rvs_epoch.png\".format(star_name)),bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # FIGURE 2\n",
    "    # fig, ax = plt.subplots(\n",
    "    #     1,\n",
    "    #     figsize=(10, 4),\n",
    "    #     facecolor=(1, 1, 1),\n",
    "    #     dpi=300,\n",
    "    #     sharey=True\n",
    "    # )\n",
    "\n",
    "    # for i in range(rv_e.shape[0]):\n",
    "    #     ax.errorbar(times[i,:],rv_e[i,:],yerr=err_e[i,:],fmt='.k',zorder=1,alpha=0.05,ms=2,label='Order Jabble RV')\n",
    "    # ax.errorbar(times_comb,rv_comb,err_comb,fmt='.r',zorder=2,alpha=0.3,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    # ax.set_ylim(-100, 100)\n",
    "    # ax.set_title('Combination Check')\n",
    "    # ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    # ax.set_xlabel( \"MJD\")\n",
    "    # ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%5.1f'))\n",
    "    # plt.savefig(os.path.join(out_dir, \"{}_rv_comb_check.png\".format(star_name)),bbox_inches='tight')\n",
    "    # plt.show()\n",
    "\n",
    "    # RV DIFFERENCE PLOT\n",
    "    fig, axes = plt.subplots(\n",
    "        3,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "    im1 = axes[0].imshow(rv_difference_array,interpolation ='nearest',vmin=-10,vmax=10)\n",
    "    fig.colorbar(im1,ax=axes[0])\n",
    "    axes[0].set_ylabel('$\\Delta$ RV')\n",
    "    \n",
    "    im2 = axes[1].imshow(err_e,interpolation ='nearest',vmin=0,vmax=3)\n",
    "    fig.colorbar(im2,ax=axes[1])\n",
    "    axes[1].set_ylabel('$\\sigma_{RV}$')\n",
    "    \n",
    "    im3 = axes[2].imshow(rv_difference_array/err_e,interpolation ='nearest',vmin=-3,vmax=3)\n",
    "    print(np.sqrt(np.mean((rv_difference_array/err_e)**2)))\n",
    "    fig.colorbar(im3,ax=axes[2])\n",
    "    axes[2].set_ylabel('$\\Delta$ RV /$\\sigma_{RV}$')\n",
    "    \n",
    "    axes[0].set_xticks([])\n",
    "    axes[1].set_xticks([])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.savefig(os.path.join(out_dir,'{}_drv.png'.format(star_name)),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist((rv_difference_array/err_e).flatten(),bins=30)\n",
    "    plt.title('$\\Delta$ RV /$\\sigma_{RV}$')\n",
    "    plt.show()\n",
    "    \n",
    "    # LOSS ARRAY PLOT\n",
    "    plt.figure(figsize=(12,6),facecolor=(1, 1, 1),dpi=300)\n",
    "    im = plt.imshow(loss_array,interpolation ='nearest',vmin=0,vmax=10000)\n",
    "\n",
    "    xs,ys = np.where(np.isnan(loss_array))\n",
    "    plt.scatter(ys,xs,s=10,facecolors='none', edgecolors='b')\n",
    "\n",
    "    xs,ys = np.where(np.isinf(loss_array))\n",
    "    plt.scatter(ys,xs,s=10,facecolors='none', edgecolors='r')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Chunks')\n",
    "    plt.title('$\\chi^2$')\n",
    "    plt.colorbar(im,shrink=0.3)  \n",
    "    plt.savefig(os.path.join(out_dir,'{}_loss.png'.format(star_name)),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # RV difference divided by RV error\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # loss = np.array([model.results[-1][''] for model in all_models\n",
    "    # loss_mean = np.mean(loss,axis=3).mean(axis=1)\n",
    "    # print(loss_mean.shape)\n",
    "\n",
    "    # fig, ax = plt.subfigures((2,2),figsize=(12,6),facecolor=(1, 1, 1),dpi=300,height_ratios=[1,4],width_ratios=[4,1],sharex='col',sharey='row')\n",
    "    # im = ax[1,0].imshow(loss_mean,interpolation ='nearest',vmin=0,vmax=10)\n",
    "    \n",
    "    \n",
    "    # ax[0,1].axis('off')\n",
    "    # plt.xlabel('epoches')\n",
    "    # plt.ylabel('chunks')\n",
    "    # plt.title('$\\chi^2$')\n",
    "    # plt.colorbar(im,shrink=0.3)\n",
    "    # plt.savefig(os.path.join(out_dir,'barn_obj.png'))\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3f57a3-8d6e-4ba3-ada3-195b9699d753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rv_all_order_plot(all_tim,all_rvs,all_err,\\\n",
    "#                   rv_array['Time_comb'],rv_array['RV_comb'],rv_array['RV_err_comb'],\\\n",
    "#                   np.array(file_b['dates']),np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "#                   np.array(file_b['bervs']))\n",
    "star_name = \"peg51\"\n",
    "rv_all_order_harps_plot(all_rv_array['Time_all'],all_rv_array['RV_all'],all_rv_array['RV_err_all'],\\\n",
    "                        rv_array['Time_comb'],rv_array['RV_comb'],rv_array['RV_err_comb'],\\\n",
    "                        np.array(file_p['dates']),np.array(file_p['pipeline_rvs']),np.array(file_p['pipeline_sigmas']),\\\n",
    "                        np.array(file_p['bervs']),all_rv_array['Loss_Avg'],all_rv_array['RV_difference'],star_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8721c4c-2954-494d-81f6-876c59fdf670",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rv_array.dtype,all_rv_array['RV_all'].shape,all_rv_array['RV_err_all'].shape,\\\n",
    "all_rv_array['Time_all'].shape,all_rv_array['Loss_Avg'].shape,all_rv_array['RV_difference'].shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d56a68-8269-4c90-a831-c0745fd214df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rv_array[\"RV_all\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9b87f-ecd0-4bb0-b908-3a62bf00cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ce084-f74a-4198-8792-5cf1d02c0efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_list = fits.open(\"~/wobble_jax/data/J_ApJ_795_23_table2.dat.fits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d0734d-2a82-49be-b2e9-72ddac76d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_list[0].header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac235039-5ec4-4b07-8467-a404e3fa6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = cm.get_cmap(\"Spectral\")\n",
    "cmap = matplotlib.colormaps[\"Spectral\"]\n",
    "# import cmastro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f8cfc-ea27-4557-b29f-c259884a0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subplot(axes,model,dataset,plt_epoch,device,lrange,line_list = None):\n",
    "\n",
    "    model.fix()\n",
    "    data, meta, keys = dataset.blockify(device,return_keys=True)\n",
    "    \n",
    "    datarow = jabble.loss.dict_ele(data,plt_epoch,device)\n",
    "    metarow = jabble.loss.dict_ele(meta,plt_epoch,device)\n",
    "    # fig.suptitle(\"Order {}\".format(keys[\"orders\"][meta[\"orders\"][plt_epoch]]))\n",
    "    # axes[0,ii].title.set_text('Date: {}'.format(keys[\"times\"][meta[\"times\"][plt_epoch]]))\n",
    "\n",
    "    # print(metarow['index'],plt_epoch)\n",
    "    \n",
    "    xplot = np.linspace(np.log(lrange.min()),np.log(lrange.max()),\\\n",
    "                        dataset.xs[plt_epoch].shape[0]*10)\n",
    "\n",
    "    yplot_norm_tot  = model([],xplot,metarow)\n",
    "    yplot_norm_stel = model[0]([],xplot,metarow)\n",
    "    yplot_norm_tell = model[1]([],xplot,metarow)\n",
    "    # yplot_norm      = model[2]([],xplot,metarow)\n",
    "    # for epoch in np.where(indices):\n",
    "    yhat = model([],dataset.xs[plt_epoch][:],metarow)\n",
    "    axes[0].set_xlim(xplot.min(),xplot.max())\n",
    "\n",
    "    velocity = jabble.physics.velocities(model[0][0].p[plt_epoch])\n",
    "    \n",
    "    # Data\n",
    "    # print(datarow)\n",
    "    axes[0].errorbar(datarow[\"xs\"][:],datarow[\"ys\"][:],yerr=1/np.sqrt(datarow[\"yivar\"][:]),fmt='.k',zorder=2,alpha=0.1,ms=5)\n",
    "\n",
    "    # Stellar Model        \n",
    "    axes[0].plot(xplot,yplot_norm_stel,'-r',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "    # Telluric Model\n",
    "    axes[0].plot(xplot,yplot_norm_tell,'-b',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "    # Total\n",
    "    # axes[0].plot(xplot,yplot_norm_tot,'-m',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "    # Norm\n",
    "    # axes[0,ii].plot(xplot,yplot_norm,'-g',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "    # Theory Model\n",
    "    # theory_ax = axes[0,ii].twinx()\n",
    "    # theory_ax.plot(dataset_theory.xs[0][:],dataset_theory.ys[0][:],'-y',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "    # theory_ax.set_ylim(-5,5)\n",
    "    # Line List\n",
    " \n",
    "    # Residuals\n",
    "    axes[1].step(dataset.xs[plt_epoch],dataset.ys[plt_epoch] - yhat,\\\n",
    "                             'k',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "\n",
    "    axes[0].text(np.log(lrange.min()+0.1),0.3,\"Order: ${}$\".format(model.metadata[\"orders\"][plt_epoch]))\n",
    "    # axes[0].text(np.log(lrange.max()-0.1),0.3,\"MJD: ${}$\".format(model.metadata[\"times\"][plt_epoch]))\n",
    "\n",
    "    axes[0].set_ylim(-2.5,0.5)\n",
    "    axes[1].set_ylim(-1,1)\n",
    "    \n",
    "    # axes[0].set_xticks([])\n",
    "    axes[0].set_xticks(np.log(lrange))\n",
    "    axes[1].set_xticks(np.log(lrange))\n",
    "\n",
    "    axes[0].set_xticklabels(['' for x in lrange])\n",
    "    axes[1].set_xticklabels(['{:0.1f}'.format(x) for x in lrange])\n",
    "    \n",
    "    axes[0].set_xlim(np.log(lrange.min()),np.log(lrange.max()))\n",
    "    axes[1].set_xlim(np.log(lrange.min()),np.log(lrange.max()))\n",
    "\n",
    "    return axes\n",
    "\n",
    "def make_grid_plots(datasets,models,size_n,size_m,plt_epochs,device,plt_name,line_list=None):\n",
    "    l_width = np.arange(-4,4,2)\n",
    "    \n",
    "    fig, axes = plt.subplots(2*size_n,size_m,figsize=(5*size_m,5*size_n),sharey='row',\\\n",
    "                             facecolor=(1, 1, 1),height_ratios=[4,1]*size_n,dpi=200)\n",
    "    # plt.tight_layout()\n",
    "\n",
    "    for ii, nn in enumerate(range(size_n)):\n",
    "        for jj, mm in enumerate(range(size_m)):\n",
    "            this_index = size_m*ii+jj\n",
    "            \n",
    "            lrange = np.floor(np.exp(datasets[this_index].xs[plt_epochs[this_index]]).mean()) + l_width\n",
    "            # print(lrange)\n",
    "            axes[2*ii:((2*ii)+2),jj] = make_subplot(axes[2*ii:((2*ii)+2),jj],models[this_index],datasets[this_index],0,device,lrange)\n",
    "            if line_list is not None:\n",
    "                plot_line_list(axes[2*ii:((2*ii)+2),jj],line_list,lrange)\n",
    "\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.15)\n",
    "    # fig.text(0.5, 0.00, 'Wavelength $[\\AA]$', ha='center')\n",
    "    # fig.text(0.08, 0.5, 'Normalized Flux', va='center', rotation='vertical')\n",
    "    if plt_name is not None:\n",
    "        plt.savefig(os.path.join(out_dir, plt_name),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_order_plot(dataset,model,lrange,plt_epoches,device,plt_name=None,line_list=None):\n",
    "    # model = jabble.model.load(model_name)\n",
    "    data_orders = np.unique(model.metadata[\"orders\"])\n",
    "    model.fix()\n",
    "        \n",
    "    fig, axes = plt.subplots(2,len(plt_epoches),figsize=(4*len(plt_epoches),4),sharex='col',sharey='row',\\\n",
    "                             facecolor=(1, 1, 1),height_ratios=[4,1],dpi=200)\n",
    "\n",
    "    for ii, plt_epoch in enumerate(plt_epoches):\n",
    "        axes[:,ii] = make_subplot(axes[:,ii],model,dataset,plt_epoch,device,lrange[ii,:])#(axes,model,dataset,plt_epoch,device,lrange)\n",
    "        if line_list is not None:\n",
    "            plot_line_list(axes[:,ii],model,line_list,lrange[ii,:],plt_epoch)\n",
    "    \n",
    "    # plt.x\n",
    "    # plt.text(1, 1, 'Wavelength ($\\AA$)', ha='center')\n",
    "    # plt_name = os.path.join(out_dir, \"02-spectra_{}_{}-{}.png\".format(os.path.split(model_name)[-1],lmin,lmax))\n",
    "    # plt.savefig(plt_name,dpi=200,bbox_inches='tight')\n",
    "    # fig.suptitle('Barnards Star')\n",
    "    # fig.text(0.5, 0.00, 'Wavelength $[\\AA]$', ha='center')\n",
    "    # fig.text(0.08, 0.5, 'Normalized Flux', va='center', rotation='vertical')\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    if plt_name is not None:\n",
    "        plt.savefig(os.path.join(out_dir, plt_name),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_line_list(axes,model,line_list,lrange,plt_epoch):\n",
    "    for line in line_list[1].data[(line_list[1].data[\"Wave\"] > lrange.min()) * (line_list[1].data[\"Wave\"] < lrange.max())]:\n",
    "        print(line[\"Species\"])\n",
    "        axes[0].axvline(np.log(line[\"Wave\"]) + model[0][0].p[plt_epoch],-5,5,c='k',linestyle='dashed',alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729ce45-8adc-408f-b219-ec959d26f839",
   "metadata": {},
   "source": [
    "0.025 stellar radii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ca706-447b-4ff0-bb71-7da5b6ae7c1c",
   "metadata": {},
   "source": [
    "$1/0.025$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d759f5c-9f59-425f-b8c5-2a847897310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind_array = np.array(np.where(all_rv_array['RV_err_all'] > 1e3))\n",
    "# ind_model_array = np.unique(ind_array[0])\n",
    "# print(ind_model_array)\n",
    "# for model_index in ind_model_array:\n",
    "#     def something(rv_inds):\n",
    "#         if len(rv_inds) == 1:\n",
    "#             rv_inds = np.append(rv_inds,rv_inds[0]+1)\n",
    "#         orders = np.unique(all_models[model_index].metadata['orders'])\n",
    "#         ls_means = np.ceil([np.exp(all_data[model_index].xs[xxx][:]) for xxx in rv_inds]).min(axis=1)\n",
    "        \n",
    "#         lrange = ls_means[:,None] + np.arange(-5,5,4)[None,:]\n",
    "#         print(lrange)\n",
    "#         plt_name = \"barnards_spectra_o{}_rv{}.png\".format(orders,rv_inds)\n",
    "#         make_order_plot(all_data[model_index],all_models[model_index],lrange,rv_inds,cpus[0],plt_name)\n",
    "#     rv_inds = ind_array[1,ind_array[0,:] == model_index]\n",
    "    \n",
    "#     row_len = 4\n",
    "#     if len(rv_inds) > row_len:\n",
    "        \n",
    "#         rounds = len(rv_inds)//row_len\n",
    "#         for rv_inds_i in range(rounds):\n",
    "#             something(rv_inds[rv_inds_i*row_len:np.min([(rv_inds_i+1)*row_len,len(rv_inds)-1])])\n",
    "#             if rv_inds_i  > 3:\n",
    "#                 break\n",
    "#     else:\n",
    "#         something(rv_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd2228-c037-4174-8304-3835271cd261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (datasets,models,size_n,size_m,plt_epochs,orders)\n",
    "order_list = [np.min(model.metadata['orders']) for model in all_models]\n",
    "# print(order_list)\n",
    "ordered_order_list = np.array(np.argsort(order_list))\n",
    "# print(ordered_order_list)\n",
    "\n",
    "plt_epochs = np.arange(0,36,1,dtype=int)\n",
    "make_grid_plots([all_data[xx] for xx in ordered_order_list],[all_models[xx] for xx in ordered_order_list],5,4,plt_epochs,cpus[0],\\\n",
    "                plt_name=\"peg51_pre_norm_{}-{}.png\".format(plt_epochs.min(),plt_epochs.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac48c05-d68b-446e-996a-695617979741",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_deriv_plots(all_data[0],all_models[0],0,cpus[0],'info.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ac635-f8cf-434d-bfa2-619826f31e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 20\n",
    "rv_inds = [-2,-1,0,1]\n",
    "# rv_data = jabble.physics.velocities(all_models[jjj][0][0].p)\n",
    "print(all_models[model_index].metadata.keys())\n",
    "orders = np.unique(all_models[model_index].metadata['orders'])\n",
    "\n",
    "# dataset, _, _, _, _ = get_dataset(file_b,orders,cpus[0])\n",
    "ls_means = np.ceil([np.exp(all_data[model_index].xs[xxx][:]) for xxx in rv_inds]).mean(axis=1)\n",
    "lrange = ls_means[:,None] + np.arange(-5,5,2)[None,:]\n",
    "# print(lrange)\n",
    "# lrange = np.repeat(lrange,len(rv_inds),axis=0) + np.zeros(len(rv_inds))[:,None]\n",
    "plt_name = \"51peg_spectra_{}.png\".format(orders)\n",
    "print(lrange)\n",
    "make_order_plot(all_data[model_index],all_models[model_index],lrange,rv_inds,cpus[0],plt_name)\n",
    "#(dataset,model,lrange,rv_inds,device,plt_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553515a0-2825-473a-91ea-cb96c99abbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8c11e-9e64-4589-b19a-47a5a3e57b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "line_fits = fits.open('~/wobble_jax/data/J_ApJ_795_23_table2.dat.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ca864-9ca4-4a86-883c-da3e51311e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_smooth(x,sigma):\n",
    "\n",
    "    return np.exp(-x**2/(2*sigma**2))\n",
    "\n",
    "def convolve_load(file,sigma_l,xl,eval_a):\n",
    "    # This reads in line by line a phoenix model\n",
    "    # storing flux and x values to the cache around the grid point\n",
    "    # then convolves to instrument resolution\n",
    "    \n",
    "    xl_ii   = 0\n",
    "    xl_curr = xl[xl_ii]\n",
    "    fl = np.zeros(xl.shape)\n",
    "\n",
    "    x_cache = np.array([])\n",
    "    f_cache = np.array([])\n",
    "\n",
    "    # xh = []\n",
    "    # fh = []\n",
    "    with open(file,'r') as file_stream:\n",
    "        cnt = 0\n",
    "        for line in file_stream:\n",
    "            cnt += 1\n",
    "            if cnt > 8:\n",
    "                words = line.split('    ')\n",
    "                for word in words:\n",
    "                    try:\n",
    "                        x_val = np.log(np.double(word))\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                        \n",
    "                if x_val > (xl_curr - eval_a):\n",
    "                    x_cache = np.append(x_cache,x_val)\n",
    "                    # xh.append(x_val)\n",
    "                    for word in words[::-1]:  \n",
    "                        try:\n",
    "                            f_cache = np.append(f_cache,np.double(word))\n",
    "                            # fh.append(np.double(word))\n",
    "                            break\n",
    "                        except ValueError:\n",
    "                            pass\n",
    "                if x_val > (xl_curr + eval_a):\n",
    "                    kern = gaussian_smooth(x_cache - xl_curr,sigma_l)\n",
    "                    if xl_ii+1 >= len(xl):\n",
    "                        break\n",
    "                    else:\n",
    "                        fl[xl_ii] = np.dot(f_cache,kern)/np.sum(kern)\n",
    "                        xl_ii += 1\n",
    "                        xl_curr = xl[xl_ii]\n",
    "                        # print(x_cache.shape,f_c\n",
    "                        f_cache = f_cache[x_cache > xl_curr - eval_a]\n",
    "                        x_cache = x_cache[x_cache > xl_curr - eval_a]\n",
    "                        \n",
    "\n",
    "    return fl\n",
    "                    \n",
    "# x_cache = np.concatenate((x_cache,[np.double(word)]))\n",
    "low_resolution = 115_000\n",
    "x_min = np.log(4000)\n",
    "x_max = np.log(4200)\n",
    "sigma_l = jabble.physics.delta_x(low_resolution)\n",
    "\n",
    "xl = np.arange(x_min,x_max,sigma_l/2)\n",
    "fl = convolve_load('../../wobble_jax/data/models_1721076003/bt-settl/lte032-5.0-0.5a+0.2.BT-NextGen.7.dat.txt',\\\n",
    "                       sigma_l,xl,3*sigma_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288fa860-41af-4dfa-bad9-266c609d6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(xl,fl,label='low',alpha=0.3)\n",
    "# plt.plot(xl,fl,marker='.',label='low',alpha=0.4)\n",
    "# plt.plot(xh,fh,label='high',alpha=0.3)\n",
    "# # plt.plot(xl.mean() + x_space,y_space,label='kernel')\n",
    "# plt.xlim(np.log(4010),np.log(4020))\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf5fdb-b2a9-4296-9ab8-c3bbc085b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nifty_ls\n",
    "import nifty_ls\n",
    "from astropy.timeseries import LombScargle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b7433-0833-49d0-9b6f-f45fb88b38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nifty_ls_diagram(dates,rvs,error,title,plt_name):\n",
    "#     # print(dates)\n",
    "#     nifty_res = nifty_ls.lombscargle(dates, rvs, dy=error, fmin=0.01, fmax=0.3)\n",
    "\n",
    "#     # help(nifty_res)\n",
    "#     plt.figure(figsize=(10,4))\n",
    "#     plt.plot(1/nifty_res.freq(),nifty_res.power,'-m',markersize=0.2,alpha=0.7)\n",
    "#     plt.xlabel('Period (Days)')\n",
    "#     plt.ylabel('Power')\n",
    "#     plt.title(title)\n",
    "#     plt.ylim(0.0,0.3)\n",
    "#     plt.savefig(os.path.join(out_dir, plt_name))\n",
    "#     plt.show()\n",
    "\n",
    "# # print(np.array(file_b['dates']).shape,np.array(jabble_file['RV']).shape,np.array(jabble_file['RV_err']).shape)\n",
    "# # print(np.array(file_b['dates']).shape,np.array(jabble_file['RV_comb']).shape,np.array(jabble_file['RV_comb_err']).shape)\n",
    "# plt_name = 'ls_jabble_noberv.png'\n",
    "# nifty_ls_diagram(np.array(file_b['dates']),rv_array['RV_comb'] + np.array(file_b['bervs']),rv_array['RV_err_comb'],title=\"Jabble - BERV\",plt_name=plt_name)\n",
    "# plt_name = 'ls_hires_noberv.png'\n",
    "# nifty_ls_diagram(np.array(file_b['dates']),np.array(file_b['pipeline_rvs']) + np.array(file_b['bervs']),np.array(file_b['pipeline_sigmas']),title=\"HIRES - BERV\",plt_name=plt_name)\n",
    "# plt_name = 'ls_berv.png'\n",
    "# nifty_ls_diagram(np.array(file_b['dates']),np.array(file_b['bervs']),None,title=\"BERVS\",plt_name=plt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2665fcf-7af9-47c1-a391-6751b0ac23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_models[-1].results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad513e-7026-43c5-bb72-cf52acda7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency, power = LombScargle(np.array(file_b['dates']), jabble_file['RV_comb']).autopower(method=\"fastnifty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda82484-10f4-482d-a4e8-7afe7a01c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_earth_residual_img(model,dataset,lrange,orders,rest_shifts,residual_resolution,plt_name,line_list,device):\n",
    "    xrange = np.log(lrange)\n",
    "    xmin, xmax = np.min(xrange), np.max(xrange)\n",
    "    # xinds = ((dataset[0].xs[:] < xmax) * (dataset[0].xs[:] > xmin)).astype(bool)\n",
    "    data, meta, keys = dataset.blockify(device,return_keys=True)\n",
    "\n",
    "    # create residual image of the size of the number of data epochs times the number of orders\n",
    "    residual_img = np.zeros((np.sum([np.sum(keys[\"orders\"][meta[\"orders\"]] == order) for order in orders]),residual_resolution))\n",
    "    \n",
    "    fig, ax = plt.subplots(2,2,figsize=(8, 8),height_ratios=[1,4],width_ratios=[4,1],sharex='col',sharey='row')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    max_shift, min_shift = np.max(rest_shifts), np.min(rest_shifts)\n",
    "    new_grid = np.linspace(xmin,xmax,residual_resolution)\n",
    "\n",
    "    epsilon = np.log(np.mean(lrange) + 1) - np.log(np.mean(lrange))\n",
    "    model.fix()\n",
    "    model.display()\n",
    "\n",
    "    # print(np.sum(np.log(line_list[1].data[\"Wave\"]) > xmin),np.sum(np.log(line_list[1].data[\"Wave\"]) < xmax),\\\n",
    "    #      np.sum((np.log(line_list[1].data[\"Wave\"]) > xmin)*(np.log(line_list[1].data[\"Wave\"]) < xmax)))\n",
    "    # for line in line_list[1].data[(np.log(line_list[1].data[\"Wave\"]) > xmin) * (np.log(line_list[1].data[\"Wave\"]) < xmax)]:\n",
    "    #     print(line[\"Species\"])\n",
    "    #     ax[1,0].vlines(np.log(line[\"Wave\"]),0,len(dataset))\n",
    "    # for line in list_list\n",
    "\n",
    "    plt_epochs = np.concatenate([np.array(np.where(keys[\"orders\"][meta[\"orders\"]] == order)).flatten() for order in orders])\n",
    "    for i,plt_epoch in enumerate(plt_epochs):\n",
    "        datarow = jabble.loss.dict_ele(data,plt_epoch,device)\n",
    "        metarow = jabble.loss.dict_ele(meta,plt_epoch,device)\n",
    "        \n",
    "        xless = (dataset[plt_epoch].xs[:] <= (xmax + epsilon))\n",
    "        xmore = (dataset[plt_epoch].xs[:] >= (xmin - epsilon)) #+ rest_shifts[plt_epoch] \n",
    "        xinds = (xless * \\\n",
    "                 xmore).astype(bool)\n",
    "        # print(np.sum(xmore),np.sum(xless))\n",
    "        x_grid = dataset[plt_epoch].xs[(~dataset[plt_epoch].mask)*xinds]\n",
    "        y_grid = dataset[plt_epoch].ys[(~dataset[plt_epoch].mask)*xinds]\n",
    "        residual = (y_grid - model([],x_grid,metarow))#*jnp.sqrt(dataset[plt_epoch].yivar[(~dataset[plt_epoch].mask)*xinds])\n",
    "\n",
    "        if np.sum(xless) == 0:\n",
    "            x_grid = np.array([dataset[plt_epoch].xs[~dataset[plt_epoch].mask].min()])\n",
    "            residual = np.array([0.0])\n",
    "        if np.sum(xmore) == 0:\n",
    "            x_grid = np.array([dataset[plt_epoch].xs[~dataset[plt_epoch].mask].max()])\n",
    "            residual = np.array([0.0])\n",
    "        # print(residual.shape,np.sum(dataset[plt_epoch].mask*xinds),np.sum(xinds),np.sum(dataset[plt_epoch].mask))\n",
    "        residual_img[i,:] = scipy.interpolate.interp1d(x_grid,residual,kind='nearest',bounds_error=False,fill_value=0.0)(new_grid )#+ \\rest_shifts[plt_epoch]\n",
    "    cmap = plt.get_cmap(\"RdBu\")\n",
    "\n",
    "    ax[0,1].axis('off')\n",
    "    ax[0,0].step(new_grid,       np.sqrt((residual_img**2).mean(axis=0)),'k',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "    # ax[0,0].step(new_grid,       (residual_img).sum(axis=0),'m',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "    ax[1,1].step(np.sqrt((residual_img**2).mean(axis=1)),np.arange(len(plt_epochs))[::-1],'k',where='post',zorder=1,alpha=0.3,ms=3)\n",
    "    \n",
    "    \n",
    "    # ax[1,0].set_ylim(0,np.max(orders)+1)\n",
    "    ax[1,0].set_xlim(xmin,xmax)\n",
    "    extent = [xmin,xmax,0,len(dataset)+1]\n",
    "    ax[1,0].imshow(residual_img,cmap=cmap,aspect=\"auto\",vmin=-0.1,vmax=0.1,extent=extent,interpolation='nearest')\n",
    "    ax[1,0].set_xlabel('Wavelength [$\\AA$]')\n",
    "    ax[1,0].set_ylabel('Chunks')\n",
    "    # # plt.xticks([])\n",
    "    ax[1,0].set_xticks(xrange)\n",
    "    ax[1,0].set_xticklabels(['{:0.1f}'.format(l) for l in lrange])\n",
    "    # ax[1,0].get_shared_x_axes().join(ax[1,0], ax[1,1])\n",
    "    # plt.xlabel()\n",
    "    if plt_name is not None:\n",
    "        plt.savefig(os.path.join(out_dir, plt_name))\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # worst_epochs = np.zeros(len(orders),dtype=bool)\n",
    "    # worst_epochs = (residual_img**2).sum(axis=1) > 0.7\n",
    "    # print(worst_epochs)\n",
    "    # print(orders)\n",
    "    # print(orders[worst_epochs])\n",
    "    # return orders[worst_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00bf0e-165a-4c65-a914-c9534302e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_min = np.exp(np.array([np.min([np.min(xs) for xs in dataset.xs]) for dataset in all_data]))\n",
    "l_max = np.exp(np.array([np.max([np.max(xs) for xs in dataset.xs]) for dataset in all_data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6c5d3-23ca-4834-ae09-9c08d2b4d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_midpoint = 5677.000\n",
    "lrange = np.ceil(ls_midpoint) + np.arange(-5.0,5.0,2.0)\n",
    "model_index = np.where((ls_midpoint > l_min)*(ls_midpoint < l_max))[0][0]\n",
    "print(model_index)\n",
    "\n",
    "# orders = np.unique(all_models[model_index].metadata['orders'])\n",
    "\n",
    "# data, meta = dataset.blockify(cpus[0])\n",
    "rest_shifts = all_models[model_index][0][0].p\n",
    "index_sort = np.argsort(rest_shifts)\n",
    "residual_resolution = 256\n",
    "orders_plt = np.unique(all_models[model_index].metadata['orders'])\n",
    "plt_name = \"res_img_barn_o{}-{}_l{}-{}_.png\".format(np.min(orders_plt),np.max(orders_plt),np.min(lrange),np.max(lrange))\n",
    "plot_earth_residual_img(all_models[model_index],all_data[model_index],lrange,orders_plt,\\\n",
    "                        rest_shifts,residual_resolution,plt_name,None,device=cpus[0])\n",
    "\n",
    "rv_inds = [-1,-2,0,1]\n",
    "lrange_stack = np.ones(len(rv_inds))[:,None] * lrange[None,:]\n",
    "plt_name = \"spectra_barn_o{}-{}_l{}-{}_.png\".format(orders_plt.min(),orders_plt.max(),np.min(lrange),np.max(lrange))\n",
    "# make_order_plot(all_data[model_index],all_models[model_index],lrange,rv_inds,cpus[0],plt_name)\n",
    "shift_by_eye = 0#np.log(5001/5000)\n",
    "make_order_plot(all_data[model_index],all_models[model_index],lrange_stack,rv_inds,cpus[0],plt_name,line_list=line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4ae3a-615b-4c5a-9a46-c1f636ed3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmin = 5580\n",
    "# lmax = 5585\n",
    "# lspacing = 1\n",
    "# residual_resolution = 2048\n",
    "# lrange = np.arange(lmin,lmax+lspacing,lspacing)\n",
    "# plt_name =  \"02-{}_res_img_starref.png\".format(os.path.split(model_name_b)[-1],np.min(orders),np.max(orders))\n",
    "\n",
    "# rest_vel = model_b[0][0].p#np.zeros(.shape)\n",
    "# index_sort = np.argsort(rest_vel)\n",
    "# rest_shifts =  model_b[0][0].p#jabble.physics.shifts(rest_vel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc66d8a-0c7f-4899-8ffe-1b68a98e21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in dir:\n",
    "#     model = jabble.model.load(filename)\n",
    "#     rv_data = jabble.physics.velocities(model[0][0].p)\n",
    "#     orders = np.unique(model.metadata['orders'])\n",
    "#     lmean = np.mean(np.exp(model[0][1].xs))\n",
    "#     lmin = lmean-10\n",
    "#     lmax = lmean+10\n",
    "#     lspace = 4\n",
    "    \n",
    "#     lrange = np.arange(lmin,lmax+lspace,lspace)\n",
    "    \n",
    "#     rv_inds = [-1,0,1]\n",
    "#     make_order_plot(file_b,model,lmin,lmax,lrange,[orders[1]],rv_inds,cpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d43831-f646-436d-8765-64a434e0b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def imshow_rv_difference(all_models,times_t,times_e,rv_e,err_e,bervs,):\n",
    "    \n",
    "#     rv_array = np.zeros((len(all_models),*all_models[0][0][0].p.shape))\n",
    "#     order_array = []\n",
    "\n",
    "#     info_e = 1/err_e**2\n",
    "#     epoches_span_e = np.zeros(len(rv_e))\n",
    "\n",
    "#     rv_comb = jnp.array(len(times_t))\n",
    "#     for iii,time in enumerate(times_t):\n",
    "#         indices = times_e == time\n",
    "\n",
    "#         rv_indiv[indices] = rv_e[indices] + bervs[iii] - correct_vel[iii]\n",
    "#         epoches_span_e[indices] = iii\n",
    "#         rv_comb = rv_comb.at[iii].set(jnp.dot(info_e[indices],rv_e[indices])/jnp.sum(info_e[indices]))\n",
    "#         info_comb = info_comb.at[iii].set((jnp.dot(info_e[indices],rv_e[indices]**2)/jnp.sum(info_e[indices])) - rv_comb[iii]**2)\n",
    "\n",
    "\n",
    "#     for i,model in enumerate(all_models):\n",
    "#         # data, _, _, full_init_shifts, _ = get_dataset(file_b,orders,cpus[0])\n",
    "#         datablock, metablock, keys = data.blockify(cpus[0],return_keys=True)\n",
    "#         rv_array[i,:] = jabble.physics.velocities(model[0][0].p)\n",
    "#         order_array.append(np.unique(model.metadata['orders']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7611249-62d4-40b9-a5db-3fa33b5e31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow_rv_difference(all_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6654272-8ed1-4917-ba8d-8c50ba250662",
   "metadata": {},
   "source": [
    "5577 night sky line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8278831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = 'barnardsvmapmodel1.mdl'\n",
    "# # model = jabble.model.load(modelname)\n",
    "# jabble.model.save(modelname,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef9439-9570-4979-8330-be3ef582835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_plot(model,dataset,init_shifts,filename):\n",
    "#     x_window = np.log(4550) - np.log(4549)\n",
    "#     lmin = np.exp(dataset.xs[0,500])\n",
    "#     lmax = np.exp(dataset.xs[0,1500])\n",
    "#     lrange = np.arange(lmin,lmax,5)\n",
    "#     plt_unit = u.Angstrom\n",
    "#     epoches = 25\n",
    "#     r_plots = 5\n",
    "\n",
    "#     vel_epoch = 5\n",
    "#     fig, axes = plt.subplots(\n",
    "#         epoches // r_plots,\n",
    "#         r_plots,\n",
    "#         figsize=(8, 8),\n",
    "#         sharex=False,\n",
    "#         sharey=True,\n",
    "#         facecolor=(1, 1, 1),\n",
    "#         dpi=200,\n",
    "#     )\n",
    "#     # fig.suptitle(filenames[model_num])\n",
    "#     for plt_epoch in range((epoches // r_plots) * r_plots):\n",
    "#         xplot = np.linspace(np.log(lmin), np.log(lmax), dataset.xs.shape[1] * 10)\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xlim(\n",
    "#             xplot.min() + model[0][0].p[plt_epoch],\n",
    "#             xplot.max() + model[0][0].p[plt_epoch],\n",
    "#         )\n",
    "\n",
    "#         # model_set[model_num].fix()\n",
    "#         # model_set[model_num].fit(0)\n",
    "#         # rv_model_deriv = jax.jacfwd(model_set[model_num], argnums=0)(model_set[model_num].get_parameters(),dataset.xs[plt_epoch,:],plt_epoch)\n",
    "#         # rv_loss_deriv = jax.jacfwd(loss, argnums=0)(model_set[model_num].get_parameters(),datasets[0],vel_epoch,model_set[model_num])\n",
    "\n",
    "#         model.fix()\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].errorbar(\n",
    "#             dataset.xs[plt_epoch, :],\n",
    "#             dataset.ys[plt_epoch, :],\n",
    "#             dataset.yerr[plt_epoch, :],\n",
    "#             fmt=\".k\",\n",
    "#             elinewidth=1.2,\n",
    "#             zorder=1,\n",
    "#             alpha=0.5,\n",
    "#             ms=3,\n",
    "#         )\n",
    "\n",
    "#         # true_model.fix()\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(\n",
    "#             xplot,\n",
    "#             model([], xplot, plt_epoch),\n",
    "#             \"-r\",\n",
    "#             linewidth=1.2,\n",
    "#             zorder=2,\n",
    "#             alpha=0.5,\n",
    "#             ms=6,\n",
    "#         )\n",
    "#         # axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(xplot,true_model([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=1,alpha=0.5,ms=6)\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_ylim(-2, 1)\n",
    "#         #         axes[i,j].set_yticks([])\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xticks(np.log(lrange))\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xticklabels(\n",
    "#             [\"{:2.0f}\".format(x) for x in lrange]\n",
    "#         )\n",
    "\n",
    "#         res_ax = axes[plt_epoch // r_plots, plt_epoch % r_plots].twinx()\n",
    "#         residual = loss(\n",
    "#             model.get_parameters(),\n",
    "#             dataset,\n",
    "#             plt_epoch,\n",
    "#             model,\n",
    "#         )\n",
    "#         res_ax.step(\n",
    "#             dataset.xs[plt_epoch, :], residual, where=\"mid\", alpha=0.3, label=\"residual\"\n",
    "#         )\n",
    "#         res_ax.set_ylim(0.0, 20)\n",
    "#         res_ax.set_yticks([])\n",
    "#         # res_ax.step(model_set[i][j][1].xs+model_set[i][j][0].p[plt_epoch],\\\n",
    "#         #             model_set[i][j].results[-2]['grad'][:],\\\n",
    "#         #             where='mid',alpha=0.4,label='residual',zorder=-1)\n",
    "#         # res_ax.set_yticks([])\n",
    "\n",
    "#         # res_ax.step(x_grid,\\\n",
    "#         #             rv_model_deriv[:,plt_epoch],\\\n",
    "#         #             where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "\n",
    "#         #     res_ax.step(x_grid,\\\n",
    "#         #                 rv_loss_deriv[:,plt_epoch],\\\n",
    "#         #                 where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "\n",
    "#         #     align_yaxis(, 0, , 0)\n",
    "\n",
    "#         align.yaxes(\n",
    "#             axes[plt_epoch // r_plots, plt_epoch % r_plots], 0.0, res_ax, 0.0, 2.0 / 3.0\n",
    "#         )\n",
    "\n",
    "#     # res.get_shared_y_axes().join(ax1, ax3)\n",
    "#     fig.text(0.5, 0.04, \"$\\lambda$\", ha=\"center\")\n",
    "#     fig.text(0.04, 0.5, \"y\", va=\"center\", rotation=\"vertical\")\n",
    "#     # fig.text(0.96, 0.5, '$d \\L /d \\delta x$', va='center', rotation=270)\n",
    "#     # fig.text(0.96, 0.5, '$d f_{{{ji}}} /d \\delta x_k$', va='center', rotation=270)\n",
    "#     fig.text(0.96, 0.5, \"residuals\", va=\"center\", rotation=270)\n",
    "\n",
    "#     plt.savefig(\n",
    "#         os.path.join(out_dir, \"02-res_{}.png\".format(filename)),\n",
    "#         dpi=300,\n",
    "#         bbox_inches=\"tight\",\n",
    "#     )\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8f735-8aca-4ee1-b4da-ea56f90eaa10",
   "metadata": {},
   "source": [
    "6563 h alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bae852-c141-4694-bc48-e9782d1cdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames = ['51peg','barnards']\n",
    "# make_plot(model_b,dataset_b,shifts_b,filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3af0c-2dd6-407c-a897-9559c565d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/models_1721076003/bt-settl/lte032-5.0-0.5a+0.2.BT-NextGen.7.dat.txt', 'r') as f:\n",
    "#     # text = f.read()\n",
    "#     cnt = 0\n",
    "#     for line in f:\n",
    "        \n",
    "#         cnt += 1\n",
    "#         if cnt > 8:\n",
    "#             print([np.double(x) for x in line.split(\"    \")[-2:]])\n",
    "\n",
    "#         if cnt > 12:\n",
    "#             break\n",
    "        \n",
    "\n",
    "# # print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f721096-7dd7-4376-93e7-8b41bfadab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_better_plot(model_set,datasets,file_set):\n",
    "    \n",
    "\n",
    "#     fig, axes = plt.subplots(2*len(model_set),4,figsize=(4*4,4*len(model_set)),sharex=True,facecolor=(1, 1, 1),dpi=200,height_ratios=[4,1]*len(model_set))\n",
    "    \n",
    "#     for jj,(model,dataset,file) in enumerate(zip(model_set,datasets,file_set)):\n",
    "#         x_window = np.log(4550) - np.log(4549)\n",
    "#         lmin = np.exp(dataset.xs[0,0])\n",
    "#         lmax = np.exp(dataset.xs[0,2000])\n",
    "#         lrange = np.arange(lmin,lmax,5)\n",
    "#         sort_airmasses = np.argsort(np.array(file['airms'][:]))\n",
    "#         plt_epochs = np.concatenate((sort_airmasses[:2],sort_airmasses[-2:]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         offset = 1.0\n",
    "#         xplot = np.linspace(np.log(lmin)-x_window,np.log(lmax)+x_window,dataset.xs.shape[1]*10)\n",
    "#         for ii,plt_epoch in enumerate(plt_epochs):\n",
    "#             axes[2*jj,ii].set_xlim(xplot.min()+model[0][0].p[plt_epoch],xplot.max()+model[0][0].p[plt_epoch])\n",
    "            \n",
    "#             model.fix()\n",
    "            \n",
    "#             axes[2*jj,ii].errorbar(dataset.xs[plt_epoch,:],dataset.ys[plt_epoch,:],\\\n",
    "#                                      dataset.yerr[plt_epoch,:],fmt='.k',elinewidth=1.2,zorder=1,alpha=0.5,ms=3)\n",
    "            \n",
    "#             axes[2*jj,ii].plot(xplot,offset + model[0]([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             axes[2*jj,ii].plot(xplot,2*offset + model[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             axes[2*jj,ii].plot(xplot,model[2]([],xplot,plt_epoch),'-m',linewidth=1.2,zorder=3,alpha=0.7,ms=6)\n",
    "            \n",
    "#             # axes[0,ii].plot(xplot,2*offset + model[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             # axes[0,ii].plot(xplot,offset + model([],xplot,plt_epoch),'-g',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             # axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(xplot,true_model([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=1,alpha=0.5,ms=6)\n",
    "            \n",
    "            \n",
    "#             axes[2*jj,ii].set_ylim(-2,3)\n",
    "#             axes[2*jj,ii].set_xticks([])\n",
    "#             # axes[0].set_yticks([])\n",
    "#             axes[2*jj+1,ii].set_xticks(np.log(lrange))\n",
    "#             axes[2*jj+1,ii].set_xticklabels(['{:2.0f}'.format(x) for x in lrange])\n",
    "            \n",
    "#             axes[2*jj+1,ii].plot(dataset.xs[plt_epoch,:],dataset.ys[plt_epoch,:] - model([],dataset.xs[plt_epoch,:],plt_epoch),'.k',alpha=0.4,ms=1)\n",
    "            \n",
    "#             axes[2*jj+1,ii].set_ylim(-0.1,0.1)\n",
    "#             axes[2*jj,ii].set_title('airmass = {}'.format(file['airms'][:][plt_epoch]))\n",
    "#         # res_ax = axes[plt_epoch // r_plots, plt_epoch % r_plots].twinx()\n",
    "#         # residual = loss(model_set[model_num].get_parameters(),dataset,plt_epoch,model_set[model_num])\n",
    "#         # res_ax.step(dataset.xs[plt_epoch,:],residual,where='mid',alpha=0.3,label='residual')\n",
    "#         # res_ax.set_ylim(0.0,20)\n",
    "#         # res_ax.set_yticks([])\n",
    "#         # res_ax.step(model_set[i][j][1].xs+model_set[i][j][0].p[plt_epoch],\\\n",
    "#         #             model_set[i][j].results[-2]['grad'][:],\\\n",
    "#         #             where='mid',alpha=0.4,label='residual',zorder=-1)\n",
    "#         # res_ax.set_yticks([])\n",
    "        \n",
    "#         # res_ax.step(x_grid,\\\n",
    "#         #             rv_model_deriv[:,plt_epoch],\\\n",
    "#         #             where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "            \n",
    "#         #     res_ax.step(x_grid,\\\n",
    "#         #                 rv_loss_deriv[:,plt_epoch],\\\n",
    "#         #                 where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "            \n",
    "#         #     align_yaxis(, 0, , 0)\n",
    "            \n",
    "#             # align.yaxes(axes[plt_epoch // r_plots, plt_epoch % r_plots], 0.0, res_ax, 0.0, 2./3.)\n",
    "        \n",
    "#         # res.get_shared_y_axes().join(ax1, ax3)\n",
    "#         fig.text(0.5, 0.04, '$\\lambda$', ha='center')\n",
    "#         # fig.text(0.04, 0.5, 'y', va='center', rotation='vertical')\n",
    "#         # fig.text(0.96, 0.5, '$d \\L /d \\delta x$', va='center', rotation=270)\n",
    "#         # fig.text(0.96, 0.5, '$d f_{{{ji}}} /d \\delta x_k$', va='center', rotation=270)\n",
    "#         # fig.text(0.96, 0.5, 'residuals', va='center', rotation=270)\n",
    "    \n",
    "#     # plt.savefig(os.path.join(out_dir,'02-full-barn-51peg.png'),dpi=300,bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04116c4-5bcc-473c-9323-90f440ebd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_better_plot(model_set,datasets,file_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e991d2-9c6e-449b-8c34-b9989fa1cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell_loss = [[],[]]\n",
    "# for jjj, (dataset, model) in enumerate(zip(datasets,model_set)):\n",
    "#     for iii in range(dataset.ys.shape[0]):\n",
    "#         tell_loss[jjj].append(loss([],dataset,iii,model[0]).sum())\n",
    "\n",
    "# plt.plot(np.array(file_p['airms'][:]),tell_loss[0],'.k',label='51 peg')\n",
    "# plt.plot(np.array(file_b['airms'][:]),tell_loss[1],'.r',label='barnards')\n",
    "# # plt.ylim(0.0,5e4)\n",
    "\n",
    "# # plt.plot(np.array(file_p['airms'][:]),model_p[1][1].p,'.k',label='51 peg')\n",
    "# # plt.plot(np.array(file_b['airms'][:]),model_b[1][1].p,'.r',label='barnards')\n",
    "# plt.xlabel('airmass')\n",
    "# plt.ylabel('$\\Sigma_* (y_* - \\hat{y}_s(x_*)) I_{y*}$')\n",
    "# # plt.plot()\n",
    "# plt.legend()\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass_loss.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cecc1d-a1f4-45fb-9bcd-b320c59182e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.array(file_p['airms'][:]),model_p[1][1].p,'.k',label='51 peg')\n",
    "# plt.plot(np.array(file_b['airms'][:]),model_b[1][1].p,'.r',label='barnards')\n",
    "# plt.xlabel('airmass')\n",
    "# plt.ylabel('~a')\n",
    "# x_space = np.linspace(np.min(np.array(file_b['airms'][:])),np.max(np.array(file_b['airms'][:])))\n",
    "# plt.plot(x_space,x_space,'-.k',alpha=0.3)\n",
    "# plt.legend()\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass_an.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2dd7d-b813-4d62-a105-87bc9806bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1,figsize=(4,4),sharex=True,facecolor=(1, 1, 1),dpi=200)\n",
    "\n",
    "# plt_epoch = 10\n",
    "# x_window = np.log(4550) - np.log(4549)\n",
    "# lmin = np.exp(dataset_p.xs[0,500])\n",
    "# lmax = np.exp(dataset_p.xs[0,1500])\n",
    "# lrange = np.arange(lmin,lmax,5)\n",
    "# xplot = np.linspace(np.log(lmin)-x_window,np.log(lmax)+x_window,dataset_p.xs.shape[1]*10)\n",
    "# axes.plot(xplot,model_p[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.6,ms=6,label='51 peg')\n",
    "# axes.plot(xplot,0.05 + model_b[1]([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=2,alpha=0.6,ms=6,label='barnard')\n",
    "# axes.legend()\n",
    "\n",
    "# axes.set_ylim(-0.2,0.1)\n",
    "# axes.set_xticks([])\n",
    "# axes.set_ylabel('log flux + offset')\n",
    "# axes.set_xlabel('$\\lambda$')\n",
    "# axes.set_xticks(np.log(lrange))\n",
    "# axes.set_xticklabels(['{:2.0f}'.format(x) for x in lrange])\n",
    "# plt.title('just tellurics')\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass-tell.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
