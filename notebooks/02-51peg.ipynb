{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a792c86-8197-40a1-8bec-fa2820a3423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "# import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff28ef2-c0b2-4dc2-9d92-120b3633ba8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[cuda(id=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c5c41e-7968-435a-8731-40e6d9e6141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nifty_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "854ca1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wobbleenv/lib/python3.11/site-packages/jax/_src/api_util.py:174: SyntaxWarning: Jitted function has static_argnums=(3, 4, 5), but only accepts 5 positional arguments. This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has {argnums_name}={argnums}, \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import jabble.model\n",
    "import jabble.dataset\n",
    "import jabble.loss\n",
    "import jabble.physics \n",
    "import astropy.units as u\n",
    "\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "# from jaxopt import GaussNewton\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "# from mpl_axes_aligner import align\n",
    "\n",
    "import os\n",
    "import jabble.physics\n",
    "\n",
    "import jax.config\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_disable_jit\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d1a69-9b8d-445a-8c96-ffe4d4833863",
   "metadata": {},
   "source": [
    "<h1>02 - Fitting RVs to HARPS e2ds Spectra</h1>\n",
    "In this notebook, I am will show you how to load in data and metadata to jabble.data objects. Then fit models to each component of the data. Due to some edge errors with normalizing near the edge of the orders in the HARPS spectrograph, we fit a longer wavelength component normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c72b75-906c-48c1-b11d-f1cb48103260",
   "metadata": {},
   "source": [
    "Create output directory here as the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae9961a7-5035-4cc7-a8b1-1be56ad0c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "out_dir = os.path.join('/scratch/mdd423/wobble_jax/','out',today.strftime(\"%y-%m-%d\"))\n",
    "os.makedirs(out_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cf2311c-3b0d-4095-bf02-e34b8570cef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/mdd423/wobble_jax/out/25-01-06'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e260ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b = h5py.File(\"../data/barnards_e2ds.hdf5\", \"r\")\n",
    "# file_p = h5py.File(\"../data/51peg_e2ds.hdf5\"   , \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5fd5722-2bc5-4c0b-b9c9-9c549405956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax:    0.4.23\n",
      "jaxlib: 0.4.23\n",
      "numpy:  1.24.3\n",
      "python: 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0]\n",
      "jax.devices (1 total, 1 local): [cuda(id=0)]\n",
      "process_count: 1\n",
      "\n",
      "$ nvidia-smi\n",
      "Mon Jan  6 15:38:50 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-PCIE-32GB           On  | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              36W / 250W |    312MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2081953      C   ...iniconda3/envs/wobbleenv/bin/python      308MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jax.print_environment_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712d1c5-0502-4549-b18d-0b3e28440511",
   "metadata": {},
   "source": [
    "<h2>Jabble.Dataset</h2>\n",
    "The 4 fields needed to create a jabble.data object are: xs, ys, yivar, and mask. xs is the input to whats being modeled, this should be a list of 1 dimensional jax.numpy arrays, which may have different shapes (lengths). These correspond to the log wavelength values being evaluated. The logarithm of wavelength means that the redshift from relative radial velocity change becomes an additive property.\n",
    "\n",
    "$$x_{obs} = \\log(\\lambda_{obs})$$\n",
    "$$\\lambda_{obs} = \\sqrt{\\frac{1 + RV/c}{1- RV/c}}\\lambda_{emit}$$\n",
    "$$x_{obs} = \\log(\\sqrt{\\frac{1 + RV/c}{1- RV/c}}\\lambda_{emit})$$\n",
    "$$x_{obs} = \\log(\\sqrt{\\frac{1 + RV/c}{1- RV/c}}) + \\log(\\lambda_{emit})$$\n",
    "$$x_{obs} = \\delta x + x_{emit}$$\n",
    "\n",
    "where $x_{obs}$ is the observed log wavelength, $RV$ is the radial velocity, $c$ is the speed of light, and $x_{emit}$ is the emitted log wavelength. ys is the measured log flux at that log wavelength, xs. This value is assumed to be in photon per wavelength bin. Thus, multiplicative properties like absorption from tellurics become additive in the logarithm. \n",
    "\n",
    "$$y = \\log(f)$$\n",
    "$$y = \\log(f_s f_t) $$\n",
    "$$y = \\log(f_s) + \\log(f_t) $$\n",
    "$$y = y_s + y_t$$\n",
    "\n",
    "yivar stands for y inverse variance, or y information. This value is produced by the instrumentation team at these detectors using some assumption about instrument readout and dark current properties along with poisson photon noise. Thus the error on the flux is not being modeled here and is assumed as correct from the pipeline. These errors are given as the standard deviation of the flux, so we need to transform error of f into the error in logarithm of flux.\n",
    "\n",
    "$$\\sigma_y = \\sigma_f \\frac{dy}{df}$$\n",
    "$$\\sigma_y = \\frac{\\sigma_f}{f}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1b00a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file,orders,device):\n",
    "    ys = []\n",
    "    xs = []\n",
    "    yivar = []\n",
    "    mask = []\n",
    "\n",
    "    init_shifts = np.array([jabble.physics.shifts(x) for x in file[\"bervs\"]])\n",
    "    times_t = np.array([x for x in file[\"dates\"]])\n",
    "    full_init_shifts = []\n",
    "    print(file[\"bervs\"].shape)\n",
    "    airmass = []\n",
    "    order_out = []\n",
    "    times = []\n",
    "    star_ids = []\n",
    "\n",
    "    for iii in orders:\n",
    "        for jjj in range(file[\"data\"].shape[1]):\n",
    "            ys.append(jnp.array(file[\"data\"][iii,jjj,:]))\n",
    "            xs.append(jnp.array(file[\"xs\"][iii,jjj,:]))\n",
    "            yivar.append(jnp.array(file[\"ivars\"][iii,jjj,:]))\n",
    "            mask.append(jnp.zeros(file[\"data\"][iii,jjj,:].shape,dtype=bool))\n",
    "\n",
    "            # init_shifts.append(\n",
    "            airmass.append(file[\"airms\"][jjj])\n",
    "            order_out.append(iii)\n",
    "            times.append(file[\"dates\"][jjj])\n",
    "            star_ids.append(str(file))\n",
    "\n",
    "            full_init_shifts.append(jabble.physics.shifts(file[\"bervs\"][jjj]))\n",
    "    \n",
    "    full_init_shifts = jnp.array(full_init_shifts)\n",
    "    airmass = jnp.array(airmass)\n",
    "\n",
    "    dataset = jabble.dataset.Data.from_lists(xs,ys,yivar,mask)\n",
    "\n",
    "    dataset.metakeys['times'] = times_t\n",
    "    dataset.metadata['times'] = np.array(times)\n",
    "    dataset.metadata['orders'] = np.array(order_out)\n",
    "    dataset.metadata['star_ids'] = np.array(star_ids)\n",
    "    dataset.metadata['init_shifts'] = np.array(full_init_shifts)\n",
    "\n",
    "    dataset.metadata['airmass'] = airmass\n",
    "    dataset.metadata['bervs'] = full_init_shifts\n",
    "    dataset.to_device(device)\n",
    "    \n",
    "    init_shifts = jax.device_put(init_shifts,device)\n",
    "    airmass = jax.device_put(airmass,device)\n",
    "    \n",
    "    return dataset, init_shifts, airmass, full_init_shifts, times_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c749e55-b98c-4948-9a15-bb9faf2a03c6",
   "metadata": {},
   "source": [
    "<h2>Jabble.Model</h2>\n",
    "Now, we will define the components of the model. These stars will have 3 components: stellar, telluric, pseudo-normalization. \n",
    "\n",
    "$$y = y_\\mathrm{s} + y_\\mathrm{t} + y_\\mathrm{N}$$\n",
    "\n",
    "The stellar component will be be high resolution (twice R of the detector) in wavelength and static in flux across all times or epochs of data. However, a nonlinear comes from the redshift radial velocity being fit inside the linear model log flux model. Because our dataset will consist of different orders at the same time, and the same order at different time, and we are assuming the wavelenght solution to be true. Thus the redshift should be the same at different orders at the same time, so $i_*$ is the index of the epoch while $i$ is the data index, which is different for all orders and times or can be thought of simply as the data index.\n",
    "\n",
    "$$y_s = f(x + \\delta x_{i_*}|\\theta_\\mathrm{s})$$\n",
    "\n",
    "The tellurics model is also high resolution in wavelength and static across all times, but the airmass is used to stretch the model at each model at each time. This parameter is not fit, however is can be if the user chooses to do so!\n",
    "\n",
    "$$y_t = a_i f(x|\\theta_\\mathrm{t})$$\n",
    "\n",
    "And finally the pseudo-normalization model is low resolution in wavelength about one parameter per 50 Angstroms and is different across all times because we expect the detector state to vary greatly with time.\n",
    "\n",
    "$$y_\\mathrm{N} = f(x|\\theta_{\\mathrm{N},i})$$\n",
    "\n",
    "The function we choose to represent the flux of these models is the cardinal spline mixture, who are evenly spaced in wavelength. \n",
    "\n",
    "$$f(x|\\theta,p) = \\sum_i \\theta_i k(x|p,i)$$\n",
    "\n",
    "where $k$ is the cardinal basis/kernel function. This basis function is a piecewise polynomial positioned on an evenly spaced grid such that the total function will be continuous in $p-1$ derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19ccb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jabble.quickplay\n",
    "def get_model(dataset,resolution,p_val,vel_padding,init_shifts,rest_shifts,airmass,pts_per_wavelength,norm_p_val):\n",
    "         \n",
    "    dx = jabble.physics.delta_x(2 * resolution)\n",
    "    x_grid = jnp.arange(np.min(np.concatenate(dataset.xs)), np.max(np.concatenate(dataset.xs)), step=dx, dtype=\"float64\")\n",
    "    \n",
    "    model_grid = jabble.model.create_x_grid(\n",
    "        x_grid, vel_padding.to(u.m/u.s).value, 2 * resolution\n",
    "    )  \n",
    "    model = jabble.quickplay.WobbleModel(init_shifts, airmass, model_grid, p_val)\n",
    "\n",
    "    # model = jabble.quickplay.PseudoNormalModel(init_shifts, init_airmass, model_grid, p_val, dataset, norm_p_val, pts_per_wavelength)\n",
    "    # model = jabble.model.CompositeModel(\n",
    "    #     [\n",
    "    #         jabble.model.EpochShiftingModel(init_shifts),#ShiftingModel(init_shifts),#\n",
    "    #         jabble.model.CardinalSplineMixture(model_grid, p_val),\n",
    "    #     ]\n",
    "    # ) + jabble.model.CompositeModel(\n",
    "    #     [\n",
    "    #         jabble.model.ShiftingModel(rest_shifts),\n",
    "    #         jabble.model.CardinalSplineMixture(model_grid, p_val),\n",
    "    #         jabble.model.StretchingModel(airmass),\n",
    "    #     ]\n",
    "    # ) #+ jabble.model.get_normalization_model(dataset,norm_p_val,pts_per_wavelength)\n",
    "\n",
    "    # model.to_device(device)\n",
    "\n",
    "    # model.fit(2)\n",
    "    # print(type(model.get_parameters()),model.get_parameters().devices())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061a342-9e58-4016-9d20-d7fdc1970ea9",
   "metadata": {},
   "source": [
    "<h2>Training Cycle</h2>\n",
    "Now we define the fitting process of the parameters of all of these models with respect to the total set of data. This is require some clever initialization, so that we don't end up too far from the solution. First, we fit the parameters of the tellurics, stellar, normalization model. Regularizing the stellar and tellurics components, so that they are close 0. Then we fit the RV alone. Then we optimize everything together with regularization on the stellar and tellurics component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f6704c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cycle(model, dataset, loss, lmb, device_store, device_op, batch_size):\n",
    "    # Fit Stellar & Telluric Template\n",
    "    model.fix()\n",
    "    # model.fit(2, 1)\n",
    "    model[\"Stellar\"][\"Template\"].fit()\n",
    "    model[\"Tellurics\"][\"Template\"].fit()\n",
    "    model.display()\n",
    "    # reg_s = lmb*jabble.loss.L2Reg([0,1])\n",
    "    # reg_t = lmb*jabble.loss.L2Reg([1,1])\n",
    "    \n",
    "    res1 = model.optimize(loss, dataset, device_store, device_op, batch_size)#model.optimize(loss, dataset)\n",
    "    print(res1)\n",
    "    \n",
    "    # Fit RV\n",
    "    model.fix()\n",
    "    model[\"Stellar\"][\"RV\"].fit()\n",
    "    res1 = model.optimize(loss, dataset, device_store, device_op, batch_size)\n",
    "    print(res1)\n",
    "\n",
    "    # RV Parabola Fit\n",
    "    # model.fix()\n",
    "    # shift_search = jabble.physics.shifts(np.linspace(-10, 10, 100))\n",
    "    # model[0][0].parabola_fit(shift_search, loss, model, dataset)\n",
    "    # print(type(model_p[0][0].p))\n",
    "\n",
    "    # Fit Everything\n",
    "    model.fix()\n",
    "    model[\"Stellar\"][\"RV\"].fit()\n",
    "    model[\"Stellar\"][\"Template\"].fit()\n",
    "    model[\"Tellurics\"][\"Template\"].fit()\n",
    "    # model.fit(2, 1)\n",
    "\n",
    "    res1 = model.optimize(loss, dataset, device_store, device_op, batch_size)#model.optimize(loss, dataset)\n",
    "    print(res1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64d128-5f26-4373-a60c-6a6572ece1ae",
   "metadata": {},
   "source": [
    "Here we define the 'loss' to be the sum of the chi squared of each flux. This is typical choice when using jabble, and why we must define the inverse variance of y, $I_{y}$ in the data object.\n",
    "\n",
    "$$\\mathcal{L}_{\\chi^2} = \\sum I_y (ys - y(xs|\\delta x_{i_*},\\theta_\\mathrm{s},\\theta_\\mathrm{t},\\theta_{\\mathrm{N},i}) )^2$$\n",
    "\n",
    "$xs, ys, I_{y}$ are from the data and are then summed over all unmasked elements.\n",
    "\n",
    "$$\\mathcal{L}_{\\mathrm{reg}} = \\sum \\theta_\\mathrm{s}^2 + \\theta_\\mathrm{t}^2$$\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{\\chi^2} + \\lambda \\mathcal{L}_{\\mathrm{reg}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d730833b-c2f3-40e0-821e-d179c37781b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = jax.devices(\"cpu\")\n",
    "gpus = jax.devices(\"gpu\")\n",
    "loss = jabble.loss.ChiSquare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383a8f0-c1b4-458d-b6bc-d8d96d9832b3",
   "metadata": {},
   "source": [
    "Here we define the parameters of the instrument that are needed to build our model. The instrument is HARPS, so the resolution is 115,000. The $p$ value for the CSM model on the stellar and telluric components is set to 2. This means just the first derivative of the model will be continuous. And we will add a wavelength padding to either side of the log wavelength grid out to $100$ km/s in log wavelength. And the lambda for the regularization term is defined here as $\\lambda = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "437c441f-ed1a-41c0-9d65-5f7f0f6a6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 115_000\n",
    "p_val = 2\n",
    "vel_padding = 100 * u.km / u.s\n",
    "\n",
    "\n",
    "pts_per_wavelengths = [1/50.0]\n",
    "norm_p_vals = [3]\n",
    "lmbs = [100.0]\n",
    "\n",
    "star_name_b, star_name_p = 'barnards','peg51'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011416e7-f905-41cb-89c1-3f2f38a68bd6",
   "metadata": {},
   "source": [
    "We will not fit the entire spectrum in one pass but we will fit chunks of multiple orders with one RV per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54e0513c-a875-4d66-a9fe-9b4745923690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]\n",
      " [24 25 26 27]\n",
      " [28 29 30 31]\n",
      " [32 33 34 35]\n",
      " [36 37 38 39]\n",
      " [40 41 42 43]\n",
      " [44 45 46 47]\n",
      " [48 49 50 51]\n",
      " [52 53 54 55]\n",
      " [56 57 58 59]\n",
      " [60 61 62 63]\n",
      " [64 65 66 67]\n",
      " [68 69 70 71]]\n"
     ]
    }
   ],
   "source": [
    "orders_s = np.arange(0,72,dtype=int).reshape(-1,4) \n",
    "print(orders_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d401a1c-a929-4772-822d-7c18c0252bd5",
   "metadata": {},
   "source": [
    "Also decide what device you want to store extra data on, and which device to put the model and do the fitting on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a72e0e9b-dd67-486d-aed2-3d1bc20ab00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306,)\n",
      "-WobbleModel---------------------------------------------------14094\n",
      "  0-StellarModel------------------------------------------------7047\n",
      "  0  0-EpochShiftingModel------------------------------------------0\n",
      "  0  1-CardinalSplineMixture------------------------------------7047\n",
      "  1-TelluricsModel----------------------------------------------7047\n",
      "  1  0-StretchingModel---------------------------------------------0\n",
      "  1  1-CardinalSplineMixture------------------------------------7047\n",
      "{'grad': array([0., 0., 0., ..., 0., 0., 0.]), 'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'funcalls': 1212, 'nit': 1166, 'warnflag': 0}\n",
      "{'grad': array([-8.33373515e+03,  6.67318048e+04, -3.15841799e+03, -4.09585961e+03,\n",
      "       -2.49136877e+03, -1.35323816e+03,  6.18215656e+02,  6.89778677e+04,\n",
      "        1.34643893e+02, -5.97270900e+03, -4.51543513e+02,  5.02110899e+03,\n",
      "       -8.03150516e+03,  2.65199599e+04,  6.27067520e+03, -7.50625323e+04,\n",
      "        4.91624180e+03,  1.00741102e+03, -2.80338396e+03, -1.03615945e+03,\n",
      "       -9.07955971e+03,  1.12886983e+03,  1.42569508e+04, -8.56730147e+03,\n",
      "       -9.77335853e+03,  1.04540720e+04, -7.61256991e+03, -2.30088401e+03,\n",
      "        7.37891850e+03,  4.43595871e+03, -1.22262193e+04, -1.24398097e+04,\n",
      "        9.26217050e+03, -1.21183657e+04,  6.24130876e+03, -2.16476195e+04,\n",
      "       -6.13448383e+02, -7.06376978e+04, -5.72252126e+04,  4.14080018e+03,\n",
      "       -2.14031291e+02,  7.57993539e+03,  2.84263256e+04, -3.85034838e+04,\n",
      "        4.13067013e+04,  9.37860333e+01, -2.41609795e+03,  2.70434749e+03,\n",
      "       -3.74598696e+03,  2.74874631e+03, -8.85874087e+02,  3.55689331e+03,\n",
      "       -2.37486359e+04, -3.17050042e+00,  2.46954112e+04,  1.32758976e+04,\n",
      "        2.45975527e+04, -8.68891812e+03,  5.54650910e+04, -1.72281513e+04,\n",
      "        5.17492610e+02, -1.32886545e+04, -2.96296893e+02,  4.19693014e+03,\n",
      "       -9.07142775e+02, -2.77950562e+02,  2.47301240e+04, -1.62125948e+05,\n",
      "        1.21684788e+04, -1.30439431e+04, -1.28139931e+04, -1.83772549e+04,\n",
      "       -2.99071478e+04,  1.24720461e+04,  9.01348444e+01,  1.57353428e+03,\n",
      "       -1.51088531e+04,  1.30379685e+04, -7.69141080e+02, -1.53104206e+04,\n",
      "       -2.91192806e+02, -5.02995117e+04, -2.51658102e+04, -1.38643627e+05,\n",
      "       -1.03412903e+05,  3.69007920e+04,  5.45003270e+03, -3.38462119e+04,\n",
      "       -3.52166331e+03,  4.66736263e+04,  6.29823886e+03, -9.94226214e+02,\n",
      "        1.61712757e+04, -2.32463594e+04, -3.25686602e+04,  5.27304935e+04,\n",
      "        1.27597529e+04, -5.10847842e+03,  1.06067762e+04,  6.06402209e+04,\n",
      "       -6.61680944e+03, -3.98104163e+04,  4.66600613e+01,  4.13351431e+04,\n",
      "       -1.13627662e+05,  8.91535745e+02,  1.56132292e+03,  1.30822328e+04,\n",
      "       -1.91257850e+02,  9.58453382e+03, -9.84606072e+03,  3.04449702e+02,\n",
      "       -6.85556898e+02,  2.18339155e+04, -4.47300054e+03, -6.12514271e+04,\n",
      "        2.72004450e+03, -5.21005095e+03,  2.23617703e+04, -5.33571857e+02,\n",
      "        5.21613658e+02, -9.47017972e+03, -5.72997977e+04, -1.64532453e+03,\n",
      "       -7.66207792e+03,  8.16960348e+02, -1.06747000e+03, -4.00844860e+04,\n",
      "        2.94305230e+04,  2.76184142e+03, -8.78791588e+02,  8.56183842e+02,\n",
      "        1.27640788e+03, -6.41527947e+03,  1.10601789e+04,  3.33319964e+03,\n",
      "        1.17327321e+03,  1.99169554e+03,  1.34583974e+04,  7.83327657e+04,\n",
      "       -6.72244734e+02,  5.62675878e+02,  9.30213669e+04,  2.65306407e+04,\n",
      "       -6.65789607e+03,  3.88408791e+04, -3.95643487e+02, -7.69575529e+02,\n",
      "       -5.81471884e+04, -3.22568454e+04,  2.45630738e+04, -2.00594857e+03,\n",
      "       -5.69527992e+01, -1.54717465e+02, -1.08136222e+04, -1.86722924e+04,\n",
      "       -1.38084340e+05,  1.39223926e+04,  1.12704070e+04,  4.28167478e+03,\n",
      "       -8.21564542e+03, -5.84294838e+03,  3.47736197e+03,  1.40705923e+05,\n",
      "        8.25507239e+03,  6.07119598e+01,  5.37559851e+03, -1.43792614e+03,\n",
      "        2.25282609e+03, -9.85635155e+03,  9.66084175e+02,  3.55195238e+04,\n",
      "       -7.36992468e+03,  9.55383478e+04,  4.97987839e+03,  2.28300772e+04,\n",
      "       -1.94260938e+04, -3.45600284e+03,  6.12663439e+03, -2.52317136e+03,\n",
      "        5.23858308e+03,  3.12406718e+03, -2.24458163e+03, -9.51178083e+02,\n",
      "       -1.10937053e+04, -2.90019048e+03, -3.10397514e+03, -1.00919302e+05,\n",
      "       -5.03854869e+02,  2.51049775e+03,  3.89836686e+04, -1.59811092e+03,\n",
      "       -3.29979728e+02,  2.54226365e+02, -5.58713867e+00,  6.98992518e+03,\n",
      "       -1.44822240e+04, -1.01451686e+04,  1.80278009e+04, -3.35547166e+03,\n",
      "       -6.81188525e+04, -9.26449424e+03,  4.72584606e+03, -5.76216158e+02,\n",
      "        7.23901134e+02, -2.11631025e+03, -1.37384689e+04,  2.42044832e+03,\n",
      "        1.00633124e+02, -2.15260877e+04,  2.22544332e+04,  0.00000000e+00,\n",
      "       -5.16362260e+04,  2.92295255e+03,  3.18909555e+03,  5.10891908e+03,\n",
      "       -1.31159546e+05,  3.45929632e+03,  2.15516026e+04, -5.46284519e+03,\n",
      "       -1.88716814e+04, -7.61845845e+03, -2.10941654e+03, -2.24298637e+04,\n",
      "        2.77767371e+03, -2.52336711e+03, -2.15735019e+04,  2.52296030e+04,\n",
      "       -3.65723875e+04, -6.06477570e+03, -2.67604981e+03,  1.64980767e+03,\n",
      "        1.07646407e+04,  2.53822121e+03,  2.85747502e+03,  9.89063658e+02,\n",
      "        7.21191074e+03,  3.86414074e+03, -1.69385510e+02,  1.18017173e+03,\n",
      "        4.74155914e+03, -3.17475325e+03,  5.68181569e+02,  1.83953372e+03,\n",
      "        1.51514915e+04,  6.05963145e+03,  3.18035458e+04,  7.48080658e+02,\n",
      "       -1.24110666e+04,  1.74454770e+04, -1.62044667e+04, -4.12366423e+05,\n",
      "       -1.11807426e+03,  2.66438150e+02, -1.93049072e+03,  2.34320709e+04,\n",
      "        2.39509046e+02, -2.81730726e+01, -2.72712484e+04,  2.47488360e+04,\n",
      "        2.66281258e+01, -9.32764915e+03,  2.59230357e+04, -8.98000299e+03,\n",
      "       -5.76945552e+03, -9.48317290e+03,  1.18029402e+04, -1.00981465e+04,\n",
      "       -1.63415986e+03, -3.91950855e+02,  3.97399219e+02, -4.87005081e+02,\n",
      "        2.83378706e+03, -6.49510449e+02, -3.97867317e+04,  3.42931053e+04,\n",
      "        1.26139963e+03, -2.29526318e+04,  2.46913238e+04, -4.65685249e+03,\n",
      "       -3.48080587e+03,  1.95563653e+04,  2.70296434e+03, -9.01156146e+03,\n",
      "        2.91034481e+03, -7.31053567e+03, -1.30760532e+04, -5.71982539e+03,\n",
      "        2.76625443e+02, -3.40392029e+04, -2.58900686e+04, -3.40139996e+03,\n",
      "       -2.47554388e+04, -2.49297738e+03,  1.94374159e+04,  1.49293343e+04,\n",
      "        1.17172861e+03, -5.48578368e+03, -3.75406201e+03, -2.31548807e+03,\n",
      "        5.51527333e+02,  3.93884160e+03,  6.15528705e+03, -2.62580370e+04,\n",
      "       -4.81851721e+03, -3.54071215e+03]), 'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'funcalls': 91, 'nit': 78, 'warnflag': 0}\n",
      "{'grad': array([-3783.9847072 , 66736.32948434, -2910.82662745, ...,\n",
      "           0.        ,     0.        ,     0.        ]), 'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'funcalls': 5, 'nit': 1, 'warnflag': 0}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "WobbleModel.get_RV_sigmas() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mto_device(device_op)\n\u001b[1;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m train_cycle(model, dataset, loss, lmb, device_store, device_op, batch_size)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m jabble\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(model_name,model)\n",
      "File \u001b[0;32m~/wobble_jax/notebooks/../jabble/quickplay.py:170\u001b[0m, in \u001b[0;36mWobbleModel.save\u001b[0;34m(self, filename, mode, data)\u001b[0m\n\u001b[1;32m    168\u001b[0m group \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRVs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m group\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRVs\u001b[39m\u001b[38;5;124m\"\u001b[39m,data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_RV())\n\u001b[0;32m--> 170\u001b[0m group\u001b[38;5;241m.\u001b[39mcreate_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRV_err\u001b[39m\u001b[38;5;124m\"\u001b[39m,data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_RV_sigmas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    172\u001b[0m     res_group \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mcreate_group(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresiduals\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: WobbleModel.get_RV_sigmas() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "device_store = cpus[0]\n",
    "device_op = gpus[0]\n",
    "batch_size = 5000\n",
    "\n",
    "for orders in orders_s:\n",
    "    for star_name,file in zip([star_name_b],[file_b]):\n",
    "        for lmb in lmbs:\n",
    "            for norm_p_val in norm_p_vals:\n",
    "                for pts_per_wavelength in pts_per_wavelengths:\n",
    "        \n",
    "                    model_name = os.path.join(out_dir,star_name + '_epoch_inds_o{}_no_norm.mdl'.format(orders))\n",
    "                    dataset, init_shifts, airmass, full_init_shifts, times_t = get_dataset(file,orders,device_op)\n",
    "\n",
    "                    datablock, metablock, meta_keys = dataset.blockify(device_store,return_keys=True)\n",
    "                    \n",
    "                    shifts = jnp.zeros(full_init_shifts.shape)\n",
    "                    model = get_model(dataset,resolution,p_val,vel_padding,-init_shifts,shifts,airmass,pts_per_wavelength,norm_p_val)\n",
    "   \n",
    "                    for key in dataset.metadata.keys():\n",
    "                        model.metadata[key] = dataset.metadata[key]\n",
    "                    \n",
    "                    model.to_device(device_op)\n",
    "                    model = train_cycle(model, dataset, loss, lmb, device_store, device_op, batch_size)\n",
    "                    model.save(model_name[:-3] + \"hdf\",mode=1,dataset)\n",
    "                    \n",
    "                    jabble.model.save(model_name,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63b68629-b79d-427e-8191-d1d4a9050797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# def load_recurse(group):\n",
    "#     # If the group is a submodel loads the parameters\n",
    "#     print(group.keys())\n",
    "#     if \"parameters\" in group.keys():\n",
    "#         model = eval(key.split(\"]\")[-1])(p=file[key][\"parameters\"])\n",
    "#     # Else the group is a ContainerModel and loads model list recursively\n",
    "#     else:\n",
    "#         model_list = []\n",
    "#         for key in group.keys():\n",
    "#             model_list.append(load_recurse(group[key]))\n",
    "#         model = eval(key.split(\"]\")[-1])(model_list)\n",
    "#     return model\n",
    "\n",
    "# files = glob.glob(\"/scratch/mdd423/wobble_jax/out/24-12-18/*.hdf\")\n",
    "# with h5py.File(files[0],'r') as file:\n",
    "#     model = load_recurse(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a8ef33e-f1f4-4450-aff8-ce839bff1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31963717-fa2c-4afc-af96-9f65403214f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[0 1 2 3]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[60 61 62 63]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[16 17 18 19]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[48 49 50 51]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[36 37 38 39]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[64 65 66 67]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[44 45 46 47]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[24 25 26 27]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[56 57 58 59]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[4 5 6 7]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[ 8  9 10 11]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[28 29 30 31]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[20 21 22 23]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[12 13 14 15]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[32 33 34 35]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[40 41 42 43]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[68 69 70 71]_no_norm..hdf', '/scratch/mdd423/wobble_jax/out/24-12-18/barnards_epoch_inds_o[52 53 54 55]_no_norm..hdf']\n"
     ]
    }
   ],
   "source": [
    "dir = glob.glob('/scratch/mdd423/wobble_jax/out/24-12-18/*.hdf')\n",
    "print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "48024a78-62ba-4440-879d-c571a011a11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_hdf_submodel(cls,group):\n",
    "\n",
    "#     return cls(p=group[\"parameters\"])\n",
    "\n",
    "# class EpochShiftingModel(jabble.model.EpochShiftingModel):\n",
    "#     def load_hdf(cls,group):\n",
    "\n",
    "#         return load_hdf_submodel(cls,group)\n",
    "\n",
    "# class ShiftingModel(jabble.model.ShiftingModel):\n",
    "#     def load_hdf(cls,group):\n",
    "\n",
    "#         return load_hdf_submodel(cls,group)\n",
    "\n",
    "# class StretchingModel(jabble.model.StretchingModel):\n",
    "#     def load_hdf(cls,group):\n",
    "        \n",
    "#         return load_hdf_submodel(cls,group)\n",
    "\n",
    "# class CardinalSplineMixture(jabble.model.CardinalSplineMixture):\n",
    "#     def load_hdf(cls,group):\n",
    "#         print(group.keys())\n",
    "#         return cls(xs=group[\"xs\"], p_val=group[\"p_val\"], p=group[\"parameters\"])\n",
    "\n",
    "# def load_hdf_container(cls,group):\n",
    "#     model_list = []\n",
    "#     for key in group.keys():\n",
    "#         cls_sub = eval(key.split(']')[-1])\n",
    "#         model_list.append(cls_sub.load_hdf(cls_sub,group[key]))\n",
    "#     return cls(model_list)\n",
    "\n",
    "# class AdditiveModel(jabble.model.AdditiveModel):\n",
    "#     def load_hdf(cls,group):\n",
    "\n",
    "#         return load_hdf_container(cls,group)\n",
    "        \n",
    "# class CompositeModel(jabble.model.AdditiveModel):\n",
    "#     def load_hdf(cls,group):\n",
    "\n",
    "#         return load_hdf_container(cls,group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6e1ba0bf-9ac6-4cdb-8689-73124419682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"barnards_epoch_inds_o[0 1 2 3]_no_norm..hdf\" (mode r)> AdditiveModel <class 'h5py._hl.group.Group'>\n",
      "AdditiveModel\n",
      "<KeysViewHDF5 ['alphas', 'parameters', 'xs']>\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Unable to synchronously open object (object 'p_val' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[38;5;66;03m# recurse_hdf(group[key])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 12\u001b[0m \u001b[43mload_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[91], line 8\u001b[0m, in \u001b[0;36mload_hdf\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;28mprint\u001b[39m(key)\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m             model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;66;03m# recurse_hdf(group[key])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "Cell \u001b[0;32mIn[86], line 35\u001b[0m, in \u001b[0;36mAdditiveModel.load_hdf\u001b[0;34m(cls, group)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_hdf\u001b[39m(\u001b[38;5;28mcls\u001b[39m,group):\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_hdf_container\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[86], line 29\u001b[0m, in \u001b[0;36mload_hdf_container\u001b[0;34m(cls, group)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m group\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     28\u001b[0m     cls_sub \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m     model_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcls_sub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(model_list)\n",
      "Cell \u001b[0;32mIn[86], line 40\u001b[0m, in \u001b[0;36mCompositeModel.load_hdf\u001b[0;34m(cls, group)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_hdf\u001b[39m(\u001b[38;5;28mcls\u001b[39m,group):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_hdf_container\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[86], line 29\u001b[0m, in \u001b[0;36mload_hdf_container\u001b[0;34m(cls, group)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m group\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m     28\u001b[0m     cls_sub \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(key\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 29\u001b[0m     model_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcls_sub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_hdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(model_list)\n",
      "Cell \u001b[0;32mIn[86], line 23\u001b[0m, in \u001b[0;36mCardinalSplineMixture.load_hdf\u001b[0;34m(cls, group)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_hdf\u001b[39m(\u001b[38;5;28mcls\u001b[39m,group):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(group\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(xs\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxs\u001b[39m\u001b[38;5;124m\"\u001b[39m], p_val\u001b[38;5;241m=\u001b[39m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mp_val\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, p\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/ext3/miniconda3/envs/wobbleenv/lib/python3.11/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m \u001b[43mh5o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:190\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to synchronously open object (object 'p_val' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "def recurse_hdf(file):\n",
    "    print(key)\n",
    "    if isinstance(file[key], h5py.Group):\n",
    "        \n",
    "        recurse_hdf(group[key])\n",
    "\n",
    "def load_hdf(filename):\n",
    "    with h5py.File(filename,'r') as file: \n",
    "        for key in file.keys():\n",
    "            print(file,key,type(file[key]))\n",
    "            if isinstance(file[key], h5py.Group):\n",
    "                print(key)\n",
    "                cls = eval(key.split(']')[-1])\n",
    "                model = cls.load_hdf(cls,file[key])\n",
    "                # recurse_hdf(group[key])\n",
    "    return model\n",
    "\n",
    "load_hdf(dir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95b1cb86-21b2-46a2-b83e-cc59eed73b25",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "invalid load key, 'H'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mjabble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m models \u001b[38;5;241m=\u001b[39m [\u001b[43mjabble\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m]\n",
      "File \u001b[0;32m~/wobble_jax/notebooks/../jabble/model.py:33\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(filename):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28minput\u001b[39m:  \u001b[38;5;66;03m# Overwrites any existing file.\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, 'H'."
     ]
    }
   ],
   "source": [
    "models = [jabble.model.load(file) for file in dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fca1c9-c6c8-4bf4-bbfa-84fb86bd3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     model.fix()\n",
    "#     model[0][0].fit()\n",
    "#     model.display()\n",
    "# models = {\"51peg epoch\": models[0],\\\n",
    "#           \"barns order\": models[1],\\\n",
    "#           \"barns epoch\": models[2],\\\n",
    "#           \"51peg order\": models[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22213a1e-f1ec-49a6-a31b-cf37e7b27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmin = 5680\n",
    "# lmax = 5690\n",
    "# lspace = 2\n",
    "\n",
    "# lrange = np.arange(lmin,lmax+lspace,lspace)\n",
    "\n",
    "# plt_name= \"\"\n",
    "# #'barnards_norm_reg_init_o[5, 6, 7, 8]_p2_w30.000_l100.0.mdl'\n",
    "# #\n",
    "# #'barnards_norm_reg_init_o[66, 67, 68, 69]_p4_w100.000_l100.mdl'\n",
    "# orders = [52]\n",
    "# rv_inds = [-2,-1,0,2]\n",
    "# # model_name_b = os.path.join('..','out','24-10-04','barnards_data_inds_o[50, 51, 52]_p3_w50.0_l10.0.mdl')\n",
    "# # model_name_b = os.path.join('..','out','24-07-02','barnards_all_no_norm.mdl',)\n",
    "# make_order_plot(file_b,models[\"barns epoch\"],lmin,lmax,lrange,orders,rv_inds,cpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269437c-a989-4ab8-bb38-3f9029214644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rv_data = jabble.physics.velocities(models[\"barns order\"][0][0].p)\n",
    "# rv_epoch = jabble.physics.velocities(models[\"barns epoch\"][0][0].p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0ce4b-a67c-41b6-95b3-80e84edab679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders = np.unique(models[\"barns order\"].metadata['orders'])\n",
    "# data, _, _, full_init_shifts, _ = get_dataset(file_b,orders,cpus[0])\n",
    "# datablock, metablock, keys = data.blockify(cpus[0],return_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e221d98-2f2d-41ce-aa80-cd45d1c1d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209cbf4-0084-41c7-97da-1e86590610e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_comparison_plot(times_d,rv_d,err_d,orders_d,times_e,rv_e,err_e,time_t,targ_vel,targ_err,bervs,bervs_d):\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    # data_indices, epoch_indices, order_indices, star_indices = data.blockify_indices(cpus[0])\n",
    "    # estimate_vel_2\n",
    "    \n",
    "    # print(bervs.shape,np.unique(bervs).shape,estimate_vel.shape)\n",
    "    # ov = (estimate_vel_d+bervs)\n",
    "    # ov -= ov.mean()\n",
    "    cmap = matplotlib.colormaps[\"viridis\"]\n",
    "    orders_unq = np.unique(orders_d)\n",
    "    \n",
    "    plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\"color\", plt.cm.viridis(np.linspace(0,1,len(orders))))\n",
    "    for order in orders_unq:\n",
    "        indices = np.array([orders_d == order]).squeeze()\n",
    "        # print(np.sum(indices),indices.shape)\n",
    "        # ax.errorbar(times_d[indices],rv_d[indices] + bervs,err_d[indices],fmt='.',zorder=1,alpha=0.4,ms=2,label='Order Jabble')\n",
    "\n",
    "    uni_times = np.unique(times_d)\n",
    "    ev     = jnp.zeros(len(uni_times))\n",
    "    evvar = jnp.zeros(len(uni_times))\n",
    "    info_d = 1/err_d**2\n",
    "    for iii,time in enumerate(times_e):\n",
    "        indices = times_d == time\n",
    "        # print(np.sum(indices))\n",
    "        # print(np.sum(info_d[indices] < 0.0))\n",
    "        ev = ev.at[iii].set(jnp.dot(info_d[indices],rv_d[indices])/jnp.sum(info_d[indices]))\n",
    "        evvar = evvar.at[iii].set((jnp.dot(info_d[indices],rv_d[indices]**2)/jnp.sum(info_d[indices])) - ev[iii]**2)\n",
    "        # print(evvar[iii],np.mean(1/info_d[indices]))\n",
    "\n",
    "    ev += bervs\n",
    "    ev -= ev.mean()\n",
    "    ax.errorbar(times_e,ev,jnp.sqrt(evvar),fmt='.r',zorder=1,alpha=0.30,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    rv_order = rv_d + bervs_d\n",
    "    rv_order -= rv_order.mean()\n",
    "    ax.errorbar(times_d,rv_order,err_d,fmt='.r',zorder=1,alpha=0.10,ms=2,label='Order Jabble RV')\n",
    "\n",
    "    correct_vel = targ_vel + bervs\n",
    "    correct_vel -= correct_vel.mean()\n",
    "    ax.errorbar(time_t,correct_vel,targ_err,fmt='.k',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "    \n",
    "    temp_vel = rv_e + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(time_t,temp_vel,err_e,fmt='.b',zorder=2,alpha=0.30,ms=2,label='Epoch Jabble RV')\n",
    "\n",
    "    \n",
    "    # fig.legend()\n",
    "    # ax.set_ylim(-40, 40)\n",
    "    fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star Relative Radial Velocities')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"MJD\")\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    plt.savefig(os.path.join(out_dir, \"02-barns_vel_o{}-{}_nobervs_epoch.png\".format(np.min(orders),np.max(orders))))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67628b4d-8b4d-4cef-9cf2-2c607dd0a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_targ_zero_plot(times_d,rv_d,err_d,orders_d,times_e,rv_e,err_e,time_t,targ_vel,targ_err,bervs,bervs_d,metadata):\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    # data_indices, epoch_indices, order_indices, star_indices = data.blockify_indices(cpus[0])\n",
    "    # estimate_vel_2\n",
    "    \n",
    "    # print(bervs.shape,np.unique(bervs).shape,estimate_vel.shape)\n",
    "    # ov = (estimate_vel_d+bervs)\n",
    "    # ov -= ov.mean()\n",
    "    cmap = matplotlib.colormaps[\"viridis\"]\n",
    "    orders_unq = np.unique(orders_d)\n",
    "    \n",
    "    plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\"color\", plt.cm.viridis(np.linspace(0,1,len(orders))))\n",
    "    for order in orders_unq:\n",
    "        indices = np.array([orders_d == order]).squeeze()\n",
    "        # print(np.sum(indices),indices.shape)\n",
    "        # ax.errorbar(times_d[indices],rv_d[indices] + bervs,err_d[indices],fmt='.',zorder=1,alpha=0.4,ms=2,label='Order Jabble')\n",
    "\n",
    "    epoches_span = np.arange(0,len(time_t),dtype=int)\n",
    "    \n",
    "    correct_vel = targ_vel + bervs\n",
    "    correct_vel -= correct_vel.mean()\n",
    "    ax.errorbar(epoches_span,correct_vel-correct_vel,targ_err,fmt='.-k',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "    \n",
    "    uni_times = np.unique(times_d)\n",
    "    ev     = jnp.zeros(len(uni_times))\n",
    "    evvar = jnp.zeros(len(uni_times))\n",
    "    info_d = 1/err_d**2\n",
    "    for iii,time in enumerate(times_e):\n",
    "        indices = times_d == time\n",
    "        # print(np.sum(indices))\n",
    "        # print(np.sum(info_d[indices] < 0.0))\n",
    "        ev = ev.at[iii].set(jnp.dot(info_d[indices],rv_d[indices])/jnp.sum(info_d[indices]))\n",
    "        evvar = evvar.at[iii].set((jnp.dot(info_d[indices],rv_d[indices]**2)/jnp.sum(info_d[indices])) - ev[iii]**2)\n",
    "        # print(evvar[iii],np.mean(1/info_d[indices]))\n",
    "\n",
    "    ev += bervs\n",
    "    ev -= ev.mean()\n",
    "    ax.errorbar(epoches_span,ev - correct_vel,jnp.sqrt(evvar),fmt='.r',zorder=1,alpha=0.30,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    rv_order = rv_d + bervs_d\n",
    "    rv_order -= rv_order.mean()\n",
    "    ax.errorbar(epoches_span[metadata['times']],rv_order - correct_vel[metadata['times']],err_d,fmt='.r',zorder=1,alpha=0.10,ms=2,label='Order Jabble RV')\n",
    "    \n",
    "    temp_vel = rv_e + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(epoches_span,temp_vel - correct_vel,err_e,fmt='.b',zorder=2,alpha=0.30,ms=2,label='Epoch Jabble RV')\n",
    "\n",
    "    \n",
    "    # fig.legend()\n",
    "    # ax.set_ylim(-40, 40)\n",
    "    fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star HARPS Comparison')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"epoch index\")\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    plt.savefig(os.path.join(out_dir, \"02-barns_targ_o{}-{}_nobervs_epoch.png\".format(np.min(orders),np.max(orders))))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a538c-e98b-4d39-91aa-9b34bdf7d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datablock, metablock, keys = dataset.blockify(cpus[0],return_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0159401-3297-435d-a501-437ca57ecc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rv_data = jabble.physics.velocities(models[\"barns order\"][0][0].p)\n",
    "# err_data = get_verr(models[\"barns order\"][0][0].p,f_info_arr_d)\n",
    "# rv_epoch = jabble.physics.velocities(models[\"barns epoch\"][0][0].p)\n",
    "# err_epoch = get_verr(models[\"barns epoch\"][0][0].p,f_info_arr_e)\n",
    "\n",
    "# rv_comparison_plot(models[\"barns order\"].metadata['times'],rv_data,err_data,models[\"barns order\"].metadata['orders'],\\\n",
    "#                    np.array(file_b['dates']),rv_epoch,err_epoch,\\\n",
    "#                    np.array(file_b['dates']),np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "#                    np.array(file_b['bervs']),np.array(file_b['bervs'])[metablock['times']])\n",
    "# rv_targ_zero_plot(models[\"barns order\"].metadata['times'],rv_data,err_data,models[\"barns order\"].metadata['orders'],\\\n",
    "#                    np.array(file_b['dates']),rv_epoch,err_epoch,\\\n",
    "#                    np.array(file_b['dates']),np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "#                    np.array(file_b['bervs']),np.array(file_b['bervs'])[metablock['times']],metablock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369042c-a526-40bb-82d4-975d33754ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dir = glob.glob('../out/24-12-06/b*.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccd075-fa4c-4f69-a812-1da734ee9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2886fb7-9152-4266-a101-3d3fb87f86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_ele(dictionary,ele_i,device):\n",
    "        out = {}\n",
    "        for key in dictionary:\n",
    "            out[key] = jax.device_put(dictionary[key][ele_i],device)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af04e8-3968-4d43-9380-c8b81add2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b.keys(),file_b['bervs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878b1f1-c554-41a9-8eb3-888f6c37d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jabble_file = h5py.File('../out/24-10-23/rv_jabble.hdf5','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13331494-fc48-4798-b5a1-d03b38675a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rv = np.zeros((len(dir),*file_b['bervs'].shape))\n",
    "all_order = np.zeros((len(dir),int(file_b['data'].shape[0]/len(dir))))\n",
    "\n",
    "all_err = np.zeros((len(dir),*file_b['bervs'].shape))\n",
    "all_times = np.array(file_b['dates'])\n",
    "all_models = []\n",
    "\n",
    "all_loss = np.zeros((len(dir),int(file_b['data'].shape[0]/len(dir)),*file_b['data'].shape[1:]))\n",
    "for iii,filename in enumerate(dir):\n",
    "\n",
    "    model = jabble.model.load(filename)\n",
    "    orders = np.unique(model.metadata['orders'])\n",
    "    dataset, init_shifts, airmass, full_init_shifts, times_t = get_dataset(file_b,orders,cpus[0])\n",
    "    datablock, metablock, meta_keys = dataset.blockify(cpus[0],return_keys=True)\n",
    "    all_order[iii,:] = meta_keys['orders']\n",
    "    #print(meta_keys)\n",
    "    for jjj in range(len(dataset)):\n",
    "        datarow = dict_ele(datablock,jjj,cpus[0])\n",
    "        metarow = dict_ele(metablock,jjj,cpus[0])\n",
    "        model.fix()\n",
    "        #model.display()\n",
    "        #print(datarow,metarow)\n",
    "        temp = loss(model.get_parameters(),datarow,metarow,model)\n",
    "        \n",
    "        # dataset.metadata['times'][iii]\n",
    "        all_loss[iii,*np.where(dataset.metadata['orders'][jjj] == meta_keys['orders']),metarow['times'],:] = temp\n",
    "        \n",
    "    \n",
    "    f_info_arr = model[0][0].f_info(model,dataset,loss,cpus[0])\n",
    "    rv_data    = jabble.physics.velocities(model[0][0].p)\n",
    "    err_data   = jabble.physics.get_verr(model[0][0].p,f_info_arr)\n",
    "\n",
    "    all_rv[iii,:] = rv_data\n",
    "    all_err[iii,:] = err_data\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a08f3e-ab74-471e-a105-35a79e8fc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('/scratch/mdd423/wobble_jax/out/24-12-18/barnards_jrv.hdf5','w') as jabble_file:\n",
    "    jabble_file.create_dataset('RV',data=all_rv)\n",
    "    jabble_file.create_dataset('RV_err',data=all_err)\n",
    "    jabble_file.create_dataset('loss',data=all_loss)\n",
    "    jabble_file.create_dataset('times',data=all_times)\n",
    "    jabble_file.create_dataset('order',data=all_order)\n",
    "\n",
    "    rv_comb = np.zeros(file_b['bervs'].shape)\n",
    "    var_comb = np.zeros(file_b['bervs'].shape)\n",
    "    info = 1/jabble_file['RV_err'][:,:]**2\n",
    "    for iii in range(jabble_file['RV'].shape[1]):\n",
    "        mask_1 = ~np.isnan(info[:,iii])\n",
    "        mask_2 = info[:,iii] < 1e10\n",
    "        mask = (mask_1 * mask_2).astype(bool)\n",
    "        mask = ~np.isnan(info[:,iii])\n",
    "        rv_comb[iii]  =   jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii] ) / jnp.sum(info[mask,iii])\n",
    "        var_comb[iii] = ( jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii]**2 ) / jnp.sum(info[mask,iii]) ) - rv_comb[iii]**2\n",
    "    \n",
    "    jabble_file.create_dataset('RV_comb',data=rv_comb)\n",
    "    jabble_file.create_dataset('RV_comb_err',data=np.sqrt(var_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c24d4-5294-4e37-8713-7c95f2f61872",
   "metadata": {},
   "outputs": [],
   "source": [
    "jabble_file =  h5py.File('/scratch/mdd423/wobble_jax/out/24-12-18/barnards_jrv.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec7860-0656-48cc-a7fa-f13cb704fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jabble_file =  h5py.File('../out/24-10-23/rv_jabble.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee885c-a33a-4919-a000-7ebcf8afc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_all_order_plot(times,rv_e,err_e,rv_comb,err_comb,targ_vel,targ_err,bervs,period=3.1533):\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "    temp_vel = targ_vel + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(times % period,temp_vel,targ_err,fmt='.g',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "    \n",
    "    temp_vel = rv_comb + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(times % period,temp_vel,err_comb,fmt='.r',zorder=2,alpha=0.60,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    temp_vel = rv_e.mean(axis=0) + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(times % period,temp_vel,0.0,fmt='.b',zorder=2,alpha=0.60,ms=2,label='Avg RV')\n",
    "\n",
    "    for i in range(rv_e.shape[0]):\n",
    "        temp_vel = rv_e[i,:] + bervs\n",
    "        temp_vel -= temp_vel.mean()\n",
    "        ax.errorbar(times % period,temp_vel,yerr=err_e[i,:],fmt='.k',zorder=1,alpha=0.05,ms=2,label='Order Jabble RV')\n",
    "\n",
    "    # fig.legend()\n",
    "    ax.set_ylim(-200, 200)\n",
    "    # fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star Relative Radial Velocities')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"MJD\")\n",
    "    plt.savefig(os.path.join(out_dir, \"barn_rvs_time.png\"))\n",
    "\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    # plt.savefig(os.path.join(out_dir, \"02-barns_all_order_nobervs_epoch.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d13a8-e5da-423f-900b-5d15989bad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_all_order_harps_plot(times,rv_e,err_e,rv_comb,err_comb,targ_vel,targ_err,bervs,loss):\n",
    "    \n",
    "    epoches_span = np.arange(0,len(times),dtype=int)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    temp_vel = targ_vel + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(epoches_span,temp_vel,targ_err,fmt='.g',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "\n",
    "    temp_vel = rv_comb + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(epoches_span,temp_vel,err_comb,fmt='.r',zorder=2,alpha=0.60,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    temp_vel = rv_e.mean(axis=0) + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(epoches_span,temp_vel,0.0,fmt='.b',zorder=2,alpha=0.60,ms=2,label='Avg RV')\n",
    "\n",
    "    for i in range(rv_e.shape[0]):\n",
    "        temp_vel = rv_e[i,:] + bervs\n",
    "        temp_vel -= temp_vel.mean()\n",
    "        ax.errorbar(epoches_span,temp_vel,yerr=err_e[i,:],fmt='.k',zorder=1,alpha=0.05,ms=2,label='Order Jabble RV')\n",
    "\n",
    "    ax.set_ylim(-200, 200)\n",
    "    # fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star Relative Radial Velocities')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"epoch index\")\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    plt.savefig(os.path.join(out_dir, \"barn_rvs_epoch.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    rv_difference_array = np.zeros(rv_e.shape)\n",
    "    print(rv_comb.shape,rv_e.shape)\n",
    "    for i in range(rv_e.shape[1]):\n",
    "        rv_difference_array[:,i] = rv_comb[i] - rv_e[:,i]\n",
    "\n",
    "    plt.figure(figsize=(12,6),facecolor=(1, 1, 1),dpi=300)\n",
    "    im = plt.imshow(rv_difference_array,interpolation ='nearest',vmin=-100,vmax=100)\n",
    "    plt.xlabel('epoches')\n",
    "    plt.ylabel('chunks')\n",
    "    plt.title('$\\Delta$ RV')\n",
    "    plt.colorbar(im,shrink=0.3)  \n",
    "    plt.savefig(os.path.join(out_dir,'barn_drv.png'))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    loss_mean = np.mean(loss,axis=3).mean(axis=1)\n",
    "    print(loss_mean.shape)\n",
    "\n",
    "    fig, ax = plt.subfigures((2,2),figsize=(12,6),facecolor=(1, 1, 1),dpi=300,height_ratios=[1,4],width_ratios=[4,1],sharex='col',sharey='row')\n",
    "    im = ax[1,0].imshow(loss_mean,interpolation ='nearest',vmin=0,vmax=10)\n",
    "    \n",
    "    \n",
    "    ax[0,1].axis('off')\n",
    "    plt.xlabel('epoches')\n",
    "    plt.ylabel('chunks')\n",
    "    plt.title('$\\chi^2$')\n",
    "    plt.colorbar(im,shrink=0.3)\n",
    "    plt.savefig(os.path.join(out_dir,'barn_obj.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e6bc7-4f2d-4e6d-a52f-843e62f1ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "jabble_file.keys()\n",
    "for key in jabble_file.keys():\n",
    "    print(key,jabble_file[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f18408-ca3b-4802-aa4e-66eb222fe3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac235039-5ec4-4b07-8467-a404e3fa6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = cm.get_cmap(\"Spectral\")\n",
    "cmap = matplotlib.colormaps[\"Spectral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f8cfc-ea27-4557-b29f-c259884a0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_order_plot(dataset,model,lrange,orders,rv_inds,device,plt_name=None):\n",
    "    # model = jabble.model.load(model_name)\n",
    "    data_orders = np.unique(model.metadata[\"orders\"])\n",
    "\n",
    "    model.fit()\n",
    "    model.display()\n",
    "    model.fix()\n",
    "    \n",
    "    # for dataframe in dataset:\n",
    "    #     print(np.exp(np.min(dataset.xs)))\n",
    "    # model_name_b = os.path.join('..','out','24-06-11','peg51_o{}_no_norm.mdl'.format(order))\n",
    "    plt_epoches = []\n",
    "    data, meta, keys = dataset.blockify(device,return_keys=True)\n",
    "    # print(meta[\"times\"].shape,model[0][0].p.shape)\n",
    "    temp = np.argsort(model[0][0].p[meta[\"times\"]])\n",
    "    # which_ones = [0,-1,-2,1]\n",
    "    # which_ones = jax.random.randint(jax.random.PRNGKey(0), shape=(3,), minval=0, maxval=len(temp), dtype=jnp.uint8)\n",
    "    # print( np.unique(model.metadata[\"orders\"]) == orders[0])\n",
    "    for uni_order in orders:\n",
    "        # print(uni_order)\n",
    "        for iii in rv_inds:\n",
    "            # print(temp,temp.dtype,model.metadata[\"orders\"],type(model.metadata[\"orders\"][0]))\n",
    "            # print(dataset.metadata[\"orders\"],np.sum(dataset.metadata[\"orders\"] == uni_order))\n",
    "            args_order = temp[np.array(dataset.metadata[\"orders\"])[temp] == uni_order]\n",
    "            # print(args_order)\n",
    "            # print(len(temp),len(args_order),uni_order,args_order[iii])\n",
    "            # plt_epoches.append(temp[uni_order == model.metadata[\"orders\"]][-1])\n",
    "            plt_epoches.append(int(args_order[iii]))\n",
    "    # plt_epoches = [0,1,2,3]#[*sorted_epoches[-1:],*sorted_epoches[:1]]\n",
    "    \n",
    "    fig, axes = plt.subplots(2,len(plt_epoches),figsize=(4*len(plt_epoches),4),sharex='col',sharey='row',\\\n",
    "                             facecolor=(1, 1, 1),height_ratios=[4,1],dpi=200)\n",
    "\n",
    "    # print(plt_epoches,len(temp))\n",
    "    model.fix()\n",
    "    # model.display()\n",
    "    nothing = model.get_parameters()\n",
    "    for ii, plt_epoch in enumerate(plt_epoches):\n",
    "        # indices = model.metadata['times'] == model.metadata['times'][plt_epoch]\n",
    "\n",
    "        datarow = jabble.loss.dict_ele(data,plt_epoch,device)\n",
    "        metarow = jabble.loss.dict_ele(meta,plt_epoch,device)\n",
    "\n",
    "        # print(metarow['index'],plt_epoch)\n",
    "        \n",
    "        xplot = np.linspace(np.log(lrange.min()),np.log(lrange.max()),\\\n",
    "                            dataset.xs[plt_epoch].shape[0]*10)\n",
    "        # yplot = model[2]([],xplot,plt_epoch)\n",
    "        yplot_norm_tot  = model([],xplot,metarow)\n",
    "        yplot_norm_stel = model[0]([],xplot,metarow)\n",
    "        yplot_norm_tell = model[1]([],xplot,metarow)\n",
    "        # yplot_norm      = model[2]([],xplot,metarow)\n",
    "        # for epoch in np.where(indices):\n",
    "        yhat = model([],dataset.xs[plt_epoch][:],metarow)\n",
    "        axes[0,ii].set_xlim(xplot.min(),xplot.max())\n",
    "\n",
    "        velocity = jabble.physics.velocities(model[0][0].p[plt_epoch])\n",
    "        \n",
    "        # Data\n",
    "        axes[0,ii].plot(datarow[\"xs\"][:],datarow[\"ys\"][:],\\\n",
    "                                 '.k',zorder=100,alpha=0.1,ms=7)\n",
    "\n",
    "        # Stellar Model        \n",
    "        axes[0,ii].plot(xplot,yplot_norm_stel,'-r',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # Telluric Model\n",
    "        axes[0,ii].plot(xplot,yplot_norm_tell,'-b',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # Total\n",
    "        axes[0,ii].plot(xplot,yplot_norm_tot,'-m',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # Norm\n",
    "        # axes[0,ii].plot(xplot,yplot_norm,'-g',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # Theory Model\n",
    "        # theory_ax = axes[0,ii].twinx()\n",
    "        # theory_ax.plot(dataset_theory.xs[0][:],dataset_theory.ys[0][:],'-y',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # theory_ax.set_ylim(-5,5)\n",
    "        # Line List\n",
    "\n",
    "        # shift_by_eye = (np.log(5002) - np.log(5000))\n",
    "        # for line in line_list[1].data[(line_list[1].data[\"Wave\"] > lmin) * (line_list[1].data[\"Wave\"] < lmax)]:\n",
    "        #     print(line[\"Species\"])\n",
    "            # axes[0,ii].vlines(np.log(line[\"Wave\"]) + model[0][0].p[plt_epoch] + shift_by_eye,-100,len(dataset),'k','dashed',alpha=0.4)\n",
    "        \n",
    "        # Residuals\n",
    "        axes[1,ii].step(dataset.xs[plt_epoch],dataset.ys[plt_epoch] - yhat,\\\n",
    "                                 'k',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "\n",
    "        # axes[0,ii].set_title(\"order $ = {}$\".format(model.metadata[\"orders\"][plt_epoch]))\n",
    "        axes[0,ii].set_ylim(-2.5,0.5)\n",
    "        # axes[1,ii].set_ylim(-0.08,0.08)\n",
    "        axes[0,ii].set_xticks([])\n",
    "        axes[0,ii].set_xticks(np.log(lrange[ii,:]))\n",
    "        axes[0,ii].set_xticklabels(['{:0.1f}'.format(x) for x in lrange[ii,:]])\n",
    "        axes[0,ii].set_xlim(np.log(lrange[ii,:].min()),np.log(lrange[ii,:].max()))\n",
    "        # axes[0,ii].set_xlim([np.log(lmin), np.max(lmax)])\n",
    "\n",
    "    # plt.x\n",
    "    # plt.text(1, 1, 'Wavelength ($\\AA$)', ha='center')\n",
    "    # plt_name = os.path.join(out_dir, \"02-spectra_{}_{}-{}.png\".format(os.path.split(model_name)[-1],lmin,lmax))\n",
    "    # plt.savefig(plt_name,dpi=200,bbox_inches='tight')\n",
    "    # fig.suptitle('Barnards Star')\n",
    "    fig.text(0.5, 0.00, 'Wavelength $[\\AA]$', ha='center')\n",
    "    fig.text(0.08, 0.5, 'Normalized Flux', va='center', rotation='vertical')\n",
    "    if plt_name is not None:\n",
    "        plt.savefig(os.path.join(out_dir, plt_name))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226f811-ce6a-4f6e-b2f4-04f5bf802548",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b['xs'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a729ce45-8adc-408f-b219-ec959d26f839",
   "metadata": {},
   "source": [
    "0.025 stellar radii"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33ca706-447b-4ff0-bb71-7da5b6ae7c1c",
   "metadata": {},
   "source": [
    "$1/0.025$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ac635-f8cf-434d-bfa2-619826f31e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 5\n",
    "rv_inds = [0]\n",
    "# rv_data = jabble.physics.velocities(all_models[jjj][0][0].p)\n",
    "orders = np.unique(all_models[model_index].metadata['orders'])\n",
    "print(orders)\n",
    "\n",
    "dataset, _, _, _, _ = get_dataset(file_b,orders,cpus[0])\n",
    "ls_means = np.ceil(np.exp(file_b['xs'][orders].mean(axis=1).mean(axis=1)))\n",
    "lrange = ls_means[:,None] + np.arange(0.0,24.0,4.0)[None,:]\n",
    "print(lrange)\n",
    "make_order_plot(dataset,all_models[model_index],lrange,orders,rv_inds,cpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333adb1-9ff9-41ea-95f5-7c3e6640c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = 1/jabble_file['RV_err'][:,:]**2\n",
    "bervs = np.array(file_b['bervs'])\n",
    "temp = np.array(jabble_file[\"loss\"]) <= 0\n",
    "# print(np.sum(temp))\n",
    "# print(np.array(jabble_file[\"loss\"])[temp])\n",
    "# print(np.stack(np.where(temp)).T)\n",
    "loss_temp = np.array(jabble_file[\"loss\"])\n",
    "loss_temp[temp] = 0.0\n",
    "print(np.sum(loss_temp <= 0)/np.product(loss_temp.shape))\n",
    "loss_mean = np.mean(loss_temp,axis=3).mean(axis=1)\n",
    "\n",
    "looking_ind = np.where(loss_mean <= 0)\n",
    "print(np.stack(looking_ind).T)\n",
    "print(loss_mean[looking_ind])\n",
    "\n",
    "# norm = cmap.LogNorm(np.min(loss_sum),np.max(loss_sum))\n",
    "\n",
    "for iii in range(jabble_file[\"RV\"].shape[1]):\n",
    "    mask_1 = ~np.isnan(info[:,iii])\n",
    "    mask_2 = info[:,iii] < 1e10\n",
    "    mask = (mask_1 * mask_2).astype(bool)\n",
    "    rv_comb = jnp.average(jabble_file['RV'][mask,iii],weights=info[mask,iii])\n",
    "        # rv_comb  =   jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii] ) / jnp.sum(info[mask,iii])\n",
    "    var_comb = jnp.average((jabble_file['RV'][mask,iii])**2,weights=info[mask,iii]) - rv_comb**2\n",
    "    # if (np.all(jabble_file[\"RV\"][:,iii]>rv_comb)) or (np.all(jabble_file[\"RV\"][:,iii]<rv_comb)):\n",
    "    #     if (np.any(info[mask,iii] > 1e10)):\n",
    "    jjjs, = np.where(info[mask,iii] > 1e10)\n",
    "    # print(rv_comb,var_comb, np.min(info[:,iii]), np.max(info[:,iii]))\n",
    "    rv_difference = np.sqrt((rv_comb - jabble_file['RV'][mask,iii])**2)\n",
    "    print(np.max(rv_difference),np.where(rv_difference == rv_difference.max()))\n",
    "    jjjs, = np.where(rv_difference == rv_difference.max())\n",
    "    \n",
    "    # print(loss_sum[:,iii])\n",
    "#     # print(jabble_file[\"RV\"][:,iii],\"\\n\", jabble_file[\"RV_comb\"][iii],jabble_file[\"RV_err\"][:,iii])\n",
    "    \n",
    "#     mask = ~np.isnan(info[:,iii])\n",
    "#     rv_comb = jnp.average(jabble_file['RV'][mask,iii] + bervs[iii],weights=info[mask,iii])\n",
    "#     # rv_comb  =   jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii] ) / jnp.sum(info[mask,iii])\n",
    "#     var_comb = jnp.average((jabble_file['RV'][mask,iii] + bervs[iii])**2,weights=info[mask,iii]) - rv_comb**2\n",
    "#     print((np.all(jabble_file[\"RV\"][:,iii]>rv_comb)) or (np.all(jabble_file[\"RV\"][:,iii]<rv_comb)))\n",
    "#     # print(( jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii]**2 ) / jnp.sum(info[mask,iii]) ) < rv_comb**2)\n",
    "#     print(rv_comb,var_comb)\n",
    "    # plt.scatter(jabble_file[\"RV\"][mask,iii],info[mask,iii],alpha=0.4,zorder=1,c=cmap(loss_mean[mask,iii]))\n",
    "    # plt.scatter([rv_comb],[1/var_comb],alpha=0.8,zorder=2,c='blue')\n",
    "    # plt.colorbar()\n",
    "    # plt.xlabel(\"RV\")\n",
    "    # plt.ylabel(\"RV Info\")\n",
    "    # plt.yscale(\"log\")\n",
    "    # plt.show()\n",
    "\n",
    "    # for jjj in jjjs:\n",
    "    #     print(jjj)\n",
    "    #     rv_data = jabble.physics.velocities(all_models[jjj][0][0].p)\n",
    "    #     orders = np.unique(all_models[jjj].metadata['orders'])\n",
    "    #     lmean = np.mean(np.exp(all_models[jjj][0][1].xs))\n",
    "    #     lmin = lmean-10\n",
    "    #     lmax = lmean+10\n",
    "    #     lspace = 4\n",
    "        \n",
    "    #     lrange = np.arange(lmin,lmax+lspace,lspace)\n",
    "        \n",
    "    #     rv_inds = [-1,0,1]\n",
    "    #     make_order_plot(file_b,all_models[jjj],lmin,lmax,lrange,[orders[1]],rv_inds,cpus[0])\n",
    "# #     # print(np.sum(np.isnan(info[:,iii])))\n",
    "#     # if (np.any(jabble_file[\"RV_err\"][:,iii]<0)):\n",
    "#     #     print(jabble_file[\"RV\"][:,iii],\"\\n\", jabble_file[\"RV_comb\"][iii],jabble_file[\"RV_err\"][:,iii])\n",
    "#     #     mask = ~np.isnan(info[:,iii])\n",
    "    #     rv_comb  =   jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii] ) / jnp.sum(info[mask,iii])\n",
    "    #     var_comb = ( jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii]**2 ) / jnp.sum(info[mask,iii]) ) - rv_comb**2\n",
    "    #     print(( jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii]**2 ) / jnp.sum(info[mask,iii]) ) < rv_comb**2)\n",
    "    #     print(rv_comb,np.sqrt(var_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509b2d5-2e4d-494b-9a58-7b4c56c1b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(np.array(jabble_file['RV_comb_err'])), np.average(np.array(file_b['pipeline_sigmas']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331987a1-44f2-45c5-b0cf-1b131928841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_all_order_plot(np.array(file_b['dates']),np.array(jabble_file['RV']),np.array(jabble_file['RV_err']),\\\n",
    "                  np.array(jabble_file['RV_comb']),np.array(jabble_file['RV_comb_err']),\\\n",
    "                  np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "                  np.array(file_b['bervs']),period=1e10)\n",
    "rv_all_order_harps_plot(np.array(file_b['dates']),np.array(jabble_file['RV']),np.array(jabble_file['RV_err']),\\\n",
    "                        np.array(jabble_file['RV_comb']),np.array(jabble_file['RV_comb_err']),\\\n",
    "                        np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "                        np.array(file_b['bervs']),np.array(jabble_file['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf5fdb-b2a9-4296-9ab8-c3bbc085b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nifty_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e886dd64-be75-45e0-862a-ec0d4ff4fb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(np.array(file_b['dates']),np.array(jabble_file['RV_comb']) + np.array(file_b['bervs']),yerr=np.array(jabble_file['RV_comb_err']),fmt='.k')\n",
    "plt.show()\n",
    "plt.errorbar(np.array(file_b['dates']),np.array(file_b['pipeline_rvs']) + np.array(file_b['bervs']),yerr=np.array(file_b['pipeline_sigmas']),fmt='.k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b7433-0833-49d0-9b6f-f45fb88b38c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nifty_ls_diagram(dates,rvs,error,title,plt_name):\n",
    "    nifty_res = nifty_ls.lombscargle(dates, rvs, dy=error, fmin=0.01, fmax=10)\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(nifty_res.freq(),nifty_res.power,'-m',markersize=0.2,alpha=0.7)\n",
    "    plt.xlabel('Frequency (Days)')\n",
    "    plt.ylabel('Power')\n",
    "    plt.title(title)\n",
    "    plt.savefig(os.path.join(out_dir, plt_name))\n",
    "    plt.show()\n",
    "\n",
    "# print(np.array(file_b['dates']).shape,np.array(jabble_file['RV']).shape,np.array(jabble_file['RV_err']).shape)\n",
    "# print(np.array(file_b['dates']).shape,np.array(jabble_file['RV_comb']).shape,np.array(jabble_file['RV_comb_err']).shape)\n",
    "plt_name = 'ls_jabble_noberv.png'\n",
    "nifty_ls_diagram(np.array(file_b['dates']),np.array(jabble_file['RV_comb']) + np.array(file_b['bervs']),np.array(jabble_file['RV_comb_err']),title=\"Jabble - BERV\",plt_name=plt_name)\n",
    "plt_name = 'ls_hires_noberv.png'\n",
    "nifty_ls_diagram(np.array(file_b['dates']),np.array(file_b['pipeline_rvs']) + np.array(file_b['bervs']),np.array(file_b['pipeline_sigmas']),title=\"HIRES - BERV\",plt_name=plt_name)\n",
    "plt_name = 'ls_berv.png'\n",
    "nifty_ls_diagram(np.array(file_b['dates']),np.array(file_b['bervs']),None,title=\"BERVS\",plt_name=plt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2665fcf-7af9-47c1-a391-6751b0ac23e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models[-1].results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad513e-7026-43c5-bb72-cf52acda7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nifty_ls\n",
    "from astropy.timeseries import LombScargle\n",
    "frequency, power = LombScargle(np.array(file_b['dates']), jabble_file['RV_comb']).autopower(method=\"fastnifty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda82484-10f4-482d-a4e8-7afe7a01c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_earth_residual_img(model,dataset,lrange,orders,rest_shifts,residual_resolution,plt_name,line_list,device):\n",
    "    xrange = np.log(lrange)\n",
    "    xmin, xmax = np.min(xrange), np.max(xrange)\n",
    "    # xinds = ((dataset[0].xs[:] < xmax) * (dataset[0].xs[:] > xmin)).astype(bool)\n",
    "    residual_img = np.zeros((len(dataset),residual_resolution))\n",
    "    fig, ax = plt.subplots(2,2,figsize=(8, 8),height_ratios=[1,4],width_ratios=[4,1],sharex='col',sharey='row')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    data, meta, keys = dataset.blockify(device,return_keys=True)\n",
    "\n",
    "    max_shift, min_shift = np.max(rest_shifts), np.min(rest_shifts)\n",
    "    new_grid = np.linspace(xmin,xmax,residual_resolution)\n",
    "\n",
    "    epsilon = np.log(np.mean(lrange) + 1) - np.log(np.mean(lrange))\n",
    "    model.fix()\n",
    "    model.display()\n",
    "\n",
    "    # print(np.sum(np.log(line_list[1].data[\"Wave\"]) > xmin),np.sum(np.log(line_list[1].data[\"Wave\"]) < xmax),\\\n",
    "    #      np.sum((np.log(line_list[1].data[\"Wave\"]) > xmin)*(np.log(line_list[1].data[\"Wave\"]) < xmax)))\n",
    "    # for line in line_list[1].data[(np.log(line_list[1].data[\"Wave\"]) > xmin) * (np.log(line_list[1].data[\"Wave\"]) < xmax)]:\n",
    "    #     print(line[\"Species\"])\n",
    "    #     ax[1,0].vlines(np.log(line[\"Wave\"]),0,len(dataset))\n",
    "    # for line in list_list\n",
    "    for i,plt_epoch in enumerate(range(len(dataset))):\n",
    "        datarow = jabble.loss.dict_ele(data,plt_epoch,device)\n",
    "        metarow = jabble.loss.dict_ele(meta,plt_epoch,device)\n",
    "        \n",
    "        xless = (dataset[plt_epoch].xs[:] <= (xmax + epsilon))\n",
    "        xmore = (dataset[plt_epoch].xs[:] >= (xmin - epsilon)) #+ rest_shifts[plt_epoch] \n",
    "        xinds = (xless * \\\n",
    "                 xmore).astype(bool)\n",
    "        # print(np.sum(xmore),np.sum(xless))\n",
    "        x_grid = dataset[plt_epoch].xs[(~dataset[plt_epoch].mask)*xinds]\n",
    "        y_grid = dataset[plt_epoch].ys[(~dataset[plt_epoch].mask)*xinds]\n",
    "        residual = (y_grid - model([],x_grid,metarow))#*jnp.sqrt(dataset[plt_epoch].yivar[(~dataset[plt_epoch].mask)*xinds])\n",
    "\n",
    "        if np.sum(xless) == 0:\n",
    "            x_grid = np.array([dataset[plt_epoch].xs[~dataset[plt_epoch].mask].min()])\n",
    "            residual = np.array([0.0])\n",
    "        if np.sum(xmore) == 0:\n",
    "            x_grid = np.array([dataset[plt_epoch].xs[~dataset[plt_epoch].mask].max()])\n",
    "            residual = np.array([0.0])\n",
    "        # print(residual.shape,np.sum(dataset[plt_epoch].mask*xinds),np.sum(xinds),np.sum(dataset[plt_epoch].mask))\n",
    "        residual_img[i,:] = scipy.interpolate.interp1d(x_grid,residual,kind='nearest',bounds_error=False,fill_value=0.0)(new_grid )#+ \\rest_shifts[plt_epoch]\n",
    "    cmap = plt.get_cmap(\"RdBu\")\n",
    "\n",
    "    ax[0,1].axis('off')\n",
    "    ax[0,0].step(new_grid,       (residual_img**2).sum(axis=0),'k',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "    ax[0,0].step(new_grid,       (residual_img).sum(axis=0),'m',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "    ax[1,1].step((residual_img**2).sum(axis=1),np.arange(len(dataset))[::-1],'k',where='post',zorder=1,alpha=0.3,ms=3)\n",
    "    \n",
    "    \n",
    "    # ax[1,0].set_ylim(0,np.max(orders)+1)\n",
    "    ax[1,0].set_xlim(xmin,xmax)\n",
    "    extent = [xmin,xmax,0,len(dataset)+1]\n",
    "    ax[1,0].imshow(residual_img,cmap=cmap,aspect=\"auto\",vmin=-.1,vmax=.1,extent=extent,interpolation='nearest')\n",
    "    ax[1,0].set_xlabel('Wavelength [$\\AA$]')\n",
    "    ax[1,0].set_ylabel('Chunks')\n",
    "    # # plt.xticks([])\n",
    "    ax[1,0].set_xticks(xrange)\n",
    "    ax[1,0].set_xticklabels(['{:0.1f}'.format(l) for l in lrange])\n",
    "    # ax[1,0].get_shared_x_axes().join(ax[1,0], ax[1,1])\n",
    "    # plt.xlabel()\n",
    "    if plt_name is not None:\n",
    "        plt.savefig(os.path.join(out_dir, plt_name))\n",
    "    plt.show()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # worst_epochs = np.zeros(len(orders),dtype=bool)\n",
    "    # worst_epochs = (residual_img**2).sum(axis=1) > 0.7\n",
    "    # print(worst_epochs)\n",
    "    # print(orders)\n",
    "    # print(orders[worst_epochs])\n",
    "    # return orders[worst_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb6c5d3-23ca-4834-ae09-9c08d2b4d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_index = 4\n",
    "# rv_data = jabble.physics.velocities(all_models[jjj][0][0].p)\n",
    "orders = np.unique(all_models[model_index].metadata['orders'])\n",
    "print(orders)\n",
    "order_index = 1\n",
    "\n",
    "dataset, _, _, _, _ = get_dataset(file_b,orders,cpus[0])\n",
    "ls_means = np.exp(file_b['xs'][orders].mean(axis=1).mean(axis=1))[order_index]\n",
    "lrange = np.ceil(ls_means) + np.arange(0.0,20.0,4.0)\n",
    "\n",
    "data, meta = dataset.blockify(cpus[0])\n",
    "rest_shifts = all_models[model_index][0][0].p\n",
    "index_sort = np.argsort(rest_shifts)\n",
    "residual_resolution = 2048\n",
    "plt_name = \"res_img_barn_o{}-{}_l{}-{}_.png\".format(orders,orders[order_index],np.min(lrange),np.max(lrange))\n",
    "plot_earth_residual_img(all_models[model_index],dataset,lrange,index_sort,rest_shifts,residual_resolution,plt_name,None,device=cpus[0])\n",
    "\n",
    "rv_inds = [0,1,2,3]\n",
    "lrange_stack = np.ones(len(rv_inds))[:,None] * lrange[None,:]\n",
    "plt_name = \"spectra_barn_o{}-{}_l{}-{}_.png\".format(orders,orders[order_index],np.min(lrange),np.max(lrange))\n",
    "make_order_plot(dataset,all_models[model_index],lrange_stack,[orders[order_index]],rv_inds,cpus[0],plt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4ae3a-615b-4c5a-9a46-c1f636ed3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmin = 5580\n",
    "lmax = 5585\n",
    "lspacing = 1\n",
    "residual_resolution = 2048\n",
    "lrange = np.arange(lmin,lmax+lspacing,lspacing)\n",
    "plt_name =  \"02-{}_res_img_starref.png\".format(os.path.split(model_name_b)[-1],np.min(orders),np.max(orders))\n",
    "\n",
    "rest_vel = model_b[0][0].p#np.zeros(.shape)\n",
    "index_sort = np.argsort(rest_vel)\n",
    "rest_shifts =  model_b[0][0].p#jabble.physics.shifts(rest_vel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc66d8a-0c7f-4899-8ffe-1b68a98e21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for filename in dir:\n",
    "#     model = jabble.model.load(filename)\n",
    "#     rv_data = jabble.physics.velocities(model[0][0].p)\n",
    "#     orders = np.unique(model.metadata['orders'])\n",
    "#     lmean = np.mean(np.exp(model[0][1].xs))\n",
    "#     lmin = lmean-10\n",
    "#     lmax = lmean+10\n",
    "#     lspace = 4\n",
    "    \n",
    "#     lrange = np.arange(lmin,lmax+lspace,lspace)\n",
    "    \n",
    "#     rv_inds = [-1,0,1]\n",
    "#     make_order_plot(file_b,model,lmin,lmax,lrange,[orders[1]],rv_inds,cpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d43831-f646-436d-8765-64a434e0b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def imshow_rv_difference(all_models,times_t,times_e,rv_e,err_e,bervs,):\n",
    "    \n",
    "#     rv_array = np.zeros((len(all_models),*all_models[0][0][0].p.shape))\n",
    "#     order_array = []\n",
    "\n",
    "#     info_e = 1/err_e**2\n",
    "#     epoches_span_e = np.zeros(len(rv_e))\n",
    "\n",
    "#     rv_comb = jnp.array(len(times_t))\n",
    "#     for iii,time in enumerate(times_t):\n",
    "#         indices = times_e == time\n",
    "\n",
    "#         rv_indiv[indices] = rv_e[indices] + bervs[iii] - correct_vel[iii]\n",
    "#         epoches_span_e[indices] = iii\n",
    "#         rv_comb = rv_comb.at[iii].set(jnp.dot(info_e[indices],rv_e[indices])/jnp.sum(info_e[indices]))\n",
    "#         info_comb = info_comb.at[iii].set((jnp.dot(info_e[indices],rv_e[indices]**2)/jnp.sum(info_e[indices])) - rv_comb[iii]**2)\n",
    "\n",
    "\n",
    "#     for i,model in enumerate(all_models):\n",
    "#         # data, _, _, full_init_shifts, _ = get_dataset(file_b,orders,cpus[0])\n",
    "#         datablock, metablock, keys = data.blockify(cpus[0],return_keys=True)\n",
    "#         rv_array[i,:] = jabble.physics.velocities(model[0][0].p)\n",
    "#         order_array.append(np.unique(model.metadata['orders']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7611249-62d4-40b9-a5db-3fa33b5e31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imshow_rv_difference(all_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6654272-8ed1-4917-ba8d-8c50ba250662",
   "metadata": {},
   "source": [
    "5577 night sky line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8278831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = 'barnardsvmapmodel1.mdl'\n",
    "# # model = jabble.model.load(modelname)\n",
    "# jabble.model.save(modelname,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef9439-9570-4979-8330-be3ef582835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_plot(model,dataset,init_shifts,filename):\n",
    "#     x_window = np.log(4550) - np.log(4549)\n",
    "#     lmin = np.exp(dataset.xs[0,500])\n",
    "#     lmax = np.exp(dataset.xs[0,1500])\n",
    "#     lrange = np.arange(lmin,lmax,5)\n",
    "#     plt_unit = u.Angstrom\n",
    "#     epoches = 25\n",
    "#     r_plots = 5\n",
    "\n",
    "#     vel_epoch = 5\n",
    "#     fig, axes = plt.subplots(\n",
    "#         epoches // r_plots,\n",
    "#         r_plots,\n",
    "#         figsize=(8, 8),\n",
    "#         sharex=False,\n",
    "#         sharey=True,\n",
    "#         facecolor=(1, 1, 1),\n",
    "#         dpi=200,\n",
    "#     )\n",
    "#     # fig.suptitle(filenames[model_num])\n",
    "#     for plt_epoch in range((epoches // r_plots) * r_plots):\n",
    "#         xplot = np.linspace(np.log(lmin), np.log(lmax), dataset.xs.shape[1] * 10)\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xlim(\n",
    "#             xplot.min() + model[0][0].p[plt_epoch],\n",
    "#             xplot.max() + model[0][0].p[plt_epoch],\n",
    "#         )\n",
    "\n",
    "#         # model_set[model_num].fix()\n",
    "#         # model_set[model_num].fit(0)\n",
    "#         # rv_model_deriv = jax.jacfwd(model_set[model_num], argnums=0)(model_set[model_num].get_parameters(),dataset.xs[plt_epoch,:],plt_epoch)\n",
    "#         # rv_loss_deriv = jax.jacfwd(loss, argnums=0)(model_set[model_num].get_parameters(),datasets[0],vel_epoch,model_set[model_num])\n",
    "\n",
    "#         model.fix()\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].errorbar(\n",
    "#             dataset.xs[plt_epoch, :],\n",
    "#             dataset.ys[plt_epoch, :],\n",
    "#             dataset.yerr[plt_epoch, :],\n",
    "#             fmt=\".k\",\n",
    "#             elinewidth=1.2,\n",
    "#             zorder=1,\n",
    "#             alpha=0.5,\n",
    "#             ms=3,\n",
    "#         )\n",
    "\n",
    "#         # true_model.fix()\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(\n",
    "#             xplot,\n",
    "#             model([], xplot, plt_epoch),\n",
    "#             \"-r\",\n",
    "#             linewidth=1.2,\n",
    "#             zorder=2,\n",
    "#             alpha=0.5,\n",
    "#             ms=6,\n",
    "#         )\n",
    "#         # axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(xplot,true_model([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=1,alpha=0.5,ms=6)\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_ylim(-2, 1)\n",
    "#         #         axes[i,j].set_yticks([])\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xticks(np.log(lrange))\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xticklabels(\n",
    "#             [\"{:2.0f}\".format(x) for x in lrange]\n",
    "#         )\n",
    "\n",
    "#         res_ax = axes[plt_epoch // r_plots, plt_epoch % r_plots].twinx()\n",
    "#         residual = loss(\n",
    "#             model.get_parameters(),\n",
    "#             dataset,\n",
    "#             plt_epoch,\n",
    "#             model,\n",
    "#         )\n",
    "#         res_ax.step(\n",
    "#             dataset.xs[plt_epoch, :], residual, where=\"mid\", alpha=0.3, label=\"residual\"\n",
    "#         )\n",
    "#         res_ax.set_ylim(0.0, 20)\n",
    "#         res_ax.set_yticks([])\n",
    "#         # res_ax.step(model_set[i][j][1].xs+model_set[i][j][0].p[plt_epoch],\\\n",
    "#         #             model_set[i][j].results[-2]['grad'][:],\\\n",
    "#         #             where='mid',alpha=0.4,label='residual',zorder=-1)\n",
    "#         # res_ax.set_yticks([])\n",
    "\n",
    "#         # res_ax.step(x_grid,\\\n",
    "#         #             rv_model_deriv[:,plt_epoch],\\\n",
    "#         #             where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "\n",
    "#         #     res_ax.step(x_grid,\\\n",
    "#         #                 rv_loss_deriv[:,plt_epoch],\\\n",
    "#         #                 where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "\n",
    "#         #     align_yaxis(, 0, , 0)\n",
    "\n",
    "#         align.yaxes(\n",
    "#             axes[plt_epoch // r_plots, plt_epoch % r_plots], 0.0, res_ax, 0.0, 2.0 / 3.0\n",
    "#         )\n",
    "\n",
    "#     # res.get_shared_y_axes().join(ax1, ax3)\n",
    "#     fig.text(0.5, 0.04, \"$\\lambda$\", ha=\"center\")\n",
    "#     fig.text(0.04, 0.5, \"y\", va=\"center\", rotation=\"vertical\")\n",
    "#     # fig.text(0.96, 0.5, '$d \\L /d \\delta x$', va='center', rotation=270)\n",
    "#     # fig.text(0.96, 0.5, '$d f_{{{ji}}} /d \\delta x_k$', va='center', rotation=270)\n",
    "#     fig.text(0.96, 0.5, \"residuals\", va=\"center\", rotation=270)\n",
    "\n",
    "#     plt.savefig(\n",
    "#         os.path.join(out_dir, \"02-res_{}.png\".format(filename)),\n",
    "#         dpi=300,\n",
    "#         bbox_inches=\"tight\",\n",
    "#     )\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8f735-8aca-4ee1-b4da-ea56f90eaa10",
   "metadata": {},
   "source": [
    "6563 h alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bae852-c141-4694-bc48-e9782d1cdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['51peg','barnards']\n",
    "make_plot(model_b,dataset_b,shifts_b,filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f3af0c-2dd6-407c-a897-9559c565d1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/models_1721076003/bt-settl/lte032-5.0-0.5a+0.2.BT-NextGen.7.dat.txt', 'r') as f:\n",
    "    # text = f.read()\n",
    "    cnt = 0\n",
    "    for line in f:\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt > 8:\n",
    "            print([np.double(x) for x in line.split(\"    \")[-2:]])\n",
    "\n",
    "        if cnt > 12:\n",
    "            break\n",
    "        \n",
    "\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f721096-7dd7-4376-93e7-8b41bfadab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_better_plot(model_set,datasets,file_set):\n",
    "    \n",
    "\n",
    "#     fig, axes = plt.subplots(2*len(model_set),4,figsize=(4*4,4*len(model_set)),sharex=True,facecolor=(1, 1, 1),dpi=200,height_ratios=[4,1]*len(model_set))\n",
    "    \n",
    "#     for jj,(model,dataset,file) in enumerate(zip(model_set,datasets,file_set)):\n",
    "#         x_window = np.log(4550) - np.log(4549)\n",
    "#         lmin = np.exp(dataset.xs[0,0])\n",
    "#         lmax = np.exp(dataset.xs[0,2000])\n",
    "#         lrange = np.arange(lmin,lmax,5)\n",
    "#         sort_airmasses = np.argsort(np.array(file['airms'][:]))\n",
    "#         plt_epochs = np.concatenate((sort_airmasses[:2],sort_airmasses[-2:]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         offset = 1.0\n",
    "#         xplot = np.linspace(np.log(lmin)-x_window,np.log(lmax)+x_window,dataset.xs.shape[1]*10)\n",
    "#         for ii,plt_epoch in enumerate(plt_epochs):\n",
    "#             axes[2*jj,ii].set_xlim(xplot.min()+model[0][0].p[plt_epoch],xplot.max()+model[0][0].p[plt_epoch])\n",
    "            \n",
    "#             model.fix()\n",
    "            \n",
    "#             axes[2*jj,ii].errorbar(dataset.xs[plt_epoch,:],dataset.ys[plt_epoch,:],\\\n",
    "#                                      dataset.yerr[plt_epoch,:],fmt='.k',elinewidth=1.2,zorder=1,alpha=0.5,ms=3)\n",
    "            \n",
    "#             axes[2*jj,ii].plot(xplot,offset + model[0]([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             axes[2*jj,ii].plot(xplot,2*offset + model[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             axes[2*jj,ii].plot(xplot,model[2]([],xplot,plt_epoch),'-m',linewidth=1.2,zorder=3,alpha=0.7,ms=6)\n",
    "            \n",
    "#             # axes[0,ii].plot(xplot,2*offset + model[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             # axes[0,ii].plot(xplot,offset + model([],xplot,plt_epoch),'-g',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             # axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(xplot,true_model([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=1,alpha=0.5,ms=6)\n",
    "            \n",
    "            \n",
    "#             axes[2*jj,ii].set_ylim(-2,3)\n",
    "#             axes[2*jj,ii].set_xticks([])\n",
    "#             # axes[0].set_yticks([])\n",
    "#             axes[2*jj+1,ii].set_xticks(np.log(lrange))\n",
    "#             axes[2*jj+1,ii].set_xticklabels(['{:2.0f}'.format(x) for x in lrange])\n",
    "            \n",
    "#             axes[2*jj+1,ii].plot(dataset.xs[plt_epoch,:],dataset.ys[plt_epoch,:] - model([],dataset.xs[plt_epoch,:],plt_epoch),'.k',alpha=0.4,ms=1)\n",
    "            \n",
    "#             axes[2*jj+1,ii].set_ylim(-0.1,0.1)\n",
    "#             axes[2*jj,ii].set_title('airmass = {}'.format(file['airms'][:][plt_epoch]))\n",
    "#         # res_ax = axes[plt_epoch // r_plots, plt_epoch % r_plots].twinx()\n",
    "#         # residual = loss(model_set[model_num].get_parameters(),dataset,plt_epoch,model_set[model_num])\n",
    "#         # res_ax.step(dataset.xs[plt_epoch,:],residual,where='mid',alpha=0.3,label='residual')\n",
    "#         # res_ax.set_ylim(0.0,20)\n",
    "#         # res_ax.set_yticks([])\n",
    "#         # res_ax.step(model_set[i][j][1].xs+model_set[i][j][0].p[plt_epoch],\\\n",
    "#         #             model_set[i][j].results[-2]['grad'][:],\\\n",
    "#         #             where='mid',alpha=0.4,label='residual',zorder=-1)\n",
    "#         # res_ax.set_yticks([])\n",
    "        \n",
    "#         # res_ax.step(x_grid,\\\n",
    "#         #             rv_model_deriv[:,plt_epoch],\\\n",
    "#         #             where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "            \n",
    "#         #     res_ax.step(x_grid,\\\n",
    "#         #                 rv_loss_deriv[:,plt_epoch],\\\n",
    "#         #                 where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "            \n",
    "#         #     align_yaxis(, 0, , 0)\n",
    "            \n",
    "#             # align.yaxes(axes[plt_epoch // r_plots, plt_epoch % r_plots], 0.0, res_ax, 0.0, 2./3.)\n",
    "        \n",
    "#         # res.get_shared_y_axes().join(ax1, ax3)\n",
    "#         fig.text(0.5, 0.04, '$\\lambda$', ha='center')\n",
    "#         # fig.text(0.04, 0.5, 'y', va='center', rotation='vertical')\n",
    "#         # fig.text(0.96, 0.5, '$d \\L /d \\delta x$', va='center', rotation=270)\n",
    "#         # fig.text(0.96, 0.5, '$d f_{{{ji}}} /d \\delta x_k$', va='center', rotation=270)\n",
    "#         # fig.text(0.96, 0.5, 'residuals', va='center', rotation=270)\n",
    "    \n",
    "#     # plt.savefig(os.path.join(out_dir,'02-full-barn-51peg.png'),dpi=300,bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04116c4-5bcc-473c-9323-90f440ebd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_better_plot(model_set,datasets,file_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e991d2-9c6e-449b-8c34-b9989fa1cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell_loss = [[],[]]\n",
    "# for jjj, (dataset, model) in enumerate(zip(datasets,model_set)):\n",
    "#     for iii in range(dataset.ys.shape[0]):\n",
    "#         tell_loss[jjj].append(loss([],dataset,iii,model[0]).sum())\n",
    "\n",
    "# plt.plot(np.array(file_p['airms'][:]),tell_loss[0],'.k',label='51 peg')\n",
    "# plt.plot(np.array(file_b['airms'][:]),tell_loss[1],'.r',label='barnards')\n",
    "# # plt.ylim(0.0,5e4)\n",
    "\n",
    "# # plt.plot(np.array(file_p['airms'][:]),model_p[1][1].p,'.k',label='51 peg')\n",
    "# # plt.plot(np.array(file_b['airms'][:]),model_b[1][1].p,'.r',label='barnards')\n",
    "# plt.xlabel('airmass')\n",
    "# plt.ylabel('$\\Sigma_* (y_* - \\hat{y}_s(x_*)) I_{y*}$')\n",
    "# # plt.plot()\n",
    "# plt.legend()\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass_loss.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cecc1d-a1f4-45fb-9bcd-b320c59182e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.array(file_p['airms'][:]),model_p[1][1].p,'.k',label='51 peg')\n",
    "# plt.plot(np.array(file_b['airms'][:]),model_b[1][1].p,'.r',label='barnards')\n",
    "# plt.xlabel('airmass')\n",
    "# plt.ylabel('~a')\n",
    "# x_space = np.linspace(np.min(np.array(file_b['airms'][:])),np.max(np.array(file_b['airms'][:])))\n",
    "# plt.plot(x_space,x_space,'-.k',alpha=0.3)\n",
    "# plt.legend()\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass_an.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2dd7d-b813-4d62-a105-87bc9806bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1,figsize=(4,4),sharex=True,facecolor=(1, 1, 1),dpi=200)\n",
    "\n",
    "# plt_epoch = 10\n",
    "# x_window = np.log(4550) - np.log(4549)\n",
    "# lmin = np.exp(dataset_p.xs[0,500])\n",
    "# lmax = np.exp(dataset_p.xs[0,1500])\n",
    "# lrange = np.arange(lmin,lmax,5)\n",
    "# xplot = np.linspace(np.log(lmin)-x_window,np.log(lmax)+x_window,dataset_p.xs.shape[1]*10)\n",
    "# axes.plot(xplot,model_p[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.6,ms=6,label='51 peg')\n",
    "# axes.plot(xplot,0.05 + model_b[1]([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=2,alpha=0.6,ms=6,label='barnard')\n",
    "# axes.legend()\n",
    "\n",
    "# axes.set_ylim(-0.2,0.1)\n",
    "# axes.set_xticks([])\n",
    "# axes.set_ylabel('log flux + offset')\n",
    "# axes.set_xlabel('$\\lambda$')\n",
    "# axes.set_xticks(np.log(lrange))\n",
    "# axes.set_xticklabels(['{:2.0f}'.format(x) for x in lrange])\n",
    "# plt.title('just tellurics')\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass-tell.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
