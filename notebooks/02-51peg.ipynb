{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "854ca1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/wobbleenv/lib/python3.11/site-packages/jax/_src/api_util.py:174: SyntaxWarning: Jitted function has static_argnums=(3, 4), but only accepts 4 positional arguments. This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has {argnums_name}={argnums}, \"\n",
      "/ext3/miniconda3/envs/wobbleenv/lib/python3.11/site-packages/jax/_src/api_util.py:174: SyntaxWarning: Jitted function has static_argnums=(3, 4, 5), but only accepts 5 positional arguments. This warning will be replaced by an error after 2022-08-20 at the earliest.\n",
      "  warnings.warn(f\"Jitted function has {argnums_name}={argnums}, \"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import jabble.model\n",
    "import jabble.dataset\n",
    "import jabble.loss\n",
    "import jabble.physics \n",
    "import astropy.units as u\n",
    "\n",
    "import h5py \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize\n",
    "\n",
    "# from jaxopt import GaussNewton\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpy as np\n",
    "# from mpl_axes_aligner import align\n",
    "\n",
    "import os\n",
    "import jabble.physics\n",
    "\n",
    "import jax.config\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "jax.config.update(\"jax_disable_jit\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d1a69-9b8d-445a-8c96-ffe4d4833863",
   "metadata": {},
   "source": [
    "<h1>02 - Fitting RVs to HARPS e2ds Spectra</h1>\n",
    "In this notebook, I am will show you how to load in data and metadata to jabble.data objects. Then fit models to each component of the data. Due to some edge errors with normalizing near the edge of the orders in the HARPS spectrograph, we fit a longer wavelength component normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c72b75-906c-48c1-b11d-f1cb48103260",
   "metadata": {},
   "source": [
    "Create output directory here as the current date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9961a7-5035-4cc7-a8b1-1be56ad0c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "today = datetime.date.today()\n",
    "out_dir = os.path.join('..','out',today.strftime(\"%y-%m-%d\"))\n",
    "os.makedirs(out_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e260ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b = h5py.File(\"../data/barnards_e2ds.hdf5\", \"r\")\n",
    "# file_p = h5py.File(\"../data/51peg_e2ds.hdf5\"   , \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5fd5722-2bc5-4c0b-b9c9-9c549405956d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax:    0.4.23\n",
      "jaxlib: 0.4.23\n",
      "numpy:  1.24.3\n",
      "python: 3.11.7 | packaged by conda-forge | (main, Dec 23 2023, 14:43:09) [GCC 12.3.0]\n",
      "jax.devices (1 total, 1 local): [cuda(id=0)]\n",
      "process_count: 1\n",
      "\n",
      "$ nvidia-smi\n",
      "Mon Nov 25 16:47:23 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.154.05             Driver Version: 535.154.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              55W / 300W |    311MiB / 32768MiB |      1%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1467411      C   ...iniconda3/envs/wobbleenv/bin/python      308MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jax.print_environment_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712d1c5-0502-4549-b18d-0b3e28440511",
   "metadata": {},
   "source": [
    "<h2>Jabble.Dataset</h2>\n",
    "The 4 fields needed to create a jabble.data object are: xs, ys, yivar, and mask. xs is the input to whats being modeled, this should be a list of 1 dimensional jax.numpy arrays, which may have different shapes (lengths). These correspond to the log wavelength values being evaluated. The logarithm of wavelength means that the redshift from relative radial velocity change becomes an additive property.\n",
    "\n",
    "$$x_{obs} = \\log(\\lambda_{obs})$$\n",
    "$$\\lambda_{obs} = \\sqrt{\\frac{1 + RV/c}{1- RV/c}}\\lambda_{emit}$$\n",
    "$$x_{obs} = \\log(\\sqrt{\\frac{1 + RV/c}{1- RV/c}}\\lambda_{emit})$$\n",
    "$$x_{obs} = \\log(\\sqrt{\\frac{1 + RV/c}{1- RV/c}}) + \\log(\\lambda_{emit})$$\n",
    "$$x_{obs} = \\delta x + x_{emit}$$\n",
    "\n",
    "where $x_{obs}$ is the observed log wavelength, $RV$ is the radial velocity, $c$ is the speed of light, and $x_{emit}$ is the emitted log wavelength. ys is the measured log flux at that log wavelength, xs. This value is assumed to be in photon per wavelength bin. Thus, multiplicative properties like absorption from tellurics become additive in the logarithm. \n",
    "\n",
    "$$y = \\log(f)$$\n",
    "$$y = \\log(f_s f_t) $$\n",
    "$$y = \\log(f_s) + \\log(f_t) $$\n",
    "$$y = y_s + y_t$$\n",
    "\n",
    "yivar stands for y inverse variance, or y information. This value is produced by the instrumentation team at these detectors using some assumption about instrument readout and dark current properties along with poisson photon noise. Thus the error on the flux is not being modeled here and is assumed as correct from the pipeline. These errors are given as the standard deviation of the flux, so we need to transform error of f into the error in logarithm of flux.\n",
    "\n",
    "$$\\sigma_y = \\sigma_f \\frac{dy}{df}$$\n",
    "$$\\sigma_y = \\frac{\\sigma_f}{f}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b00a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(file,orders,device):\n",
    "    ys = []\n",
    "    xs = []\n",
    "    yivar = []\n",
    "    mask = []\n",
    "\n",
    "    init_shifts = np.array([jabble.physics.shifts(x) for x in file[\"bervs\"]])\n",
    "    times_t = np.array([x for x in file[\"dates\"]])\n",
    "    full_init_shifts = []\n",
    "    print(file[\"bervs\"].shape)\n",
    "    airmass = []\n",
    "    order_out = []\n",
    "    times = []\n",
    "    star_ids = []\n",
    "\n",
    "    for iii in orders:\n",
    "        for jjj in range(file[\"data\"].shape[1]):\n",
    "            ys.append(jnp.array(file[\"data\"][iii,jjj,:]))\n",
    "            xs.append(jnp.array(file[\"xs\"][iii,jjj,:]))\n",
    "            yivar.append(jnp.array(file[\"ivars\"][iii,jjj,:]))\n",
    "            mask.append(jnp.zeros(file[\"data\"][iii,jjj,:].shape,dtype=bool))\n",
    "\n",
    "            # init_shifts.append(\n",
    "            airmass.append(file[\"airms\"][jjj])\n",
    "            order_out.append(iii)\n",
    "            times.append(file[\"dates\"][jjj])\n",
    "            star_ids.append(str(file))\n",
    "\n",
    "            full_init_shifts.append(jabble.physics.shifts(file[\"bervs\"][jjj]))\n",
    "    \n",
    "    full_init_shifts = jnp.array(full_init_shifts)\n",
    "    airmass = jnp.array(airmass)\n",
    "\n",
    "    dataset = jabble.dataset.Data.from_lists(xs,ys,yivar,mask)\n",
    "\n",
    "    dataset.metakeys['times'] = times_t\n",
    "    dataset.metadata['times'] = np.array(times)\n",
    "    dataset.metadata['orders'] = np.array(order_out)\n",
    "    dataset.metadata['star_ids'] = np.array(star_ids)\n",
    "    dataset.metadata['init_shifts'] = np.array(full_init_shifts)\n",
    "    dataset.to_device(device)\n",
    "    \n",
    "    init_shifts = jax.device_put(init_shifts,device)\n",
    "    airmass = jax.device_put(airmass,device)\n",
    "    \n",
    "    return dataset, init_shifts, airmass, full_init_shifts, times_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c749e55-b98c-4948-9a15-bb9faf2a03c6",
   "metadata": {},
   "source": [
    "<h2>Jabble.Model</h2>\n",
    "Now, we will define the components of the model. These stars will have 3 components: stellar, telluric, pseudo-normalization. \n",
    "\n",
    "$$y = y_\\mathrm{s} + y_\\mathrm{t} + y_\\mathrm{N}$$\n",
    "\n",
    "The stellar component will be be high resolution (twice R of the detector) in wavelength and static in flux across all times or epochs of data. However, a nonlinear comes from the redshift radial velocity being fit inside the linear model log flux model. Because our dataset will consist of different orders at the same time, and the same order at different time, and we are assuming the wavelenght solution to be true. Thus the redshift should be the same at different orders at the same time, so $i_*$ is the index of the epoch while $i$ is the data index, which is different for all orders and times or can be thought of simply as the data index.\n",
    "\n",
    "$$y_s = f(x + \\delta x_{i_*}|\\theta_\\mathrm{s})$$\n",
    "\n",
    "The tellurics model is also high resolution in wavelength and static across all times, but the airmass is used to stretch the model at each model at each time. This parameter is not fit, however is can be if the user chooses to do so!\n",
    "\n",
    "$$y_t = a_i f(x|\\theta_\\mathrm{t})$$\n",
    "\n",
    "And finally the pseudo-normalization model is low resolution in wavelength about one parameter per 50 Angstroms and is different across all times because we expect the detector state to vary greatly with time.\n",
    "\n",
    "$$y_\\mathrm{N} = f(x|\\theta_{\\mathrm{N},i})$$\n",
    "\n",
    "The function we choose to represent the flux of these models is the cardinal spline mixture, who are evenly spaced in wavelength. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19ccb439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(dataset,resolution,p_val,vel_padding,init_shifts,rest_shifts,airmass,pts_per_wavelength,norm_p_val):\n",
    "         \n",
    "    dx = jabble.physics.delta_x(2 * resolution)\n",
    "    x_grid = jnp.arange(np.min(np.concatenate(dataset.xs)), np.max(np.concatenate(dataset.xs)), step=dx, dtype=\"float64\")\n",
    "    \n",
    "    model_grid = jabble.model.create_x_grid(\n",
    "        x_grid, vel_padding.to(u.m/u.s).value, 2 * resolution\n",
    "    )  \n",
    "\n",
    "    model = jabble.model.CompositeModel(\n",
    "        [\n",
    "            jabble.model.EpochShiftingModel(init_shifts),#ShiftingModel(init_shifts),#\n",
    "            jabble.model.IrwinHallModel_vmap(model_grid, p_val),\n",
    "        ]\n",
    "    ) + jabble.model.CompositeModel(\n",
    "        [\n",
    "            jabble.model.ShiftingModel(rest_shifts),\n",
    "            jabble.model.IrwinHallModel_vmap(model_grid, p_val),\n",
    "            jabble.model.StretchingModel(airmass),\n",
    "        ]\n",
    "    ) + jabble.model.get_normalization_model(dataset,norm_p_val,pts_per_wavelength)\n",
    "\n",
    "    # model.to_device(device)\n",
    "\n",
    "    # model.fit(2)\n",
    "    # print(type(model.get_parameters()),model.get_parameters().devices())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061a342-9e58-4016-9d20-d7fdc1970ea9",
   "metadata": {},
   "source": [
    "<h2>Training Cycle</h2>\n",
    "Now we define the fitting process of the parameters of all of these models with respect to the total set of data. This is require some clever initialization, so that we don't end up too far from the solution. First, we fit the parameters of the tellurics, stellar, normalization model. Regularizing the stellar and tellurics components, so that they are close 0. Then we fit the RV alone. Then we optimize everything together with regularization on the stellar and tellurics component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6704c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cycle(model, dataset, loss, lmb, device_store, device_op, batch_size):\n",
    "    # Fit Normalization\n",
    "    # model.fix()\n",
    "    # # model.fit(0,1)\n",
    "    # model.fit(2,1)\n",
    "    # model.display()\n",
    "    # res1 = gpu_optimize(model,loss,dataset, device_store, device_op,batch_size)\n",
    "    # print(res1)\n",
    "\n",
    "    \n",
    "    # Fit Stellar & Telluric Template\n",
    "    model.fix()\n",
    "    model.fit(2, 1)\n",
    "    model.fit(0, 1)\n",
    "    model.fit(1, 1)\n",
    "    model.display()\n",
    "    reg_s = lmb*jabble.loss.L2Reg([0,1])\n",
    "    reg_t = lmb*jabble.loss.L2Reg([1,1])\n",
    "    \n",
    "    res1 = model.optimize(loss + reg_s + reg_t, dataset, device_store, device_op, batch_size)#model.optimize(loss, dataset)\n",
    "    print(res1)\n",
    "    \n",
    "    # Fit RV\n",
    "    model.fix()\n",
    "    model.fit(0, 0)\n",
    "    res1 = model.optimize(loss, dataset, device_store, device_op, batch_size)\n",
    "    print(res1)\n",
    "\n",
    "    # RV Parabola Fit\n",
    "    # model.fix()\n",
    "    # shift_search = jabble.physics.shifts(np.linspace(-10, 10, 100))\n",
    "    # model[0][0].parabola_fit(shift_search, loss, model, dataset)\n",
    "    # print(type(model_p[0][0].p))\n",
    "\n",
    "    # Fit Everything\n",
    "    model.fix()\n",
    "    model.fit(0, 0)\n",
    "    model.fit(0, 1)\n",
    "    model.fit(1, 1)\n",
    "    model.fit(2, 1)\n",
    "\n",
    "    res1 = model.optimize(loss + reg_s + reg_t, dataset, device_store, device_op, batch_size)#model.optimize(loss, dataset)\n",
    "    print(res1)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64d128-5f26-4373-a60c-6a6572ece1ae",
   "metadata": {},
   "source": [
    "Here we define the 'loss' to be the sum of the chi squared of each flux. This is typical choice when using jabble, and why we must define the inverse variance of y, $I_{y}$ in the data object.\n",
    "\n",
    "$$\\mathcal{L}_{\\chi^2} = \\sum I_y (y(xs|\\delta x_{i_*},\\theta_\\mathrm{s},\\theta_\\mathrm{t},\\theta_{\\mathrm{N},i}) - ys)^2$$\n",
    "\n",
    "$xs, ys, I_{y}$ are from the data and are then summed over all unmasked elements.\n",
    "\n",
    "$$\\mathcal{L}_{\\mathrm{reg}} = \\sum \\theta_\\mathrm{s}^2 + \\theta_\\mathrm{t}^2$$\n",
    "\n",
    "$$\\mathcal{L}_{total} = \\mathcal{L}_{\\chi^2} + \\lambda \\mathcal{L}_{\\mathrm{reg}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d730833b-c2f3-40e0-821e-d179c37781b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = jax.devices(\"cpu\")\n",
    "gpus = jax.devices(\"gpu\")\n",
    "loss = jabble.loss.ChiSquare()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383a8f0-c1b4-458d-b6bc-d8d96d9832b3",
   "metadata": {},
   "source": [
    "Here we define the parameters of the instrument that are needed to build our model. The instrument is HARPS, so the resolution is 115,000. The $p$ value for the CSM model on the stellar and telluric components is set to 2. This means just the first derivative of the model will be continuous. And we will add a wavelength padding to either side of the log wavelength grid out to $100$ km/s in log wavelength. And the lambda for the regularization term is defined here as $\\lambda = 100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "437c441f-ed1a-41c0-9d65-5f7f0f6a6c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 115_000\n",
    "p_val = 2\n",
    "vel_padding = 100 * u.km / u.s\n",
    "\n",
    "\n",
    "pts_per_wavelengths = [1/50.0]\n",
    "norm_p_vals = [3]\n",
    "lmbs = [100.0]\n",
    "\n",
    "star_name_b, star_name_p = 'barnards','peg51'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011416e7-f905-41cb-89c1-3f2f38a68bd6",
   "metadata": {},
   "source": [
    "We will not fit the entire spectrum in one pass but we will fit chunks of multiple orders with one RV per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e0513c-a875-4d66-a9fe-9b4745923690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2]\n",
      " [ 3  4  5]\n",
      " [ 6  7  8]\n",
      " [ 9 10 11]\n",
      " [12 13 14]\n",
      " [15 16 17]\n",
      " [18 19 20]\n",
      " [21 22 23]\n",
      " [24 25 26]\n",
      " [27 28 29]\n",
      " [30 31 32]\n",
      " [33 34 35]\n",
      " [36 37 38]\n",
      " [39 40 41]\n",
      " [42 43 44]\n",
      " [45 46 47]\n",
      " [48 49 50]\n",
      " [51 52 53]\n",
      " [54 55 56]\n",
      " [57 58 59]\n",
      " [60 61 62]\n",
      " [63 64 65]\n",
      " [66 67 68]\n",
      " [69 70 71]]\n"
     ]
    }
   ],
   "source": [
    "orders_s = np.arange(0,72,dtype=int).reshape(-1,3)\n",
    "print(orders_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d401a1c-a929-4772-822d-7c18c0252bd5",
   "metadata": {},
   "source": [
    "Also decide what device you want to store extra data on, and which device to put the model and do the fitting on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa80aa7-1f09-4294-8eeb-d19bb81f6e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72e0e9b-dd67-486d-aed2-3d1bc20ab00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306,)\n",
      "918 3\n",
      "-AdditiveModel---------------------------------------------------306\n",
      "  0-CompositeModel-----------------------------------------------306\n",
      "  0  0-EpochShiftingModel----------------------------------------306\n",
      "  0  1-IrwinHallModel_vmap-----------------------------------------0\n",
      "  1-CompositeModel-------------------------------------------------0\n",
      "  1  0-ShiftingModel-----------------------------------------------0\n",
      "  1  1-IrwinHallModel_vmap-----------------------------------------0\n",
      "  1  2-StretchingModel---------------------------------------------0\n",
      "  2-CompositeModel-------------------------------------------------0\n",
      "  2  0-ShiftingModel-----------------------------------------------0\n",
      "  2  1-NormalizationModel------------------------------------------0\n",
      "-AdditiveModel-------------------------------------------------13946\n",
      "  0-CompositeModel----------------------------------------------5596\n",
      "  0  0-EpochShiftingModel------------------------------------------0\n",
      "  0  1-IrwinHallModel_vmap--------------------------------------5596\n",
      "  1-CompositeModel----------------------------------------------5596\n",
      "  1  0-ShiftingModel-----------------------------------------------0\n",
      "  1  1-IrwinHallModel_vmap--------------------------------------5596\n",
      "  1  2-StretchingModel---------------------------------------------0\n",
      "  2-CompositeModel----------------------------------------------2754\n",
      "  2  0-ShiftingModel-----------------------------------------------0\n",
      "  2  1-NormalizationModel---------------------------------------2754\n",
      "-AdditiveModel-------------------------------------------------13946\n",
      "  0-CompositeModel----------------------------------------------5596\n",
      "  0  0-EpochShiftingModel------------------------------------------0\n",
      "  0  1-IrwinHallModel_vmap--------------------------------------5596\n",
      "  1-CompositeModel----------------------------------------------5596\n",
      "  1  0-ShiftingModel-----------------------------------------------0\n",
      "  1  1-IrwinHallModel_vmap--------------------------------------5596\n",
      "  1  2-StretchingModel---------------------------------------------0\n",
      "  2-CompositeModel----------------------------------------------2754\n",
      "  2  0-ShiftingModel-----------------------------------------------0\n",
      "  2  1-NormalizationModel---------------------------------------2754\n",
      "-CompositeModel-------------------------------------------------5596\n",
      "  0-EpochShiftingModel---------------------------------------------0\n",
      "  1-IrwinHallModel_vmap-----------------------------------------5596\n",
      "13946 5596\n",
      "-AdditiveModel-------------------------------------------------13946\n",
      "  0-CompositeModel----------------------------------------------5596\n",
      "  0  0-EpochShiftingModel------------------------------------------0\n",
      "  0  1-IrwinHallModel_vmap--------------------------------------5596\n",
      "  1-CompositeModel----------------------------------------------5596\n",
      "  1  0-ShiftingModel-----------------------------------------------0\n",
      "  1  1-IrwinHallModel_vmap--------------------------------------5596\n",
      "  1  2-StretchingModel---------------------------------------------0\n",
      "  2-CompositeModel----------------------------------------------2754\n",
      "  2  0-ShiftingModel-----------------------------------------------0\n",
      "  2  1-NormalizationModel---------------------------------------2754\n",
      "-CompositeModel-------------------------------------------------5596\n",
      "  0-ShiftingModel--------------------------------------------------0\n",
      "  1-IrwinHallModel_vmap-----------------------------------------5596\n",
      "  2-StretchingModel------------------------------------------------0\n",
      "13946 5596\n"
     ]
    }
   ],
   "source": [
    "device_store = cpus[0]\n",
    "device_op = gpus[0]\n",
    "batch_size = 5000\n",
    "    \n",
    "def function_function(orders):\n",
    "    for star_name,file in zip([star_name_b],[file_b]):\n",
    "        models = []\n",
    "        for lmb in lmbs:\n",
    "            for norm_p_val in norm_p_vals:\n",
    "                for pts_per_wavelength in pts_per_wavelengths:\n",
    "        \n",
    "                    model_name = os.path.join(out_dir,star_name + '_epoch_inds_o{}_p{}_w{}_l{}.mdl'.format(orders,norm_p_val,1/pts_per_wavelength,lmb))\n",
    "                    dataset, init_shifts, airmass, full_init_shifts, times_t = get_dataset(file,orders,device_op)\n",
    "\n",
    "                    datablock, metablock, meta_keys = dataset.blockify(device_store,return_keys=True)\n",
    "                    \n",
    "                    shifts = jnp.zeros(full_init_shifts.shape)\n",
    "               \n",
    "                    model = get_model(dataset,resolution,p_val,vel_padding,-init_shifts,shifts,airmass,pts_per_wavelength,norm_p_val)\n",
    "                    for key in dataset.metadata.keys():\n",
    "                        model.metadata[key] = dataset.metadata[key]\n",
    "                    model.fix()\n",
    "                    model[0][0].fit()\n",
    "                    model.display()\n",
    "                    # test_inds = model[2][1].get_indices(2)\n",
    "                    # print(len(test_inds), np.sum(test_inds))\n",
    "                    model.to_device(device_op)\n",
    "                    model = train_cycle(model, dataset, loss, lmb, device_store, device_op, batch_size)\n",
    "                    jabble.model.save(model_name,model)\n",
    "                    models.append(model)\n",
    "    return models\n",
    "\n",
    "for orders in orders_s:\n",
    "    models = function_function(orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ef33e-f1f4-4450-aff8-ce839bff1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31963717-fa2c-4afc-af96-9f65403214f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = glob.glob('../out/24-10-18/*.mdl')\n",
    "print(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b1cb86-21b2-46a2-b83e-cc59eed73b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [jabble.model.load(file) for file in dir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fca1c9-c6c8-4bf4-bbfa-84fb86bd3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in models:\n",
    "#     model.fix()\n",
    "#     model[0][0].fit()\n",
    "#     model.display()\n",
    "# models = {\"51peg epoch\": models[0],\\\n",
    "#           \"barns order\": models[1],\\\n",
    "#           \"barns epoch\": models[2],\\\n",
    "#           \"51peg order\": models[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29334ead-695c-417b-bfad-4b0edb53e854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_order_plot(file,model,lmin,lmax,lrange,orders,rv_inds,device):\n",
    "    # model = jabble.model.load(model_name)\n",
    "    data_orders = np.unique(model.metadata[\"orders\"])\n",
    "    dataset, _, _, _, _ = get_dataset(file,data_orders,device)\n",
    "\n",
    "    model.fit()\n",
    "    model.display()\n",
    "    model.fix()\n",
    "    \n",
    "    # for dataframe in dataset:\n",
    "    #     print(np.exp(np.min(dataset.xs)))\n",
    "    # model_name_b = os.path.join('..','out','24-06-11','peg51_o{}_no_norm.mdl'.format(order))\n",
    "    plt_epoches = []\n",
    "    data, meta, keys = dataset.blockify(device,return_keys=True)\n",
    "    # print(meta[\"times\"].shape,model[0][0].p.shape)\n",
    "    temp = np.argsort(model[0][0].p[meta[\"times\"]])\n",
    "    # which_ones = [0,-1,-2,1]\n",
    "    # which_ones = jax.random.randint(jax.random.PRNGKey(0), shape=(3,), minval=0, maxval=len(temp), dtype=jnp.uint8)\n",
    "    # print( np.unique(model.metadata[\"orders\"]) == orders[0])\n",
    "    for uni_order in orders:\n",
    "        # print(uni_order)\n",
    "        for iii in rv_inds:\n",
    "            # print(temp,temp.dtype,model.metadata[\"orders\"],type(model.metadata[\"orders\"][0]))\n",
    "            # print(dataset.metadata[\"orders\"],np.sum(dataset.metadata[\"orders\"] == uni_order))\n",
    "            args_order = temp[np.array(dataset.metadata[\"orders\"])[temp] == uni_order]\n",
    "            # print(args_order)\n",
    "            # print(len(temp),len(args_order),uni_order,args_order[iii])\n",
    "            # plt_epoches.append(temp[uni_order == model.metadata[\"orders\"]][-1])\n",
    "            plt_epoches.append(int(args_order[iii]))\n",
    "    # plt_epoches = [0,1,2,3]#[*sorted_epoches[-1:],*sorted_epoches[:1]]\n",
    "    \n",
    "    fig, axes = plt.subplots(2,len(plt_epoches),figsize=(4*len(plt_epoches),4),sharex=True,sharey='row',\\\n",
    "                             facecolor=(1, 1, 1),height_ratios=[4,1],dpi=200)\n",
    "\n",
    "    # print(plt_epoches,len(temp))\n",
    "    model.fix()\n",
    "    # model.display()\n",
    "    nothing= model.get_parameters()\n",
    "    for ii, plt_epoch in enumerate(plt_epoches):\n",
    "        # indices = model.metadata['times'] == model.metadata['times'][plt_epoch]\n",
    "        datarow = dict_slice(data,plt_epoch,plt_epoch+1,device)\n",
    "        metarow = dict_slice(meta,plt_epoch,plt_epoch+1,device)\n",
    "\n",
    "        # print(metarow['index'],plt_epoch)\n",
    "        \n",
    "        xplot = np.linspace(np.log(lmin),np.log(lmax),\\\n",
    "                            dataset.xs[plt_epoch].shape[0]*10)\n",
    "        # yplot = model[2]([],xplot,plt_epoch)\n",
    "        yplot_norm_tot = model([],xplot,metarow)\n",
    "        yplot_norm_stel = model[0]([],xplot,metarow)\n",
    "        yplot_norm_tell = model[1]([],xplot,metarow)\n",
    "        yplot_norm = model[2]([],xplot,metarow)\n",
    "        # for epoch in np.where(indices):\n",
    "        yhat = model([],dataset.xs[plt_epoch][:],metarow)\n",
    "        axes[0,ii].set_xlim(xplot.min(),xplot.max())\n",
    "\n",
    "        velocity = jabble.physics.velocities(model[0][0].p[plt_epoch])\n",
    "        \n",
    "        # Data\n",
    "        axes[0,ii].plot(datarow[\"xs\"][:],datarow[\"ys\"][:],\\\n",
    "                                 '.k',zorder=100,alpha=0.1,ms=7)\n",
    "\n",
    "        # Stellar Model        \n",
    "        axes[0,ii].plot(xplot,yplot_norm_stel,'-r',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # Telluric Model\n",
    "        axes[0,ii].plot(xplot,yplot_norm_tell,'-b',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # Total\n",
    "        axes[0,ii].plot(xplot,yplot_norm_tot,'-m',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # Norm\n",
    "        axes[0,ii].plot(xplot,yplot_norm,'-g',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # Theory Model\n",
    "        # theory_ax = axes[0,ii].twinx()\n",
    "        # theory_ax.plot(dataset_theory.xs[0][:],dataset_theory.ys[0][:],'-y',linewidth=1.2,zorder=10,alpha=0.7,ms=6)\n",
    "        # theory_ax.set_ylim(-5,5)\n",
    "        # Line List\n",
    "\n",
    "        # shift_by_eye = (np.log(5002) - np.log(5000))\n",
    "        # for line in line_list[1].data[(line_list[1].data[\"Wave\"] > lmin) * (line_list[1].data[\"Wave\"] < lmax)]:\n",
    "        #     print(line[\"Species\"])\n",
    "            # axes[0,ii].vlines(np.log(line[\"Wave\"]) + model[0][0].p[plt_epoch] + shift_by_eye,-100,len(dataset),'k','dashed',alpha=0.4)\n",
    "        \n",
    "        # Residuals\n",
    "        axes[1,ii].step(dataset.xs[plt_epoch],dataset.ys[plt_epoch] - yhat,\\\n",
    "                                 'k',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "\n",
    "        # axes[0,ii].set_title(\"order $ = {}$\".format(model.metadata[\"orders\"][plt_epoch]))\n",
    "        axes[0,ii].set_ylim(-2.5,0.5)\n",
    "        # axes[1,ii].set_ylim(-0.08,0.08)\n",
    "        axes[0,ii].set_xticks([])\n",
    "        axes[0,ii].set_xticks(np.log(lrange))\n",
    "        axes[0,ii].set_xticklabels(['{:0.1f}'.format(x) for x in lrange])\n",
    "        # axes[0,ii].set_xlim([np.log(lmin), np.max(lmax)])\n",
    "\n",
    "    # plt.x\n",
    "    # plt.text(1, 1, 'Wavelength ($\\AA$)', ha='center')\n",
    "    # plt_name = os.path.join(out_dir, \"02-spectra_{}_{}-{}.png\".format(os.path.split(model_name)[-1],lmin,lmax))\n",
    "    # plt.savefig(plt_name,dpi=200,bbox_inches='tight')\n",
    "    # fig.suptitle('Barnards Star')\n",
    "    fig.text(0.5, 0.00, 'Wavelength $[\\AA]$', ha='center')\n",
    "    fig.text(0.08, 0.5, 'Normalized Flux', va='center', rotation='vertical')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22213a1e-f1ec-49a6-a31b-cf37e7b27143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmin = 5680\n",
    "# lmax = 5690\n",
    "# lspace = 2\n",
    "\n",
    "# lrange = np.arange(lmin,lmax+lspace,lspace)\n",
    "\n",
    "# plt_name= \"\"\n",
    "# #'barnards_norm_reg_init_o[5, 6, 7, 8]_p2_w30.000_l100.0.mdl'\n",
    "# #\n",
    "# #'barnards_norm_reg_init_o[66, 67, 68, 69]_p4_w100.000_l100.mdl'\n",
    "# orders = [52]\n",
    "# rv_inds = [-2,-1,0,2]\n",
    "# # model_name_b = os.path.join('..','out','24-10-04','barnards_data_inds_o[50, 51, 52]_p3_w50.0_l10.0.mdl')\n",
    "# # model_name_b = os.path.join('..','out','24-07-02','barnards_all_no_norm.mdl',)\n",
    "# make_order_plot(file_b,models[\"barns epoch\"],lmin,lmax,lrange,orders,rv_inds,cpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0269437c-a989-4ab8-bb38-3f9029214644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rv_data = jabble.physics.velocities(models[\"barns order\"][0][0].p)\n",
    "# rv_epoch = jabble.physics.velocities(models[\"barns epoch\"][0][0].p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0ce4b-a67c-41b6-95b3-80e84edab679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders = np.unique(models[\"barns order\"].metadata['orders'])\n",
    "# data, _, _, full_init_shifts, _ = get_dataset(file_b,orders,cpus[0])\n",
    "# datablock, metablock, keys = data.blockify(cpus[0],return_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e221d98-2f2d-41ce-aa80-cd45d1c1d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8209cbf4-0084-41c7-97da-1e86590610e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_comparison_plot(times_d,rv_d,err_d,orders_d,times_e,rv_e,err_e,time_t,targ_vel,targ_err,bervs,bervs_d):\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    # data_indices, epoch_indices, order_indices, star_indices = data.blockify_indices(cpus[0])\n",
    "    # estimate_vel_2\n",
    "    \n",
    "    # print(bervs.shape,np.unique(bervs).shape,estimate_vel.shape)\n",
    "    # ov = (estimate_vel_d+bervs)\n",
    "    # ov -= ov.mean()\n",
    "    cmap = matplotlib.colormaps[\"viridis\"]\n",
    "    orders_unq = np.unique(orders_d)\n",
    "    \n",
    "    plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\"color\", plt.cm.viridis(np.linspace(0,1,len(orders))))\n",
    "    for order in orders_unq:\n",
    "        indices = np.array([orders_d == order]).squeeze()\n",
    "        # print(np.sum(indices),indices.shape)\n",
    "        # ax.errorbar(times_d[indices],rv_d[indices] + bervs,err_d[indices],fmt='.',zorder=1,alpha=0.4,ms=2,label='Order Jabble')\n",
    "\n",
    "    uni_times = np.unique(times_d)\n",
    "    ev     = jnp.zeros(len(uni_times))\n",
    "    evvar = jnp.zeros(len(uni_times))\n",
    "    info_d = 1/err_d**2\n",
    "    for iii,time in enumerate(times_e):\n",
    "        indices = times_d == time\n",
    "        # print(np.sum(indices))\n",
    "        # print(np.sum(info_d[indices] < 0.0))\n",
    "        ev = ev.at[iii].set(jnp.dot(info_d[indices],rv_d[indices])/jnp.sum(info_d[indices]))\n",
    "        evvar = evvar.at[iii].set((jnp.dot(info_d[indices],rv_d[indices]**2)/jnp.sum(info_d[indices])) - ev[iii]**2)\n",
    "        # print(evvar[iii],np.mean(1/info_d[indices]))\n",
    "\n",
    "    ev += bervs\n",
    "    ev -= ev.mean()\n",
    "    ax.errorbar(times_e,ev,jnp.sqrt(evvar),fmt='.r',zorder=1,alpha=0.30,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    rv_order = rv_d + bervs_d\n",
    "    rv_order -= rv_order.mean()\n",
    "    ax.errorbar(times_d,rv_order,err_d,fmt='.r',zorder=1,alpha=0.10,ms=2,label='Order Jabble RV')\n",
    "\n",
    "    correct_vel = targ_vel + bervs\n",
    "    correct_vel -= correct_vel.mean()\n",
    "    ax.errorbar(time_t,correct_vel,targ_err,fmt='.k',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "    \n",
    "    temp_vel = rv_e + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(time_t,temp_vel,err_e,fmt='.b',zorder=2,alpha=0.30,ms=2,label='Epoch Jabble RV')\n",
    "\n",
    "    \n",
    "    # fig.legend()\n",
    "    # ax.set_ylim(-40, 40)\n",
    "    fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star Relative Radial Velocities')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"MJD\")\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    plt.savefig(os.path.join(out_dir, \"02-barns_vel_o{}-{}_nobervs_epoch.png\".format(np.min(orders),np.max(orders))))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67628b4d-8b4d-4cef-9cf2-2c607dd0a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_targ_zero_plot(times_d,rv_d,err_d,orders_d,times_e,rv_e,err_e,time_t,targ_vel,targ_err,bervs,bervs_d,metadata):\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    # data_indices, epoch_indices, order_indices, star_indices = data.blockify_indices(cpus[0])\n",
    "    # estimate_vel_2\n",
    "    \n",
    "    # print(bervs.shape,np.unique(bervs).shape,estimate_vel.shape)\n",
    "    # ov = (estimate_vel_d+bervs)\n",
    "    # ov -= ov.mean()\n",
    "    cmap = matplotlib.colormaps[\"viridis\"]\n",
    "    orders_unq = np.unique(orders_d)\n",
    "    \n",
    "    plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(\"color\", plt.cm.viridis(np.linspace(0,1,len(orders))))\n",
    "    for order in orders_unq:\n",
    "        indices = np.array([orders_d == order]).squeeze()\n",
    "        # print(np.sum(indices),indices.shape)\n",
    "        # ax.errorbar(times_d[indices],rv_d[indices] + bervs,err_d[indices],fmt='.',zorder=1,alpha=0.4,ms=2,label='Order Jabble')\n",
    "\n",
    "    epoches_span = np.arange(0,len(time_t),dtype=int)\n",
    "    \n",
    "    correct_vel = targ_vel + bervs\n",
    "    correct_vel -= correct_vel.mean()\n",
    "    ax.errorbar(epoches_span,correct_vel-correct_vel,targ_err,fmt='.-k',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "    \n",
    "    uni_times = np.unique(times_d)\n",
    "    ev     = jnp.zeros(len(uni_times))\n",
    "    evvar = jnp.zeros(len(uni_times))\n",
    "    info_d = 1/err_d**2\n",
    "    for iii,time in enumerate(times_e):\n",
    "        indices = times_d == time\n",
    "        # print(np.sum(indices))\n",
    "        # print(np.sum(info_d[indices] < 0.0))\n",
    "        ev = ev.at[iii].set(jnp.dot(info_d[indices],rv_d[indices])/jnp.sum(info_d[indices]))\n",
    "        evvar = evvar.at[iii].set((jnp.dot(info_d[indices],rv_d[indices]**2)/jnp.sum(info_d[indices])) - ev[iii]**2)\n",
    "        # print(evvar[iii],np.mean(1/info_d[indices]))\n",
    "\n",
    "    ev += bervs\n",
    "    ev -= ev.mean()\n",
    "    ax.errorbar(epoches_span,ev - correct_vel,jnp.sqrt(evvar),fmt='.r',zorder=1,alpha=0.30,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    rv_order = rv_d + bervs_d\n",
    "    rv_order -= rv_order.mean()\n",
    "    ax.errorbar(epoches_span[metadata['times']],rv_order - correct_vel[metadata['times']],err_d,fmt='.r',zorder=1,alpha=0.10,ms=2,label='Order Jabble RV')\n",
    "    \n",
    "    temp_vel = rv_e + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(epoches_span,temp_vel - correct_vel,err_e,fmt='.b',zorder=2,alpha=0.30,ms=2,label='Epoch Jabble RV')\n",
    "\n",
    "    \n",
    "    # fig.legend()\n",
    "    # ax.set_ylim(-40, 40)\n",
    "    fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star HARPS Comparison')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"epoch index\")\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    plt.savefig(os.path.join(out_dir, \"02-barns_targ_o{}-{}_nobervs_epoch.png\".format(np.min(orders),np.max(orders))))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1a538c-e98b-4d39-91aa-9b34bdf7d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datablock, metablock, keys = dataset.blockify(cpus[0],return_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0159401-3297-435d-a501-437ca57ecc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rv_data = jabble.physics.velocities(models[\"barns order\"][0][0].p)\n",
    "# err_data = get_verr(models[\"barns order\"][0][0].p,f_info_arr_d)\n",
    "# rv_epoch = jabble.physics.velocities(models[\"barns epoch\"][0][0].p)\n",
    "# err_epoch = get_verr(models[\"barns epoch\"][0][0].p,f_info_arr_e)\n",
    "\n",
    "# rv_comparison_plot(models[\"barns order\"].metadata['times'],rv_data,err_data,models[\"barns order\"].metadata['orders'],\\\n",
    "#                    np.array(file_b['dates']),rv_epoch,err_epoch,\\\n",
    "#                    np.array(file_b['dates']),np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "#                    np.array(file_b['bervs']),np.array(file_b['bervs'])[metablock['times']])\n",
    "# rv_targ_zero_plot(models[\"barns order\"].metadata['times'],rv_data,err_data,models[\"barns order\"].metadata['orders'],\\\n",
    "#                    np.array(file_b['dates']),rv_epoch,err_epoch,\\\n",
    "#                    np.array(file_b['dates']),np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "#                    np.array(file_b['bervs']),np.array(file_b['bervs'])[metablock['times']],metablock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369042c-a526-40bb-82d4-975d33754ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = glob.glob('../out/24-10-23/b*.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fccd075-fa4c-4f69-a812-1da734ee9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2886fb7-9152-4266-a101-3d3fb87f86dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_ele(dictionary,ele_i,device):\n",
    "        out = {}\n",
    "        for key in dictionary:\n",
    "            out[key] = jax.device_put(dictionary[key][ele_i],device)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20af04e8-3968-4d43-9380-c8b81add2ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_b.keys(),file_b['bervs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878b1f1-c554-41a9-8eb3-888f6c37d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jabble_file = h5py.File('../out/24-10-23/rv_jabble.hdf5','w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13331494-fc48-4798-b5a1-d03b38675a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rv = np.zeros((len(dir),*file_b['bervs'].shape))\n",
    "all_order = np.zeros((len(dir),int(file_b['data'].shape[0]/len(dir))))\n",
    "\n",
    "all_err = np.zeros((len(dir),*file_b['bervs'].shape))\n",
    "all_times = np.array(file_b['dates'])\n",
    "all_models = []\n",
    "\n",
    "all_loss = np.zeros((len(dir),int(file_b['data'].shape[0]/len(dir)),*file_b['data'].shape[1:]))\n",
    "for iii,filename in enumerate(dir):\n",
    "\n",
    "    model = jabble.model.load(filename)\n",
    "    orders = np.unique(model.metadata['orders'])\n",
    "    dataset, init_shifts, airmass, full_init_shifts, times_t = get_dataset(file_b,orders,cpus[0])\n",
    "    datablock, metablock, meta_keys = dataset.blockify(cpus[0],return_keys=True)\n",
    "    all_order[iii,:] = meta_keys['orders']\n",
    "    #print(meta_keys)\n",
    "    for jjj in range(len(dataset)):\n",
    "        datarow = dict_ele(datablock,jjj,cpus[0])\n",
    "        metarow = dict_ele(metablock,jjj,cpus[0])\n",
    "        model.fix()\n",
    "        #model.display()\n",
    "        #print(datarow,metarow)\n",
    "        temp = loss(model.get_parameters(),datarow,metarow,model)\n",
    "        \n",
    "        # dataset.metadata['times'][iii]\n",
    "        all_loss[iii,*np.where(dataset.metadata['orders'][jjj] == meta_keys['orders']),metarow['times'],:] = temp\n",
    "        \n",
    "    \n",
    "    f_info_arr = f_info(model[0][0],model,dataset,loss,cpus[0])\n",
    "    rv_data = jabble.physics.velocities(model[0][0].p)\n",
    "    err_data = get_verr(model[0][0].p,f_info_arr)\n",
    "\n",
    "    all_rv[iii,:] = rv_data\n",
    "    all_err[iii,:] = err_data\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a08f3e-ab74-471e-a105-35a79e8fc21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('../out/24-10-23/barnards_jrv_3.hdf5','w') as jabble_file:\n",
    "    jabble_file.create_dataset('RV',data=all_rv)\n",
    "    jabble_file.create_dataset('RV_err',data=all_err)\n",
    "    jabble_file.create_dataset('loss',data=all_loss)\n",
    "    jabble_file.create_dataset('times',data=all_times)\n",
    "    jabble_file.create_dataset('order',data=all_order)\n",
    "\n",
    "    rv_comb = np.zeros(file_b['bervs'].shape)\n",
    "    var_comb = np.zeros(file_b['bervs'].shape)\n",
    "    info = 1/jabble_file['RV_err'][:,:]**2\n",
    "    for iii in range(jabble_file['RV'].shape[1]):\n",
    "        mask_1 = ~np.isnan(info[:,iii])\n",
    "        mask_2 = info[:,iii] < 1e10\n",
    "        mask = (mask_1 * mask_2).astype(bool)\n",
    "        mask = ~np.isnan(info[:,iii])\n",
    "        rv_comb[iii]  =   jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii] ) / jnp.sum(info[mask,iii])\n",
    "        var_comb[iii] = ( jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii]**2 ) / jnp.sum(info[mask,iii]) ) - rv_comb[iii]**2\n",
    "    \n",
    "    jabble_file.create_dataset('RV_comb',data=rv_comb)\n",
    "    jabble_file.create_dataset('RV_comb_err',data=np.sqrt(var_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c24d4-5294-4e37-8713-7c95f2f61872",
   "metadata": {},
   "outputs": [],
   "source": [
    "jabble_file =  h5py.File('../out/24-10-23/barnards_jrv_3.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec7860-0656-48cc-a7fa-f13cb704fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jabble_file =  h5py.File('../out/24-10-23/rv_jabble.hdf5','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bee885c-a33a-4919-a000-7ebcf8afc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_all_order_plot(times,rv_e,err_e,rv_comb,err_comb,targ_vel,targ_err,bervs,period=3.1533):\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "    temp_vel = targ_vel + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(times % period,temp_vel,targ_err,fmt='.g',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "    \n",
    "    temp_vel = rv_comb + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(times % period,temp_vel,err_comb,fmt='.r',zorder=2,alpha=0.60,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    temp_vel = rv_e.mean(axis=0) + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(times % period,temp_vel,0.0,fmt='.b',zorder=2,alpha=0.60,ms=2,label='Avg RV')\n",
    "\n",
    "    for i in range(rv_e.shape[0]):\n",
    "        temp_vel = rv_e[i,:] + bervs\n",
    "        temp_vel -= temp_vel.mean()\n",
    "        ax.errorbar(times % period,temp_vel,yerr=err_e[i,:],fmt='.k',zorder=1,alpha=0.05,ms=2,label='Order Jabble RV')\n",
    "\n",
    "    # fig.legend()\n",
    "    # ax.set_ylim(-4000, 4000)\n",
    "    # fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star Relative Radial Velocities')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"MJD\")\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    # plt.savefig(os.path.join(out_dir, \"02-barns_all_order_nobervs_epoch.png\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d13a8-e5da-423f-900b-5d15989bad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rv_all_order_harps_plot(times,rv_e,err_e,rv_comb,err_comb,targ_vel,targ_err,bervs,loss):\n",
    "    \n",
    "    epoches_span = np.arange(0,len(times),dtype=int)\n",
    "\n",
    "    fig, ax = plt.subplots(\n",
    "        1,\n",
    "        figsize=(10, 4),\n",
    "        facecolor=(1, 1, 1),\n",
    "        dpi=300,\n",
    "        sharey=True\n",
    "    )\n",
    "\n",
    "    temp_vel = targ_vel + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(epoches_span,temp_vel,targ_err,fmt='.g',zorder=3,alpha=0.30,ms=2,label='HARPS RV')\n",
    "\n",
    "    temp_vel = rv_comb + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(epoches_span,temp_vel,err_comb,fmt='.r',zorder=2,alpha=0.60,ms=2,label='Order Jabble Combined RV')\n",
    "\n",
    "    temp_vel = rv_e.mean(axis=0) + bervs\n",
    "    temp_vel -= temp_vel.mean()\n",
    "    ax.errorbar(epoches_span,temp_vel,0.0,fmt='.b',zorder=2,alpha=0.60,ms=2,label='Avg RV')\n",
    "\n",
    "    for i in range(rv_e.shape[0]):\n",
    "        temp_vel = rv_e[i,:] + bervs\n",
    "        temp_vel -= temp_vel.mean()\n",
    "        ax.errorbar(epoches_span,temp_vel,yerr=err_e[i,:],fmt='.k',zorder=1,alpha=0.05,ms=2,label='Order Jabble RV')\n",
    "\n",
    "    # ax.set_ylim(-4000, 4000)\n",
    "    # fig.legend()\n",
    "    ax.set_title('Barnard\\'s Star Relative Radial Velocities')\n",
    "    ax.set_ylabel(\"RV [$m/s$]\")\n",
    "    ax.set_xlabel( \"epoch index\")\n",
    "    # ax.set_xlim(2.4564e6,2.45644e6)\n",
    "    # ax.set_ylim(-20e3,-10e3)\n",
    "    # plt.savefig(os.path.join(out_dir, \"02-barns_all_order_nobervs_epoch.png\"))\n",
    "    plt.show()\n",
    "\n",
    "    rv_difference_array = np.zeros(rv_e.shape)\n",
    "    print(rv_comb.shape,rv_e.shape)\n",
    "    for i in range(rv_e.shape[1]):\n",
    "        rv_difference_array[:,i] = rv_comb[i] - rv_e[:,i]\n",
    "\n",
    "    plt.figure(figsize=(10,4),facecolor=(1, 1, 1),dpi=300)\n",
    "    im = plt.imshow(rv_difference_array,interpolation ='nearest')\n",
    "    plt.xlabel('epoches')\n",
    "    plt.ylabel('orders')\n",
    "    plt.title('$\\Delta$ RV')\n",
    "    plt.colorbar(im,shrink=0.3)  \n",
    "    plt.savefig(os.path.join(out_dir,'drv.png'))\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    loss_mean = np.mean(loss,axis=3).mean(axis=1)\n",
    "    print(loss_mean.shape)\n",
    "\n",
    "    plt.figure(figsize=(10,4),facecolor=(1, 1, 1),dpi=300)\n",
    "    im = plt.imshow(loss_mean,interpolation ='nearest',vmin=0,vmax=10)\n",
    "    plt.xlabel('epoches')\n",
    "    plt.ylabel('orders')\n",
    "    plt.title('Objective')\n",
    "    plt.colorbar(im,shrink=0.3)\n",
    "    plt.savefig(os.path.join(out_dir,'obj.png'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63e6bc7-4f2d-4e6d-a52f-843e62f1ee88",
   "metadata": {},
   "outputs": [],
   "source": [
    "jabble_file.keys()\n",
    "for key in jabble_file.keys():\n",
    "    print(key,jabble_file[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f18408-ca3b-4802-aa4e-66eb222fe3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac235039-5ec4-4b07-8467-a404e3fa6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmap = cm.get_cmap(\"Spectral\")\n",
    "cmap = matplotlib.colormaps[\"Spectral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333adb1-9ff9-41ea-95f5-7c3e6640c8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = 1/jabble_file['RV_err'][:,:]**2\n",
    "bervs = np.array(file_b['bervs'])\n",
    "temp = np.array(jabble_file[\"loss\"]) <= 0\n",
    "# print(np.sum(temp))\n",
    "# print(np.array(jabble_file[\"loss\"])[temp])\n",
    "# print(np.stack(np.where(temp)).T)\n",
    "loss_temp = np.array(jabble_file[\"loss\"])\n",
    "loss_temp[temp] = 0.0\n",
    "print(np.sum(loss_temp <= 0)/np.product(loss_temp.shape))\n",
    "loss_mean = np.mean(loss_temp,axis=3).mean(axis=1)\n",
    "\n",
    "looking_ind = np.where(loss_mean <= 0)\n",
    "print(np.stack(looking_ind).T)\n",
    "print(loss_mean[looking_ind])\n",
    "\n",
    "# norm = cmap.LogNorm(np.min(loss_sum),np.max(loss_sum))\n",
    "\n",
    "for iii in range(jabble_file[\"RV\"].shape[1]):\n",
    "    mask_1 = ~np.isnan(info[:,iii])\n",
    "    mask_2 = info[:,iii] < 1e10\n",
    "    mask = (mask_1 * mask_2).astype(bool)\n",
    "    rv_comb = jnp.average(jabble_file['RV'][mask,iii],weights=info[mask,iii])\n",
    "        # rv_comb  =   jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii] ) / jnp.sum(info[mask,iii])\n",
    "    var_comb = jnp.average((jabble_file['RV'][mask,iii])**2,weights=info[mask,iii]) - rv_comb**2\n",
    "    # if (np.all(jabble_file[\"RV\"][:,iii]>rv_comb)) or (np.all(jabble_file[\"RV\"][:,iii]<rv_comb)):\n",
    "    #     if (np.any(info[mask,iii] > 1e10)):\n",
    "    jjjs, = np.where(info[mask,iii] > 1e10)\n",
    "    # print(rv_comb,var_comb, np.min(info[:,iii]), np.max(info[:,iii]))\n",
    "    rv_difference = np.sqrt((rv_comb - jabble_file['RV'][mask,iii])**2)\n",
    "    print(np.max(rv_difference),np.where(rv_difference == rv_difference.max()))\n",
    "    jjjs, = np.where(rv_difference == rv_difference.max())\n",
    "    \n",
    "    # print(loss_sum[:,iii])\n",
    "#     # print(jabble_file[\"RV\"][:,iii],\"\\n\", jabble_file[\"RV_comb\"][iii],jabble_file[\"RV_err\"][:,iii])\n",
    "    \n",
    "#     mask = ~np.isnan(info[:,iii])\n",
    "#     rv_comb = jnp.average(jabble_file['RV'][mask,iii] + bervs[iii],weights=info[mask,iii])\n",
    "#     # rv_comb  =   jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii] ) / jnp.sum(info[mask,iii])\n",
    "#     var_comb = jnp.average((jabble_file['RV'][mask,iii] + bervs[iii])**2,weights=info[mask,iii]) - rv_comb**2\n",
    "#     print((np.all(jabble_file[\"RV\"][:,iii]>rv_comb)) or (np.all(jabble_file[\"RV\"][:,iii]<rv_comb)))\n",
    "#     # print(( jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii]**2 ) / jnp.sum(info[mask,iii]) ) < rv_comb**2)\n",
    "#     print(rv_comb,var_comb)\n",
    "    plt.scatter(jabble_file[\"RV\"][mask,iii],info[mask,iii],alpha=0.4,zorder=1,c=cmap(loss_mean[mask,iii]))\n",
    "    plt.scatter([rv_comb],[1/var_comb],alpha=0.8,zorder=2,c='blue')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"RV\")\n",
    "    plt.ylabel(\"RV Info\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "\n",
    "    for jjj in jjjs:\n",
    "        print(jjj)\n",
    "        rv_data = jabble.physics.velocities(all_models[jjj][0][0].p)\n",
    "        orders = np.unique(all_models[jjj].metadata['orders'])\n",
    "        lmean = np.mean(np.exp(all_models[jjj][0][1].xs))\n",
    "        lmin = lmean-10\n",
    "        lmax = lmean+10\n",
    "        lspace = 4\n",
    "        \n",
    "        lrange = np.arange(lmin,lmax+lspace,lspace)\n",
    "        \n",
    "        rv_inds = [-1,0,1]\n",
    "        make_order_plot(file_b,all_models[jjj],lmin,lmax,lrange,[orders[1]],rv_inds,cpus[0])\n",
    "# #     # print(np.sum(np.isnan(info[:,iii])))\n",
    "#     # if (np.any(jabble_file[\"RV_err\"][:,iii]<0)):\n",
    "#     #     print(jabble_file[\"RV\"][:,iii],\"\\n\", jabble_file[\"RV_comb\"][iii],jabble_file[\"RV_err\"][:,iii])\n",
    "#     #     mask = ~np.isnan(info[:,iii])\n",
    "    #     rv_comb  =   jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii] ) / jnp.sum(info[mask,iii])\n",
    "    #     var_comb = ( jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii]**2 ) / jnp.sum(info[mask,iii]) ) - rv_comb**2\n",
    "    #     print(( jnp.dot( info[mask,iii],jabble_file['RV'][mask,iii]**2 ) / jnp.sum(info[mask,iii]) ) < rv_comb**2)\n",
    "    #     print(rv_comb,np.sqrt(var_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331987a1-44f2-45c5-b0cf-1b131928841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_all_order_plot(np.array(file_b['dates']),np.array(jabble_file['RV']),np.array(jabble_file['RV_err']),\\\n",
    "                  np.array(jabble_file['RV_comb']),np.array(jabble_file['RV_comb_err']),\\\n",
    "                  np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "                  np.array(file_b['bervs']),period=1e7)\n",
    "rv_all_order_harps_plot(np.array(file_b['dates']),np.array(jabble_file['RV']),np.array(jabble_file['RV_err']),\\\n",
    "                        np.array(jabble_file['RV_comb']),np.array(jabble_file['RV_comb_err']),\\\n",
    "                        np.array(file_b['pipeline_rvs']),np.array(file_b['pipeline_sigmas']),\\\n",
    "                        np.array(file_b['bervs']),np.array(jabble_file['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda82484-10f4-482d-a4e8-7afe7a01c0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_earth_residual_img(model,dataset,lrange,orders,rest_shifts,residual_resolution,plt_name,line_list):\n",
    "    xrange = np.log(lrange)\n",
    "    xmin, xmax = np.min(xrange), np.max(xrange)\n",
    "    # xinds = ((dataset[0].xs[:] < xmax) * (dataset[0].xs[:] > xmin)).astype(bool)\n",
    "    residual_img = np.zeros((len(dataset),residual_resolution))\n",
    "    fig, ax = plt.subplots(2,2,figsize=(8, 8),height_ratios=[1,4],width_ratios=[4,1],sharex='col',sharey='row')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    max_shift, min_shift = np.max(rest_shifts), np.min(rest_shifts)\n",
    "    new_grid = np.linspace(xmin,xmax,residual_resolution)\n",
    "\n",
    "    epsilon = np.log(np.mean(lrange) + 1) - np.log(np.mean(lrange))\n",
    "    model.fix()\n",
    "    model.display()\n",
    "\n",
    "    print(np.sum(np.log(line_list[1].data[\"Wave\"]) > xmin),np.sum(np.log(line_list[1].data[\"Wave\"]) < xmax),\\\n",
    "         np.sum((np.log(line_list[1].data[\"Wave\"]) > xmin)*(np.log(line_list[1].data[\"Wave\"]) < xmax)))\n",
    "    for line in line_list[1].data[(np.log(line_list[1].data[\"Wave\"]) > xmin) * (np.log(line_list[1].data[\"Wave\"]) < xmax)]:\n",
    "        print(line[\"Species\"])\n",
    "        ax[1,0].vlines(np.log(line[\"Wave\"]),0,len(dataset))\n",
    "    # for line in list_list\n",
    "    \n",
    "    for i,plt_epoch in enumerate(orders):\n",
    "        xless = (dataset[plt_epoch].xs[:] <= (xmax + rest_shifts[plt_epoch] + epsilon))\n",
    "        xmore = (dataset[plt_epoch].xs[:] >= (xmin + rest_shifts[plt_epoch] - epsilon))\n",
    "        xinds = (xless * \\\n",
    "                 xmore).astype(bool)\n",
    "        # print(np.sum(xmore),np.sum(xless))\n",
    "        x_grid = dataset[plt_epoch].xs[(~dataset[plt_epoch].mask)*xinds]\n",
    "        y_grid = dataset[plt_epoch].ys[(~dataset[plt_epoch].mask)*xinds]\n",
    "        residual = (y_grid - model([],x_grid,plt_epoch))#*jnp.sqrt(dataset[plt_epoch].yivar[(~dataset[plt_epoch].mask)*xinds])\n",
    "\n",
    "        if np.sum(xless) == 0:\n",
    "            x_grid = np.array([dataset[plt_epoch].xs[~dataset[plt_epoch].mask].min()])\n",
    "            residual = np.array([0.0])\n",
    "        if np.sum(xmore) == 0:\n",
    "            x_grid = np.array([dataset[plt_epoch].xs[~dataset[plt_epoch].mask].max()])\n",
    "            residual = np.array([0.0])\n",
    "        # print(residual.shape,np.sum(dataset[plt_epoch].mask*xinds),np.sum(xinds),np.sum(dataset[plt_epoch].mask))\n",
    "        residual_img[i,:] = scipy.interpolate.interp1d(x_grid,residual,kind='nearest',bounds_error=False,fill_value=0.0)(new_grid + \\\n",
    "                                                                                                             rest_shifts[plt_epoch])\n",
    "    cmap = plt.get_cmap(\"RdBu\")\n",
    "\n",
    "    ax[0,1].axis('off')\n",
    "    ax[0,0].step(new_grid,       (residual_img**2).sum(axis=0),'k',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "    ax[0,0].step(new_grid,       (residual_img).sum(axis=0),'m',where='mid',zorder=1,alpha=0.3,ms=3)\n",
    "    ax[1,1].step((residual_img**2).sum(axis=1),np.arange(len(orders))[::-1],'k',where='post',zorder=1,alpha=0.3,ms=3)\n",
    "    \n",
    "    \n",
    "    ax[1,0].set_ylim(0,np.max(orders)+1)\n",
    "    ax[1,0].set_xlim(xmin,xmax)\n",
    "    extent = [xmin,xmax,0,np.max(orders)+1]\n",
    "    ax[1,0].imshow(residual_img,cmap=cmap,aspect=\"auto\",vmin=-0.1,vmax=0.1,extent=extent,interpolation='nearest')\n",
    "    ax[1,0].set_xlabel('Wavelength [$\\AA$]')\n",
    "    ax[1,0].set_ylabel('Epoches')\n",
    "    # # plt.xticks([])\n",
    "    ax[1,0].set_xticks(xrange)\n",
    "    ax[1,0].set_xticklabels(['{:0.1f}'.format(l) for l in lrange])\n",
    "    # ax[1,0].get_shared_x_axes().join(ax[1,0], ax[1,1])\n",
    "    # plt.xlabel()\n",
    "    plt.savefig(os.path.join(out_dir, plt_name))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # worst_epochs = np.zeros(len(orders),dtype=bool)\n",
    "    worst_epochs = (residual_img**2).sum(axis=1) > 0.7\n",
    "    # print(worst_epochs)\n",
    "    # print(orders)\n",
    "    # print(orders[worst_epochs])\n",
    "    return orders[worst_epochs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c294ac2-e277-4285-a31d-04af0dd72fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dir = glob.glob(\"../out/24-10-23/b*.mdl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b781670-3838-4f9d-ac6b-38aa87f7ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e7f80-310d-4175-952a-6eb21f38a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n",
    "for filename in dir:\n",
    "    model = jabble.model.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4ae3a-615b-4c5a-9a46-c1f636ed3906",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmin = 5580\n",
    "lmax = 5585\n",
    "lspacing = 1\n",
    "residual_resolution = 2048\n",
    "lrange = np.arange(lmin,lmax+lspacing,lspacing)\n",
    "plt_name =  \"02-{}_res_img_starref.png\".format(os.path.split(model_name_b)[-1],np.min(orders),np.max(orders))\n",
    "\n",
    "rest_vel = model_b[0][0].p#np.zeros(.shape)\n",
    "index_sort = np.argsort(rest_vel)\n",
    "rest_shifts =  model_b[0][0].p#jabble.physics.shifts(rest_vel)\n",
    "plot_earth_residual_img(model_b,dataset_b,lrange,index_sort,rest_shifts,residual_resolution,plt_name,line_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc66d8a-0c7f-4899-8ffe-1b68a98e21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in dir:\n",
    "    model = jabble.model.load(filename)\n",
    "    rv_data = jabble.physics.velocities(model[0][0].p)\n",
    "    orders = np.unique(model.metadata['orders'])\n",
    "    lmean = np.mean(np.exp(model[0][1].xs))\n",
    "    lmin = lmean-10\n",
    "    lmax = lmean+10\n",
    "    lspace = 4\n",
    "    \n",
    "    lrange = np.arange(lmin,lmax+lspace,lspace)\n",
    "    \n",
    "    rv_inds = [-1,0,1]\n",
    "    make_order_plot(file_b,model,lmin,lmax,lrange,[orders[1]],rv_inds,cpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868e127f-db8c-4a5b-85c0-4ff291a3cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rv_array = np.zeros(())\n",
    "# order_array = np.zeros(())\n",
    "# time_array = np.zeros(())\n",
    "all_models =  []\n",
    "for filename in dir:\n",
    "    model = jabble.model.load(filename)\n",
    "    all_models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d698ec6-69c3-4faf-a3a5-9ffcd51acdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d43831-f646-436d-8765-64a434e0b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_rv_difference(all_models,times_t,times_e,rv_e,err_e,bervs,):\n",
    "    \n",
    "    rv_array = np.zeros((len(all_models),*all_models[0][0][0].p.shape))\n",
    "    order_array = []\n",
    "\n",
    "    info_e = 1/err_e**2\n",
    "    epoches_span_e = np.zeros(len(rv_e))\n",
    "\n",
    "    rv_comb = jnp.array(len(times_t))\n",
    "    for iii,time in enumerate(times_t):\n",
    "        indices = times_e == time\n",
    "\n",
    "        rv_indiv[indices] = rv_e[indices] + bervs[iii] - correct_vel[iii]\n",
    "        epoches_span_e[indices] = iii\n",
    "        rv_comb = rv_comb.at[iii].set(jnp.dot(info_e[indices],rv_e[indices])/jnp.sum(info_e[indices]))\n",
    "        info_comb = info_comb.at[iii].set((jnp.dot(info_e[indices],rv_e[indices]**2)/jnp.sum(info_e[indices])) - rv_comb[iii]**2)\n",
    "\n",
    "\n",
    "    for i,model in enumerate(all_models):\n",
    "        # data, _, _, full_init_shifts, _ = get_dataset(file_b,orders,cpus[0])\n",
    "        datablock, metablock, keys = data.blockify(cpus[0],return_keys=True)\n",
    "        rv_array[i,:] = jabble.physics.velocities(model[0][0].p)\n",
    "        order_array.append(np.unique(model.metadata['orders']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7611249-62d4-40b9-a5db-3fa33b5e31ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_rv_difference(all_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44733f-83cc-4349-842b-ef4b905cf13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for star_name,file in zip([star_name_b, star_name_p],[file_b,file_p]):\n",
    "#     # prerun_orders(file,star_name)\n",
    "#     run_orders(file   ,star_name, cpus[0],cpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e533ae3a-9f6f-4097-abcd-1eee033e7a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jax.distributed.initialize()  # On GPU, see above for the necessary arguments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43107445-bfbc-41c2-9f1f-3bd230626736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prerun_orders_mpi(file,star_name,devices):\n",
    "    \n",
    "#     for order_num in range(file['data'].shape[0]):\n",
    "#         model_name = os.path.join('..','models',star_name+'_o{}.mdl'.format(order_num))\n",
    "\n",
    "#         if not os.path.exists(model_name):\n",
    "#             dataset, shifts, airmass = get_dataset(file,[order_num],dev)\n",
    "    \n",
    "#             print(dataset[0].xs.devices(),shifts.devices())\n",
    "#             model = get_model(dataset,resolution,p_val,vel_padding,shifts,airmass,pts_per_wavelength,norm_p_val,cpus[0])\n",
    "#             model = pre_train_cycle(model, dataset, loss)\n",
    "#             jabble.model.save(model_name,model)\n",
    "#         else:\n",
    "#             print(model_name, ' already exists skipping.')\n",
    "#             continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf80427-d6cc-41da-a6e1-5e5e40ea38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs = np.arange(6).reshape(2,3)\n",
    "# print(len(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7ffd3-9235-4197-8d4b-166dce0487a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8329d61-8e8a-4a9e-b552-4f86a2008162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _internal(order_num):\n",
    "#     # for order_num in orders:\n",
    "#     model_name = os.path.join('..','models',star_name+'_o{}.mdl'.format(order_num))\n",
    "\n",
    "#     # if not os.path.exists(model_name):\n",
    "#     dataset, shifts, airmass = get_dataset(file,[order_num])\n",
    "\n",
    "#     print(dataset[0].xs.devices(),shifts.devices())\n",
    "#     model = get_model(dataset,resolution,p_val,vel_padding,shifts,airmass,pts_per_wavelength,norm_p_val)\n",
    "#     model = pre_train_cycle(model, dataset, loss)\n",
    "#     jabble.model.save(model_name,model)\n",
    "#     # else:\n",
    "#     #     print(model_name, ' already exists skipping.')\n",
    "#     #     continue\n",
    "\n",
    "# def prerun_orders(file,star_name):\n",
    "    \n",
    "#     num_devices = jax.device_count()\n",
    "#     orders = np.arange(file['data'].shape[0])\n",
    "#     with Pool(num_devices) as p:\n",
    "#         p.map(_internal,orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bce99b-decd-43a5-a570-d46257ad1548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector = jnp.arange(10)\n",
    "# size = len(vector)\n",
    "# _param_bool = np.zeros((size,int(np.sum(vector))))\n",
    "# for i in range(size):\n",
    "#     _param_bool[i,int(jnp.sum(vector[:i])):int(jnp.sum(vector[: i + 1]))] = jnp.ones(\n",
    "#                                     (int(jnp.sum(vector[: i + 1])) - int(jnp.sum(vector[:i]))),\n",
    "#                                     dtype=bool,\n",
    "#                                 )\n",
    "# _param_bool = jnp.array(_param_bool,dtype=bool)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6654272-8ed1-4917-ba8d-8c50ba250662",
   "metadata": {},
   "source": [
    "5577 night sky line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8278831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modelname = 'barnardsvmapmodel1.mdl'\n",
    "# # model = jabble.model.load(modelname)\n",
    "# jabble.model.save(modelname,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef9439-9570-4979-8330-be3ef582835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_plot(model,dataset,init_shifts,filename):\n",
    "#     x_window = np.log(4550) - np.log(4549)\n",
    "#     lmin = np.exp(dataset.xs[0,500])\n",
    "#     lmax = np.exp(dataset.xs[0,1500])\n",
    "#     lrange = np.arange(lmin,lmax,5)\n",
    "#     plt_unit = u.Angstrom\n",
    "#     epoches = 25\n",
    "#     r_plots = 5\n",
    "\n",
    "#     vel_epoch = 5\n",
    "#     fig, axes = plt.subplots(\n",
    "#         epoches // r_plots,\n",
    "#         r_plots,\n",
    "#         figsize=(8, 8),\n",
    "#         sharex=False,\n",
    "#         sharey=True,\n",
    "#         facecolor=(1, 1, 1),\n",
    "#         dpi=200,\n",
    "#     )\n",
    "#     # fig.suptitle(filenames[model_num])\n",
    "#     for plt_epoch in range((epoches // r_plots) * r_plots):\n",
    "#         xplot = np.linspace(np.log(lmin), np.log(lmax), dataset.xs.shape[1] * 10)\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xlim(\n",
    "#             xplot.min() + model[0][0].p[plt_epoch],\n",
    "#             xplot.max() + model[0][0].p[plt_epoch],\n",
    "#         )\n",
    "\n",
    "#         # model_set[model_num].fix()\n",
    "#         # model_set[model_num].fit(0)\n",
    "#         # rv_model_deriv = jax.jacfwd(model_set[model_num], argnums=0)(model_set[model_num].get_parameters(),dataset.xs[plt_epoch,:],plt_epoch)\n",
    "#         # rv_loss_deriv = jax.jacfwd(loss, argnums=0)(model_set[model_num].get_parameters(),datasets[0],vel_epoch,model_set[model_num])\n",
    "\n",
    "#         model.fix()\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].errorbar(\n",
    "#             dataset.xs[plt_epoch, :],\n",
    "#             dataset.ys[plt_epoch, :],\n",
    "#             dataset.yerr[plt_epoch, :],\n",
    "#             fmt=\".k\",\n",
    "#             elinewidth=1.2,\n",
    "#             zorder=1,\n",
    "#             alpha=0.5,\n",
    "#             ms=3,\n",
    "#         )\n",
    "\n",
    "#         # true_model.fix()\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(\n",
    "#             xplot,\n",
    "#             model([], xplot, plt_epoch),\n",
    "#             \"-r\",\n",
    "#             linewidth=1.2,\n",
    "#             zorder=2,\n",
    "#             alpha=0.5,\n",
    "#             ms=6,\n",
    "#         )\n",
    "#         # axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(xplot,true_model([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=1,alpha=0.5,ms=6)\n",
    "\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_ylim(-2, 1)\n",
    "#         #         axes[i,j].set_yticks([])\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xticks(np.log(lrange))\n",
    "#         axes[plt_epoch // r_plots, plt_epoch % r_plots].set_xticklabels(\n",
    "#             [\"{:2.0f}\".format(x) for x in lrange]\n",
    "#         )\n",
    "\n",
    "#         res_ax = axes[plt_epoch // r_plots, plt_epoch % r_plots].twinx()\n",
    "#         residual = loss(\n",
    "#             model.get_parameters(),\n",
    "#             dataset,\n",
    "#             plt_epoch,\n",
    "#             model,\n",
    "#         )\n",
    "#         res_ax.step(\n",
    "#             dataset.xs[plt_epoch, :], residual, where=\"mid\", alpha=0.3, label=\"residual\"\n",
    "#         )\n",
    "#         res_ax.set_ylim(0.0, 20)\n",
    "#         res_ax.set_yticks([])\n",
    "#         # res_ax.step(model_set[i][j][1].xs+model_set[i][j][0].p[plt_epoch],\\\n",
    "#         #             model_set[i][j].results[-2]['grad'][:],\\\n",
    "#         #             where='mid',alpha=0.4,label='residual',zorder=-1)\n",
    "#         # res_ax.set_yticks([])\n",
    "\n",
    "#         # res_ax.step(x_grid,\\\n",
    "#         #             rv_model_deriv[:,plt_epoch],\\\n",
    "#         #             where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "\n",
    "#         #     res_ax.step(x_grid,\\\n",
    "#         #                 rv_loss_deriv[:,plt_epoch],\\\n",
    "#         #                 where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "\n",
    "#         #     align_yaxis(, 0, , 0)\n",
    "\n",
    "#         align.yaxes(\n",
    "#             axes[plt_epoch // r_plots, plt_epoch % r_plots], 0.0, res_ax, 0.0, 2.0 / 3.0\n",
    "#         )\n",
    "\n",
    "#     # res.get_shared_y_axes().join(ax1, ax3)\n",
    "#     fig.text(0.5, 0.04, \"$\\lambda$\", ha=\"center\")\n",
    "#     fig.text(0.04, 0.5, \"y\", va=\"center\", rotation=\"vertical\")\n",
    "#     # fig.text(0.96, 0.5, '$d \\L /d \\delta x$', va='center', rotation=270)\n",
    "#     # fig.text(0.96, 0.5, '$d f_{{{ji}}} /d \\delta x_k$', va='center', rotation=270)\n",
    "#     fig.text(0.96, 0.5, \"residuals\", va=\"center\", rotation=270)\n",
    "\n",
    "#     plt.savefig(\n",
    "#         os.path.join(out_dir, \"02-res_{}.png\".format(filename)),\n",
    "#         dpi=300,\n",
    "#         bbox_inches=\"tight\",\n",
    "#     )\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8f735-8aca-4ee1-b4da-ea56f90eaa10",
   "metadata": {},
   "source": [
    "6563 h alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bae852-c141-4694-bc48-e9782d1cdff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['51peg','barnards']\n",
    "make_plot(model_b,dataset_b,shifts_b,filenames[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef5806f-2751-40e3-828f-48bcde270614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rv_plot(model_set,datasets,shift_set,filenames,file_set):\n",
    "#     fig, ax = plt.subplots(\n",
    "#         len(model_set),\n",
    "#         figsize=(8, 8),\n",
    "#         facecolor=(1, 1, 1),\n",
    "#         dpi=300,\n",
    "#         sharey=True,\n",
    "#     )\n",
    "     \n",
    "#     for i in range(len(model_set)):\n",
    "#         velocities = jabble.physics.velocities(shift_set[i]) * u.m/u.s\n",
    "#         epoches = datasets[i].xs.shape[0]\n",
    "#         epoch_range = np.arange(0, epoches, dtype=int)\n",
    "#         fischer_information = np.zeros(epoches)\n",
    "#         for e_num in range(epoches):\n",
    "#             model_set[i].fix()\n",
    "#             model_set[i].fit(0,0)\n",
    "#             temp = jax.jacfwd(model_set[i], argnums=0)(model_set[i].get_parameters(),datasets[i].xs[e_num,:],e_num)\n",
    "#             # print(temp.shape)\n",
    "#             fischer_information[e_num] = jnp.dot(\n",
    "#                 temp[:, e_num] ** 2, datasets[i].yivar[e_num, :]\n",
    "#             )\n",
    "    \n",
    "#         dvddx = jnp.array(\n",
    "#             [jax.grad(jabble.physics.velocities)(x) for x in model_set[i][0][0].p]\n",
    "#         )\n",
    "#         verr = np.sqrt(1 / fischer_information) * dvddx\n",
    "#         estimate_vel = jabble.physics.velocities(model_set[i][0][0].p)\n",
    "#         tv = velocities.to(u.m/u.s).value - velocities.to(u.m/u.s).value.mean()\n",
    "#         ev = estimate_vel - estimate_vel.mean()\n",
    "#         ax[i].errorbar(\n",
    "#             epoch_range,\n",
    "#             tv - tv,\n",
    "#             yerr=file_set[i][\"pipeline_sigmas\"][:],\n",
    "#             fmt=\".r\",\n",
    "#             elinewidth=2.2,\n",
    "#             zorder=1,\n",
    "#             alpha=0.5,\n",
    "#             ms=6,\n",
    "#         )\n",
    "    \n",
    "#         ax[i].errorbar(epoch_range,tv - ev,yerr=verr,fmt='.k',elinewidth=2.2,zorder=1,alpha=0.5,ms=6)\n",
    "    \n",
    "#         ax[i].set_title('{}'.format(filenames[i], model_set[i][1][0].p_val))\n",
    "#         ax[i].set_xlim(-0.5, epoches - 0.5)\n",
    "#     fig.text(0.04, 0.5, \"$v_{truth} - v_{est}$ [$m/s$]\", va=\"center\", rotation=\"vertical\")\n",
    "#     fig.text(0.5, 0.04, \"epochs\", ha=\"center\")\n",
    "#     # plt.savefig(os.path.join(out_dir, \"02-dv-barn-51peg.png\"))\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d731a5-2510-4846-a9fb-05bcfdf46c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_set = [model_p,  model_b]\n",
    "# datasets  = [dataset_p,dataset_b]\n",
    "# shift_set = [shifts_p, shifts_b]\n",
    "# file_set  = [file_p,   file_b]\n",
    "# rv_plot(model_set,datasets,shift_set,filenames,file_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f721096-7dd7-4376-93e7-8b41bfadab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_better_plot(model_set,datasets,file_set):\n",
    "    \n",
    "\n",
    "#     fig, axes = plt.subplots(2*len(model_set),4,figsize=(4*4,4*len(model_set)),sharex=True,facecolor=(1, 1, 1),dpi=200,height_ratios=[4,1]*len(model_set))\n",
    "    \n",
    "#     for jj,(model,dataset,file) in enumerate(zip(model_set,datasets,file_set)):\n",
    "#         x_window = np.log(4550) - np.log(4549)\n",
    "#         lmin = np.exp(dataset.xs[0,0])\n",
    "#         lmax = np.exp(dataset.xs[0,2000])\n",
    "#         lrange = np.arange(lmin,lmax,5)\n",
    "#         sort_airmasses = np.argsort(np.array(file['airms'][:]))\n",
    "#         plt_epochs = np.concatenate((sort_airmasses[:2],sort_airmasses[-2:]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         offset = 1.0\n",
    "#         xplot = np.linspace(np.log(lmin)-x_window,np.log(lmax)+x_window,dataset.xs.shape[1]*10)\n",
    "#         for ii,plt_epoch in enumerate(plt_epochs):\n",
    "#             axes[2*jj,ii].set_xlim(xplot.min()+model[0][0].p[plt_epoch],xplot.max()+model[0][0].p[plt_epoch])\n",
    "            \n",
    "#             model.fix()\n",
    "            \n",
    "#             axes[2*jj,ii].errorbar(dataset.xs[plt_epoch,:],dataset.ys[plt_epoch,:],\\\n",
    "#                                      dataset.yerr[plt_epoch,:],fmt='.k',elinewidth=1.2,zorder=1,alpha=0.5,ms=3)\n",
    "            \n",
    "#             axes[2*jj,ii].plot(xplot,offset + model[0]([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             axes[2*jj,ii].plot(xplot,2*offset + model[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             axes[2*jj,ii].plot(xplot,model[2]([],xplot,plt_epoch),'-m',linewidth=1.2,zorder=3,alpha=0.7,ms=6)\n",
    "            \n",
    "#             # axes[0,ii].plot(xplot,2*offset + model[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             # axes[0,ii].plot(xplot,offset + model([],xplot,plt_epoch),'-g',linewidth=1.2,zorder=2,alpha=0.7,ms=6)\n",
    "#             # axes[plt_epoch // r_plots, plt_epoch % r_plots].plot(xplot,true_model([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=1,alpha=0.5,ms=6)\n",
    "            \n",
    "            \n",
    "#             axes[2*jj,ii].set_ylim(-2,3)\n",
    "#             axes[2*jj,ii].set_xticks([])\n",
    "#             # axes[0].set_yticks([])\n",
    "#             axes[2*jj+1,ii].set_xticks(np.log(lrange))\n",
    "#             axes[2*jj+1,ii].set_xticklabels(['{:2.0f}'.format(x) for x in lrange])\n",
    "            \n",
    "#             axes[2*jj+1,ii].plot(dataset.xs[plt_epoch,:],dataset.ys[plt_epoch,:] - model([],dataset.xs[plt_epoch,:],plt_epoch),'.k',alpha=0.4,ms=1)\n",
    "            \n",
    "#             axes[2*jj+1,ii].set_ylim(-0.1,0.1)\n",
    "#             axes[2*jj,ii].set_title('airmass = {}'.format(file['airms'][:][plt_epoch]))\n",
    "#         # res_ax = axes[plt_epoch // r_plots, plt_epoch % r_plots].twinx()\n",
    "#         # residual = loss(model_set[model_num].get_parameters(),dataset,plt_epoch,model_set[model_num])\n",
    "#         # res_ax.step(dataset.xs[plt_epoch,:],residual,where='mid',alpha=0.3,label='residual')\n",
    "#         # res_ax.set_ylim(0.0,20)\n",
    "#         # res_ax.set_yticks([])\n",
    "#         # res_ax.step(model_set[i][j][1].xs+model_set[i][j][0].p[plt_epoch],\\\n",
    "#         #             model_set[i][j].results[-2]['grad'][:],\\\n",
    "#         #             where='mid',alpha=0.4,label='residual',zorder=-1)\n",
    "#         # res_ax.set_yticks([])\n",
    "        \n",
    "#         # res_ax.step(x_grid,\\\n",
    "#         #             rv_model_deriv[:,plt_epoch],\\\n",
    "#         #             where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "            \n",
    "#         #     res_ax.step(x_grid,\\\n",
    "#         #                 rv_loss_deriv[:,plt_epoch],\\\n",
    "#         #                 where='mid',alpha=0.4,label='RV Derivative',zorder=-1)\n",
    "            \n",
    "#         #     align_yaxis(, 0, , 0)\n",
    "            \n",
    "#             # align.yaxes(axes[plt_epoch // r_plots, plt_epoch % r_plots], 0.0, res_ax, 0.0, 2./3.)\n",
    "        \n",
    "#         # res.get_shared_y_axes().join(ax1, ax3)\n",
    "#         fig.text(0.5, 0.04, '$\\lambda$', ha='center')\n",
    "#         # fig.text(0.04, 0.5, 'y', va='center', rotation='vertical')\n",
    "#         # fig.text(0.96, 0.5, '$d \\L /d \\delta x$', va='center', rotation=270)\n",
    "#         # fig.text(0.96, 0.5, '$d f_{{{ji}}} /d \\delta x_k$', va='center', rotation=270)\n",
    "#         # fig.text(0.96, 0.5, 'residuals', va='center', rotation=270)\n",
    "    \n",
    "#     # plt.savefig(os.path.join(out_dir,'02-full-barn-51peg.png'),dpi=300,bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04116c4-5bcc-473c-9323-90f440ebd983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_better_plot(model_set,datasets,file_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e991d2-9c6e-449b-8c34-b9989fa1cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell_loss = [[],[]]\n",
    "# for jjj, (dataset, model) in enumerate(zip(datasets,model_set)):\n",
    "#     for iii in range(dataset.ys.shape[0]):\n",
    "#         tell_loss[jjj].append(loss([],dataset,iii,model[0]).sum())\n",
    "\n",
    "# plt.plot(np.array(file_p['airms'][:]),tell_loss[0],'.k',label='51 peg')\n",
    "# plt.plot(np.array(file_b['airms'][:]),tell_loss[1],'.r',label='barnards')\n",
    "# # plt.ylim(0.0,5e4)\n",
    "\n",
    "# # plt.plot(np.array(file_p['airms'][:]),model_p[1][1].p,'.k',label='51 peg')\n",
    "# # plt.plot(np.array(file_b['airms'][:]),model_b[1][1].p,'.r',label='barnards')\n",
    "# plt.xlabel('airmass')\n",
    "# plt.ylabel('$\\Sigma_* (y_* - \\hat{y}_s(x_*)) I_{y*}$')\n",
    "# # plt.plot()\n",
    "# plt.legend()\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass_loss.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cecc1d-a1f4-45fb-9bcd-b320c59182e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(np.array(file_p['airms'][:]),model_p[1][1].p,'.k',label='51 peg')\n",
    "# plt.plot(np.array(file_b['airms'][:]),model_b[1][1].p,'.r',label='barnards')\n",
    "# plt.xlabel('airmass')\n",
    "# plt.ylabel('~a')\n",
    "# x_space = np.linspace(np.min(np.array(file_b['airms'][:])),np.max(np.array(file_b['airms'][:])))\n",
    "# plt.plot(x_space,x_space,'-.k',alpha=0.3)\n",
    "# plt.legend()\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass_an.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb2dd7d-b813-4d62-a105-87bc9806bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(1,figsize=(4,4),sharex=True,facecolor=(1, 1, 1),dpi=200)\n",
    "\n",
    "# plt_epoch = 10\n",
    "# x_window = np.log(4550) - np.log(4549)\n",
    "# lmin = np.exp(dataset_p.xs[0,500])\n",
    "# lmax = np.exp(dataset_p.xs[0,1500])\n",
    "# lrange = np.arange(lmin,lmax,5)\n",
    "# xplot = np.linspace(np.log(lmin)-x_window,np.log(lmax)+x_window,dataset_p.xs.shape[1]*10)\n",
    "# axes.plot(xplot,model_p[1]([],xplot,plt_epoch),'-b',linewidth=1.2,zorder=2,alpha=0.6,ms=6,label='51 peg')\n",
    "# axes.plot(xplot,0.05 + model_b[1]([],xplot,plt_epoch),'-r',linewidth=1.2,zorder=2,alpha=0.6,ms=6,label='barnard')\n",
    "# axes.legend()\n",
    "\n",
    "# axes.set_ylim(-0.2,0.1)\n",
    "# axes.set_xticks([])\n",
    "# axes.set_ylabel('log flux + offset')\n",
    "# axes.set_xlabel('$\\lambda$')\n",
    "# axes.set_xticks(np.log(lrange))\n",
    "# axes.set_xticklabels(['{:2.0f}'.format(x) for x in lrange])\n",
    "# plt.title('just tellurics')\n",
    "# # plt.savefig(os.path.join(out_dir,'02-airmass-tell.png'),dpi=300,bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07406b9-398f-4f2e-b5f7-64a7dc12c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_p.fix()\n",
    "# model_p.fit(0,1)\n",
    "# e_num = 0\n",
    "# dudth = jax.jacfwd(model_p, argnums=0)(model_p.get_parameters(),dataset_p.xs[e_num,:],e_num)\n",
    "# ith   = dudth * dataset.yivar * dudth.T\n",
    "# print(dudth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a173f-f1bb-4a7c-89f9-dfa053f51f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import itertools\n",
    "# from scipy.linalg.blas import zhpr, dspr2, zhpmv\n",
    "\n",
    "# def acc_vect(pos, mas):\n",
    "#     n = mas.size\n",
    "#     d2 = pos@(-2*pos.T)\n",
    "#     diag = -0.5 * np.einsum('ii->i', d2)\n",
    "#     d2 += diag + diag[:, None]\n",
    "#     np.einsum('ii->i', d2)[...] = 1\n",
    "#     return np.nansum((pos[:, None, :] - pos) * (mas[:, None] * d2**-1.5)[..., None], axis=0)\n",
    "\n",
    "# def acc_blas(pos, mas):\n",
    "#     n = mas.size\n",
    "#     # trick: use complex Hermitian to get the packed anti-symmetric\n",
    "#     # outer difference in the imaginary part of the zhpr answer\n",
    "#     # don't want to sum over dimensions yet, therefore must do them one-by-one\n",
    "#     trck = np.zeros((3, n * (n + 1) // 2), complex)\n",
    "#     for a, p in zip(trck, pos.T - 1j):\n",
    "#         zhpr(n, -2, p, a, 1, 0, 0, 1)\n",
    "#         # does  a  ->  a + alpha x x^H\n",
    "#         # parameters: n             --  matrix dimension\n",
    "#         #             alpha         --  real scalar\n",
    "#         #             x             --  complex vector\n",
    "#         #             ap            --  packed Hermitian n x n matrix a\n",
    "#         #                               i.e. an n(n+1)/2 vector\n",
    "#         #             incx          --  x stride\n",
    "#         #             offx          --  x offset\n",
    "#         #             lower         --  is storage of ap lower or upper\n",
    "#         #             overwrite_ap  --  whether to change a inplace\n",
    "#     # as a by-product we get pos pos^T:\n",
    "#     ppT = trck.real.sum(0) + 6\n",
    "#     # now compute matrix of squared distances ...\n",
    "#     # ... using (A-B)^2 = A^2 + B^2 - 2AB\n",
    "#     # ... that and the outer sum X (+) X.T equals X ones^T + ones X^T\n",
    "#     dspr2(n, -0.5, ppT[np.r_[0, 2:n+1].cumsum()], np.ones((n,)), ppT,\n",
    "#           1, 0, 1, 0, 0, 1)\n",
    "#     # does  a  ->  a + alpha x y^T + alpha y x^T    in packed symmetric storage\n",
    "#     # scale anti-symmetric differences by distance^-3\n",
    "#     np.divide(trck.imag, ppT*np.sqrt(ppT), where=ppT.astype(bool),\n",
    "#               out=trck.imag)\n",
    "#     # it remains to scale by mass and sum\n",
    "#     # this can be done by matrix multiplication with the vector of masses ...\n",
    "#     # ... unfortunately because we need anti-symmetry we need to work\n",
    "#     # with Hermitian storage, i.e. complex numbers, even though the actual\n",
    "#     # computation is only real:\n",
    "#     out = np.zeros((3, n), complex)\n",
    "#     for a, o in zip(trck, out):\n",
    "#         zhpmv(n, 0.5, a, mas*-1j, 1, 0, 0, o, 1, 0, 0, 1)\n",
    "#         # multiplies packed Hermitian matrix by vector\n",
    "#     return out.real.T\n",
    "\n",
    "# def accelerations(positions, masses, epsilon=1e-6, gravitational_constant=1.0):\n",
    "#     '''Params:\n",
    "#     - positions: numpy array of size (n,3)\n",
    "#     - masses: numpy array of size (n,)\n",
    "#     '''\n",
    "#     n_bodies = len(masses)\n",
    "#     accelerations = np.zeros([n_bodies,3]) # n_bodies * (x,y,z)\n",
    "\n",
    "#     # vectors from mass(i) to mass(j)\n",
    "#     D = np.zeros([n_bodies,n_bodies,3]) # n_bodies * n_bodies * (x,y,z)\n",
    "#     for i, j in itertools.product(range(n_bodies), range(n_bodies)):\n",
    "#         D[i][j] = positions[j]-positions[i]\n",
    "\n",
    "#     # Acceleration due to gravitational force between each pair of bodies\n",
    "#     A = np.zeros((n_bodies, n_bodies,3))\n",
    "#     for i, j in itertools.product(range(n_bodies), range(n_bodies)):\n",
    "#         if np.linalg.norm(D[i][j]) > epsilon:\n",
    "#             A[i][j] = gravitational_constant * masses[j] * D[i][j] \\\n",
    "#             / np.linalg.norm(D[i][j])**3\n",
    "\n",
    "#     # Calculate net accleration of each body\n",
    "#     accelerations = np.sum(A, axis=1) # sum of accel vectors for each body\n",
    "\n",
    "#     return accelerations\n",
    "\n",
    "# from numpy.linalg import norm\n",
    "\n",
    "# def acc_pm(positions, masses, G=1):\n",
    "#     '''Params:\n",
    "#     - positions: numpy array of size (n,3)\n",
    "#     - masses: numpy array of size (n,)\n",
    "#     '''\n",
    "#     mass_matrix = masses.reshape((1, -1, 1))*masses.reshape((-1, 1, 1))\n",
    "#     disps = positions.reshape((1, -1, 3)) - positions.reshape((-1, 1, 3)) # displacements\n",
    "#     dists = norm(disps, axis=2)\n",
    "#     dists[dists == 0] = 1 # Avoid divide by zero warnings\n",
    "#     forces = G*disps*mass_matrix/np.expand_dims(dists, 2)**3\n",
    "#     return forces.sum(axis=1)/masses.reshape(-1, 1)\n",
    "\n",
    "# n = 500\n",
    "# pos = np.random.random((n, 3))\n",
    "# mas = np.random.random((n,))\n",
    "\n",
    "# from timeit import timeit\n",
    "\n",
    "# print(f\"loops:      {timeit('accelerations(pos, mas)', globals=globals(), number=1)*1000:10.3f} ms\")\n",
    "# print(f\"pmende:     {timeit('acc_pm(pos, mas)', globals=globals(), number=10)*100:10.3f} ms\")\n",
    "# print(f\"vectorized: {timeit('acc_vect(pos, mas)', globals=globals(), number=10)*100:10.3f} ms\")\n",
    "# print(f\"blas:       {timeit('acc_blas(pos, mas)', globals=globals(), number=10)*100:10.3f} ms\")\n",
    "\n",
    "# A = accelerations(pos, mas)\n",
    "# AV = acc_vect(pos, mas)\n",
    "# AB = acc_blas(pos, mas)\n",
    "# AP = acc_pm(pos, mas)\n",
    "\n",
    "# assert np.allclose(A, AV) and np.allclose(AB, AV) and np.allclose(AP, AV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6cd0c5-474a-4d94-9746-4a733eeaee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def variation_info(self,model,dataset):\n",
    "#     f_info = np.zeros(dataset.xs.shape)\n",
    "#     model.fix()\n",
    "#     self.fit()\n",
    "#     for e_num in range(dataset.xs.shape[0]):\n",
    "#         duddx = jax.jacfwd(model, argnums=0)(model.get_parameters(),dataset.xs[e_num,:],e_num)\n",
    "#         f_info[e_num,:] =  jnp.dot(duddx[:,e_num]**2,dataset.yivar[e_num,:])\n",
    "#     return f_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
